<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>James Chen&#39;s Blogs</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://james20141606.github.io/"/>
  <updated>2018-05-04T16:04:27.763Z</updated>
  <id>http://james20141606.github.io/</id>
  
  <author>
    <name>James Chen</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Jupyter自定义设置详解</title>
    <link href="http://james20141606.github.io/2018/05/04/jupyter/"/>
    <id>http://james20141606.github.io/2018/05/04/jupyter/</id>
    <published>2018-05-04T07:09:56.000Z</published>
    <updated>2018-05-04T16:04:27.763Z</updated>
    
    <content type="html"><![CDATA[<p>今天专门花时间总结梳理一下jupyter的一些高级设置，jupyter我已经介绍过一次基本内容了,<a href="https://james20141606.github.io/2018/04/12/setup/">Setup and Linux | James Chen’s Blogs</a>，尤其是如何在服务器运行jupyter并且在本地浏览器显示，简直是使用python进行机器学习、深度学习、大数据的工作者的巨大福音。作为一个重度python&amp;jupyter使用者，我已经习惯于在jupyter上进行大量的实验以及一次性的小工作、作业，需要跑很久的代码才会在运行通过后用VSCode编辑一下提交上去跑。用jupyter写了很多脚本，尤其适合可视化、展示和教学。可以在我的<a href="https://github.com/james20141606/" target="_blank" rel="noopener">GitHub</a>找到很多用jupyter写的代码，事实上很多教程和实验大家也都习惯于jupyter做了，比如我在这篇<a href="https://james20141606.github.io/2018/04/10/Deep-Learning-Practice/">Deep Learning Practice</a>介绍的资源中就有大量用jupyter写的。</p><p>下面就讲一下jupyter的一些高级玩法。<br><a id="more"></a></p><h2 id="自定义主题"><a href="#自定义主题" class="headerlink" title="自定义主题"></a>自定义主题</h2><p><a href="https://github.com/dunovank/jupyter-themes" target="_blank" rel="noopener">https://github.com/dunovank/jupyter-themes</a><br><img src="http://i4.fuimg.com/640680/6dd3179b5d60eccb.png" alt="Markdown"></p><h2 id="jupyter-extension"><a href="#jupyter-extension" class="headerlink" title="jupyter  extension"></a>jupyter  extension</h2><p><strong>相见恨晚，用着非常非常爽，强烈推荐</strong><br><a href="https://github.com/ipython-contrib/jupyter_contrib_nbextensions" target="_blank" rel="noopener">https://github.com/ipython-contrib/jupyter_contrib_nbextensions</a></p><figure class="highlight mipsasm"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">pip <span class="keyword">install </span><span class="keyword">jupyter_contrib_nbextensions</span></span><br><span class="line"><span class="keyword">jupyter </span>contrib nbextension <span class="keyword">install </span>--user</span><br><span class="line">pip3 <span class="keyword">install </span><span class="keyword">jupyter_contrib_nbextensions</span></span><br><span class="line"><span class="keyword">jupyter </span>contrib nbextension <span class="keyword">install </span>--user</span><br></pre></td></tr></table></figure><h3 id="推荐使用的插件"><a href="#推荐使用的插件" class="headerlink" title="推荐使用的插件"></a>推荐使用的插件</h3><h4 id="table-of-contents"><a href="#table-of-contents" class="headerlink" title="table of contents"></a>table of contents</h4><p>设置着很简单，找到后打个对勾就行了，<strong>效果很棒</strong>：<br><img src="http://i4.fuimg.com/640680/5bfe6a9b5a48e822.png" alt="Markdown"></p><h4 id="freeze"><a href="#freeze" class="headerlink" title="freeze"></a>freeze</h4><p><img src="http://i4.fuimg.com/640680/a6412152652237a6.png" alt="Markdown"></p><h4 id="highlighter"><a href="#highlighter" class="headerlink" title="highlighter"></a>highlighter</h4><p><img src="http://i4.fuimg.com/640680/5cf4c30a3f01295d.png" alt="Markdown"></p><h4 id="Gist-it"><a href="#Gist-it" class="headerlink" title="Gist-it"></a>Gist-it</h4><p>Adds a button to publish the current notebook as a gist</p><h4 id="Snippets-menu"><a href="#Snippets-menu" class="headerlink" title="Snippets menu"></a>Snippets menu</h4><p>帮助懒人插入一些经典库的经典方法的代码块，感觉很不错哦，比去Stack Overflow搜要快一些~不过支持的还是比较少的。<br><img src="http://i4.fuimg.com/640680/7fc861662f88de4a.png" alt="Markdown"></p><h4 id="Snippet"><a href="#Snippet" class="headerlink" title="Snippet:"></a>Snippet:</h4><p><strong>强烈强烈推荐！</strong>感觉有望帮助节约不少写代码的时间</p><p><img src="http://i4.fuimg.com/640680/5a2fe35422f87b5e.png" alt="Markdown"></p><p>This extension adds a drop-down menu to the IPython toolbar that allows easy insertion of code snippet cells into the current notebook. The code snippets are defined in a JSON file in nbextensions/snippets/snippets.json and an example snippet is included with this extension</p><p>Snippets are specified by adding a new JSON block to the list of existing snippets in $(jupyter —data-dir)/nbextensions/snippets/snippets.json. <strong>(I put my customized json file in jupyter-notebooks directory in /Users/james/)</strong> For example, to add a new snippet that imports numpy, matplotlib, and a print statement, the JSON file should be modified。</p><p>先分别列一下各种情景下需要导入哪些库：</p><h5 id="Basic-science"><a href="#Basic-science" class="headerlink" title="Basic science"></a>Basic science</h5><figure class="highlight xl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> argparse, sys, os, errno</span><br><span class="line">%pylab inline</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">plt.style.use(<span class="string">'ggplot'</span>)</span><br><span class="line"><span class="keyword">import</span> seaborn <span class="keyword">as</span> sns</span><br><span class="line"><span class="keyword">import</span> h5py</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line">from tqdm <span class="keyword">import</span> tqdm</span><br><span class="line"><span class="keyword">import</span> scipy</span><br><span class="line"><span class="keyword">import</span> sklearn</span><br><span class="line">from scipy.stats <span class="keyword">import</span> pearsonr</span><br><span class="line"><span class="keyword">import</span> warnings</span><br><span class="line">warnings.filterwarnings(<span class="string">'ignore'</span>)</span><br></pre></td></tr></table></figure><h5 id="Highlevel-plot"><a href="#Highlevel-plot" class="headerlink" title="Highlevel plot"></a>Highlevel plot</h5><figure class="highlight xl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.animation <span class="keyword">as</span> animation</span><br><span class="line">from matplotlib <span class="keyword">import</span> rc</span><br><span class="line">from IPython.display <span class="keyword">import</span> HTML, Image</span><br><span class="line">rc(<span class="string">'animation'</span>, html=<span class="string">'html5'</span>)</span><br><span class="line"><span class="keyword">import</span> plotly</span><br><span class="line"><span class="keyword">import</span> plotly.offline <span class="keyword">as</span> off</span><br><span class="line"><span class="keyword">import</span> plotly.plotly <span class="keyword">as</span> py</span><br><span class="line"><span class="keyword">import</span> plotly.graph_objs <span class="keyword">as</span> go</span><br></pre></td></tr></table></figure><h5 id="Deeplearning-keras"><a href="#Deeplearning-keras" class="headerlink" title="Deeplearning  keras"></a>Deeplearning  keras</h5><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">import keras</span><br><span class="line">from keras import backend as K</span><br><span class="line">from keras<span class="selector-class">.callbacks</span> import TensorBoard</span><br><span class="line">from keras<span class="selector-class">.callbacks</span> import EarlyStopping</span><br><span class="line">from keras<span class="selector-class">.optimizers</span> import Adam</span><br><span class="line">from keras<span class="selector-class">.callbacks</span> import ModelCheckpoint</span><br><span class="line">import tensorflow as tf</span><br><span class="line">from keras<span class="selector-class">.models</span> import Model</span><br><span class="line">from keras<span class="selector-class">.layers</span> import Input, Conv2D, MaxPooling2D, UpSampling2D,Lambda, Dot,average,add, concatenate</span><br><span class="line">from keras<span class="selector-class">.layers</span><span class="selector-class">.normalization</span> import BatchNormalization</span><br><span class="line">from keras<span class="selector-class">.layers</span><span class="selector-class">.core</span> import Dropout, Activation,Reshape</span><br><span class="line">from keras<span class="selector-class">.layers</span><span class="selector-class">.merge</span> import concatenate</span><br><span class="line">from keras<span class="selector-class">.callbacks</span> import TensorBoard, EarlyStopping, ModelCheckpoint</span><br><span class="line">from keras<span class="selector-class">.initializers</span> import RandomNormal</span><br><span class="line">import os</span><br><span class="line">os<span class="selector-class">.environ</span>[<span class="string">'CUDA_DEVICE_ORDER'</span>] = <span class="string">'PCI_BUS_ID'</span> </span><br><span class="line">os<span class="selector-class">.environ</span>[<span class="string">'CUDA_VISIBLE_DEVICES'</span>] = <span class="string">'4'</span></span><br><span class="line">from keras<span class="selector-class">.backend</span><span class="selector-class">.tensorflow_backend</span> import set_session</span><br><span class="line">config = tf.ConfigProto()</span><br><span class="line">config<span class="selector-class">.gpu_options</span><span class="selector-class">.per_process_gpu_memory_fraction</span> = <span class="number">0.99</span></span><br><span class="line"><span class="function"><span class="title">set_session</span><span class="params">(tf.Session(config=config)</span></span>)</span><br></pre></td></tr></table></figure><h5 id="Pytorch"><a href="#Pytorch" class="headerlink" title="Pytorch"></a>Pytorch</h5><figure class="highlight elm"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br></pre></td></tr></table></figure><p>总结起来就是下面这样，注意语法别错，东西一多看着还是很头疼的，配置好就可以用了。</p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    <span class="attr">"snippets"</span> : [</span><br><span class="line">&#123;</span><br><span class="line"><span class="attr">"name"</span> : <span class="string">"science basic"</span>,</span><br><span class="line"><span class="attr">"code"</span> : [</span><br><span class="line"><span class="string">"import argparse, sys, os, errno"</span>,</span><br><span class="line"><span class="string">"%pylab inline"</span>,</span><br><span class="line"><span class="string">"import numpy as np"</span>,</span><br><span class="line"><span class="string">"import pandas as pd"</span>,</span><br><span class="line"><span class="string">"import matplotlib.pyplot as plt"</span>,</span><br><span class="line"><span class="string">"plt.style.use('ggplot')"</span>,</span><br><span class="line"><span class="string">"import seaborn as sns"</span>,</span><br><span class="line"><span class="string">"import h5py"</span>,</span><br><span class="line"><span class="string">"import os"</span>,</span><br><span class="line"><span class="string">"from tqdm import tqdm"</span>,</span><br><span class="line"><span class="string">"import scipy"</span>,</span><br><span class="line"><span class="string">"import sklearn"</span>,</span><br><span class="line"><span class="string">"from scipy.stats import pearsonr"</span>,</span><br><span class="line"><span class="string">"import warnings"</span>,</span><br><span class="line"><span class="string">"warnings.filterwarnings('ignore')"</span></span><br><span class="line">]</span><br><span class="line">        &#125;,</span><br><span class="line">&#123;</span><br><span class="line">        <span class="attr">"name"</span> : <span class="string">"high level plot"</span>,</span><br><span class="line">        <span class="attr">"code"</span> : [</span><br><span class="line">                    <span class="string">"import matplotlib.animation as animation"</span>,</span><br><span class="line">    <span class="string">"from matplotlib import rc"</span>,</span><br><span class="line">    <span class="string">"from IPython.display import HTML, Image"</span>,</span><br><span class="line">    <span class="string">"rc('animation', html='html5')"</span>,</span><br><span class="line">    <span class="string">"import plotly"</span>,</span><br><span class="line">    <span class="string">"import plotly.offline as off"</span>,</span><br><span class="line">    <span class="string">"import plotly.plotly as py"</span>,</span><br><span class="line">    <span class="string">"import plotly.graph_objs as go"</span></span><br><span class="line">                ]</span><br><span class="line">        &#125;,</span><br><span class="line">&#123;</span><br><span class="line"><span class="attr">"name"</span> : <span class="string">"deep learning"</span>,</span><br><span class="line"><span class="attr">"code"</span> : [</span><br><span class="line"><span class="string">"import keras"</span>,</span><br><span class="line"><span class="string">"from keras import backend as K"</span>,</span><br><span class="line"><span class="string">"from keras.callbacks import TensorBoard"</span>,</span><br><span class="line"><span class="string">"from keras.callbacks import EarlyStopping"</span>,</span><br><span class="line"><span class="string">"from keras.optimizers import Adam"</span>,</span><br><span class="line"><span class="string">"from keras.callbacks import ModelCheckpoint"</span>,</span><br><span class="line"><span class="string">"import tensorflow as tf"</span>,</span><br><span class="line"><span class="string">"from keras.models import Model"</span>,</span><br><span class="line"><span class="string">"from keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D,Lambda, Dot,average,add, concatenate"</span>,</span><br><span class="line"><span class="string">"from keras.layers.normalization import BatchNormalization"</span>,</span><br><span class="line"><span class="string">"from keras.layers.core import Dropout, Activation,Reshape"</span>,</span><br><span class="line"><span class="string">"from keras.layers.merge import concatenate"</span>,</span><br><span class="line"><span class="string">"from keras.callbacks import TensorBoard, EarlyStopping, ModelCheckpoint"</span>,</span><br><span class="line"><span class="string">"from keras.initializers import RandomNormal"</span>,</span><br><span class="line"><span class="string">"import os"</span>,</span><br><span class="line"><span class="string">"os.environ['CUDA_DEVICE_ORDER'] = 'PCI_BUS_ID'"</span>,</span><br><span class="line"><span class="string">"os.environ['CUDA_VISIBLE_DEVICES'] = '4'"</span>,</span><br><span class="line"><span class="string">"from keras.backend.tensorflow_backend import set_session"</span>,</span><br><span class="line"><span class="string">"config = tf.ConfigProto()"</span>,</span><br><span class="line"><span class="string">"config.gpu_options.per_process_gpu_memory_fraction = 0.99"</span>,</span><br><span class="line"><span class="string">"set_session(tf.Session(config=config))"</span></span><br><span class="line">]</span><br><span class="line">&#125;,</span><br><span class="line">&#123;</span><br><span class="line">        <span class="attr">"name"</span> : <span class="string">"pytorch"</span>,</span><br><span class="line">        <span class="attr">"code"</span> : [</span><br><span class="line">                    <span class="string">"import torch"</span>,</span><br><span class="line">    <span class="string">"import math"</span>,</span><br><span class="line">    <span class="string">"import torch.nn as nn"</span>,</span><br><span class="line">    <span class="string">"import torch.nn.functional as F"</span></span><br><span class="line">                ]</span><br><span class="line">        &#125;</span><br><span class="line">    ]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>这个配置这里坑了我很久，主要是snippets不会显示错误信息，debug得很仔细一个一个找，因此花了很久发现是keras部分的几个破双引号冲突了，还有一个注释竟然也忘删了，折腾了很久才配置好。用sublime text是可以很容易发现语法错误的，可惜最开始没在意</p><p>本地的snippets.json在：</p><figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="regexp">/Users/</span>james<span class="regexp">/Library/</span>Jupyter<span class="regexp">/nbextensions/</span>snippets<span class="regexp">/snippets.json</span></span><br></pre></td></tr></table></figure><p>我在james目录下也放了一份，因为最近估计需要不断更新完善，所以就放一份在：</p><figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="regexp">/Users/</span>james<span class="regexp">/snippets.json</span></span><br></pre></td></tr></table></figure><p>每次修改后，需要同步到本地以及ibme、cnode、hpc1几个机器上面，这样<strong>不用我几个地方都各自改一遍</strong>，费事。</p><p>首先找到各个机器上的json文件在哪儿</p><figure class="highlight makefile"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="variable">$(jupyter --data-<span class="built_in">dir</span>)</span>/nbextensions/snippets/snippets.json</span><br></pre></td></tr></table></figure><p>然后写个同步的脚本syncsnip.sh</p><figure class="highlight groovy"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">cp <span class="regexp">/Users/</span>james<span class="regexp">/snippets.json /</span>Users<span class="regexp">/james/</span>Library<span class="regexp">/Jupyter/</span>nbextensions<span class="regexp">/snippets/</span>snippets.json</span><br><span class="line">rsync -avzh <span class="regexp">/Users/</span>james<span class="regexp">/snippets.json ibme:/</span>Share<span class="regexp">/home/</span>chenxupeng<span class="regexp">/.local/</span>share<span class="regexp">/jupyter/</span>nbextensions<span class="regexp">/snippets/</span></span><br><span class="line">rsync -avzh <span class="regexp">/Users/</span>james<span class="regexp">/snippets.json cnode:/</span>home<span class="regexp">/chenxupeng/</span>.local<span class="regexp">/share/</span>jupyter<span class="regexp">/nbextensions/</span>snippets/snippets.json</span><br><span class="line">rsync -avzh <span class="regexp">/Users/</span>james<span class="regexp">/snippets.json hpc1:/</span>home<span class="regexp">/chenxupeng/</span>.local<span class="regexp">/share/</span>jupyter<span class="regexp">/nbextensions/</span>snippets/</span><br></pre></td></tr></table></figure><p>然后做个同步快捷方式：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">alias</span> snip=<span class="string">'bash syncsnip.sh'</span></span><br></pre></td></tr></table></figure><p>测试几个机器的配置均通过</p><p>配置好了还是很美的：</p><p><img src="http://i1.fuimg.com/640680/c493f72487089e34.png" alt="Markdown"></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;今天专门花时间总结梳理一下jupyter的一些高级设置，jupyter我已经介绍过一次基本内容了,&lt;a href=&quot;https://james20141606.github.io/2018/04/12/setup/&quot;&gt;Setup and Linux | James Chen’s Blogs&lt;/a&gt;，尤其是如何在服务器运行jupyter并且在本地浏览器显示，简直是使用python进行机器学习、深度学习、大数据的工作者的巨大福音。作为一个重度python&amp;amp;jupyter使用者，我已经习惯于在jupyter上进行大量的实验以及一次性的小工作、作业，需要跑很久的代码才会在运行通过后用VSCode编辑一下提交上去跑。用jupyter写了很多脚本，尤其适合可视化、展示和教学。可以在我的&lt;a href=&quot;https://github.com/james20141606/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;GitHub&lt;/a&gt;找到很多用jupyter写的代码，事实上很多教程和实验大家也都习惯于jupyter做了，比如我在这篇&lt;a href=&quot;https://james20141606.github.io/2018/04/10/Deep-Learning-Practice/&quot;&gt;Deep Learning Practice&lt;/a&gt;介绍的资源中就有大量用jupyter写的。&lt;/p&gt;
&lt;p&gt;下面就讲一下jupyter的一些高级玩法。&lt;br&gt;
    
    </summary>
    
      <category term="techniques" scheme="http://james20141606.github.io/categories/techniques/"/>
    
    
      <category term="codes" scheme="http://james20141606.github.io/tags/codes/"/>
    
      <category term="techniques" scheme="http://james20141606.github.io/tags/techniques/"/>
    
  </entry>
  
  <entry>
    <title>论文与排版</title>
    <link href="http://james20141606.github.io/2018/05/04/%E8%AE%BA%E6%96%87%E4%B8%8E%E6%8E%92%E7%89%88/"/>
    <id>http://james20141606.github.io/2018/05/04/论文与排版/</id>
    <published>2018-05-04T06:58:25.000Z</published>
    <updated>2018-05-04T11:43:45.552Z</updated>
    
    <content type="html"><![CDATA[<p>三年前当我还是个什么都不懂的小屁孩的时候，就听闻了高大上的LaTeX，还去听了图书馆某工科博士开的LaTeX入门讲解，听的天花乱坠毫无头绪，回去兴冲冲地装上，运行，出现bug，然后傻眼了，解决不了，我现在还记得当时在百度文科搜到一篇讲解文章，教人一点点开始，第一次生成出来一句话（真是愚蠢的却又不得不这么教的教程）。然后放弃了这个大坑。</p><p>后来慢慢发现，其实就是个简单的工具而已，适应了基本的思维方式，命令熟练，特别是搜索能力变强可以快速debug之后，latex和markdown真的是完美的排版神器，尤其是在mac版的word经常崩而且公式极其丑陋且卡顿且长篇文章格式会让人非常恶心的情况下，我已经放弃了word，这一年除了老师要求必须交word的作业以及让人毫无兴趣的作业，基本都是用latex或markdown写的，这两个东西极大的方便了我的生活。</p><p>在这里分享一下各种相关资源和知识</p><a id="more"></a><h1 id="排版"><a href="#排版" class="headerlink" title="排版"></a>排版</h1><h2 id="markdown"><a href="#markdown" class="headerlink" title="markdown"></a>markdown</h2><p>下面列一下使用场景，这些场景有的是基本工具，有的是完成某些任务（如博客、README）所需要的。</p><h3 id="Bear"><a href="#Bear" class="headerlink" title="Bear"></a>Bear</h3><p>神器，我现在所有的笔记都在上面，非常完美兼容markdown各语法，并有快捷键，非常简洁清爽的让人变得无比有条理的程序员风格笔记管理软件。<strong>一个巨大的好处是，平时什么东西都放在bear上，再转移到下面的各个工具上就直接复制粘贴好了，再没有排版的困扰。</strong>另外备份所有笔记也很方便，对于害怕东西丢失的强迫症来说非常有意义。（话说经常手动备份在电脑端，且使用time machine直接备份和间接备份，并且最近清华云盘也每天备份，应该不会丢了吧，这些笔记确实非常宝贵，如果丢了我会心痛死的）</p><p>Markdown方便到最近都不怎么愿意用latex了，很多作业先随手写在bear上，然后本来想再用latex写，后来觉得直接导出到pdf也能看，起码比word强，也就懒得用latex再排版一遍了。</p><h3 id="Gitbook"><a href="#Gitbook" class="headerlink" title="Gitbook"></a>Gitbook</h3><p>Gitbook，这部分内容比较多，专门写了一篇文章：<br><a href="https://james20141606.github.io/2018/05/04/gitbook/">GitBook intro &amp; Grandpa’s Bio | James Chen’s Blogs</a></p><h3 id="MWeb"><a href="#MWeb" class="headerlink" title="MWeb"></a>MWeb</h3><p><a href="http://www.mweb.co.za/" target="_blank" rel="noopener">MWEB Homepage</a>，某个开发者开发的，功能强悍，可以一边写一边看效果，替代gitbook桌面版，代价是预览会有些卡顿抖动。但是实在是很好用，<strong>必备工具</strong></p><h3 id="GitHub"><a href="#GitHub" class="headerlink" title="GitHub"></a>GitHub</h3><p>各种README、Wiki都会用到。注意GitHub的wiki页面还可以有特殊的排版技巧，可以编辑一个特殊的_Sidebar页面来组织wiki的板式，比如实验室training项目的wiki：<a href="https://github.com/lulab/training/wiki/_Sidebar/_edit" target="_blank" rel="noopener">Home · lulab/training Wiki · GitHub</a></p><h3 id="BLOG"><a href="#BLOG" class="headerlink" title="BLOG"></a>BLOG</h3><p>这个博客的博文都是markdown写的，相当好用便捷。因为目前完全转向bear记笔记，所以写博客实在是非常轻松，基本hexo new “newblog”，然后把bear上的内容搬过去就行了，用MWEB编辑一下，图片用图片库做个外链，pdf用google drive加上插件展示，相当自动化了。</p><h3 id="Rmarkdown"><a href="#Rmarkdown" class="headerlink" title="Rmarkdown"></a>Rmarkdown</h3><p>统计学作业都用它，最开始用了两次latex简直要吐，后来发现这种公式太多的东西还是所见即所得的Rmarkdown最完美，用Rstutio排版，谁用谁知道，而且还可以无缝嵌入R代码，用来写统辅作业必备。</p><p><strong>展示如何用Rstudio插入公式、代码：</strong><br><img src="http://i4.fuimg.com/640680/a7923414b1e9f50e.png" alt="Markdown"></p><h3 id="Pandoc"><a href="#Pandoc" class="headerlink" title="Pandoc"></a>Pandoc</h3><p>貌似是很强大的格式转换工具，可以把markdown转latex之类的，不过我觉得意义不大，有这功夫还是直接用latex吧，怎么可能完美转换，之前看过一个强行转的论文，说实话并没有纯粹的latex漂亮</p><h3 id="Jupyter-markdown"><a href="#Jupyter-markdown" class="headerlink" title="Jupyter markdown"></a>Jupyter markdown</h3><p>这个还是蛮有趣的，写代码的时候注释很有用，而且最近发现了一个很好的插件，在另一篇文章中我也写了，叫nbextensions，其中一个功能就是可以直接把jupyter的markdown注释收集起来，根据标题层级生成目录。安装如下，具体内容可自行搜索或者看我的jupyter配置的文章</p><figure class="highlight mipsasm"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">pip <span class="keyword">install </span><span class="keyword">jupyter_contrib_nbextensions</span></span><br><span class="line"><span class="keyword">jupyter </span>contrib nbextension <span class="keyword">install </span>--user</span><br></pre></td></tr></table></figure><p><strong>效果</strong></p><p><img src="http://i4.fuimg.com/640680/b3489af920c29d1e.png" alt="Markdown"></p><h2 id="LaTeX"><a href="#LaTeX" class="headerlink" title="LaTeX"></a>LaTeX</h2><p>论文排版、作业必需工具，完美替代word，尤其略微复杂的带有公式和格式要求的，秒杀word无疑问。从数学建模课作业第一次在感冒时吃了过量的药什么命令都不会打一点儿搜一个到排版emaize报告，ANN作业，模式识别作业，各种汇报，统辅课的各种作业（后来转用配合R的Rmarkdown）</p><p>安装latex太简单了，网上教程很多，就不废话了，介绍几个编辑器：</p><h3 id="TexStudio"><a href="#TexStudio" class="headerlink" title="TexStudio"></a>TexStudio</h3><p>预览更容易，但是界面真的奇丑无比受不了，功能过于繁杂，不过有一些常用公式可以点击挺不错。</p><h3 id="TexWork"><a href="#TexWork" class="headerlink" title="TexWork"></a>TexWork</h3><p>最简陋也就最轻便，用的最多，坏处：什么都得自己手动输入，好像回到了九十年代</p><h3 id="VSCode"><a href="#VSCode" class="headerlink" title="VSCode"></a>VSCode</h3><p>最近老是路过一些人的电脑前发现他们在用VSCode，更神奇的是发现了几个人在用vscode排latex，觉得十分有趣，所以这部分是边学边写的，主要是为了自己学。</p><p>首先是为啥要用VSCode编辑LaTeX？已经有专用编辑器了，为什么要用通用编辑器？</p><p>因为不只写LaTeX，极简主义好。</p><p>相比传统『神级』编辑器(vim, emacs等)，新一代编辑器(VSCode, Atom)有何优势？</p><ul><li>效率：传统编辑器高</li><li>难度：新编辑器上手简单</li><li>颜值：新编辑器高</li></ul><p>顺便比对Atom和VSCode，其实Atom不是我不想用，而是它很神奇的在我的电脑上总是崩溃，让我留下了很差的印象，，</p><ul><li>支持：Atom插件多且好，包括且不局限于LaTeX范围，但是现在VSCode的插件也非常非常多了。</li><li>性能：VSCode好，启动快，不卡，Atom就不提了，，</li><li>难度：VSCode上手难度略高。</li></ul><p>VSCode所有的配置都在settings.json中，使用者会更加明白发生了什么，包括默认设置做了什么。</p><p>综上所述，VSCode的适合人群：同时具有编程和LaTeX需求，且对编辑器性能要求较高，有一定动手能力的人们。</p><h4 id="接下来配置环境"><a href="#接下来配置环境" class="headerlink" title="接下来配置环境"></a>接下来配置环境</h4><h5 id="测试样本：THUthesis"><a href="#测试样本：THUthesis" class="headerlink" title="测试样本：THUthesis"></a>测试样本：THUthesis</h5><p>用THUthesis测试一下，去GitHub下载最新版的<a href="https://github.com/xueruini/thuthesis/blob/master/main.tex" target="_blank" rel="noopener">thuthesis/main.tex at master · xueruini/thuthesis · GitHub</a>。就当是为写毕业论文预习一下吧，看起来THUthesis一直在改进，字体似乎也支持Mac了，虽然还是有一些坑要踩一踩，首先要注意在main.tex把以下选项选填到documentclass里<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">\documentclass[degree=master, tocarialchapter]&#123;thuthesis&#125;</span><br><span class="line"><span class="meta">%</span><span class="bash"> 选项</span></span><br><span class="line"><span class="meta">%</span><span class="bash">   degree=[bachelor|master|doctor|postdoctor], % 必选，学位类型</span></span><br><span class="line"><span class="meta">%</span><span class="bash">   secret,                % 可选（默认：关闭），是否有密级</span></span><br><span class="line"><span class="meta">%</span><span class="bash">   tocarialchapter,       % 可选（默认：关闭），章目录中使用黑体（这项表示同时打开下面两项）</span></span><br><span class="line"><span class="meta">%</span><span class="bash">   tocarialchapterentry,  % 可选（默认：关闭），单独控制章标题在目录中使用黑体</span></span><br><span class="line"><span class="meta">%</span><span class="bash">   tocarialchapterpage,   % 可选（默认：关闭），单独控制章页码在目录中使用黑体</span></span><br><span class="line"><span class="meta">%</span><span class="bash">   pifootnote,            % 可选（默认：关闭），页脚编号采用 pifont 字体符号，建议打开</span></span><br></pre></td></tr></table></figure></p><p>对于本科生来说，选填</p><figure class="highlight tex"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">\<span class="name">documentclass</span><span class="string">[degree=bachelor, tocarialchapter，pifootnote]</span><span class="string">&#123;thuthesis&#125;</span></span></span><br></pre></td></tr></table></figure><p>用Texworks可以编译通过，下面尝试VSCode</p><h5 id="默认TeX-Live套装和VSCode都装好了"><a href="#默认TeX-Live套装和VSCode都装好了" class="headerlink" title="默认TeX Live套装和VSCode都装好了"></a>默认TeX Live套装和VSCode都装好了</h5><p>安装 LaTeX Workshop 并配置用户设置</p><p>打开 VS Code 的插件页面，并搜索 LaTeX Workshop 插件，并选择安装。然后打开 “文件”-“首选项”-“设置”，或者直接使用 快捷键（Ctrl+逗号）打开。在搜索框中搜索“toolchain”，找到设置（新版是tool），在“右侧”粘贴配置的设置，保存，并退出 VS Code。</p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">        <span class="attr">"latex-workshop.latex.toolchain"</span>: [</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="attr">"command"</span>: <span class="string">"latexmk"</span>,</span><br><span class="line">            <span class="attr">"args"</span>: [</span><br><span class="line">            <span class="string">"-synctex=1"</span>,</span><br><span class="line">            <span class="string">"-interaction=nonstopmode"</span>,</span><br><span class="line">            <span class="string">"-file-line-error"</span>,</span><br><span class="line">            <span class="string">"-pdf"</span>,</span><br><span class="line">            <span class="string">"%DOC%"</span></span><br><span class="line">            ]</span><br><span class="line">        &#125;</span><br><span class="line">    ]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>快捷键<br><figure class="highlight armasm"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">1</span>. Ctrl+Alt+<span class="keyword">B </span>编译，<span class="keyword">Build </span>LaTeX project。</span><br><span class="line"><span class="number">2</span>. Ctrl+Alt+T 分栏预览生成的 PDF。</span><br></pre></td></tr></table></figure></p><p>放一些资源：</p><p><a href="https://www.jianshu.com/p/57f8d1e026f5" target="_blank" rel="noopener">VS Code 与 LaTeX 真乃天作之合 - 简书</a>，<a href="https://zhuanlan.zhihu.com/p/31883018" target="_blank" rel="noopener">配置VSCode为LaTeX集成开发环境(IDE) - 初级版</a>，<a href="https://blog.csdn.net/u010751257/article/details/54615563" target="_blank" rel="noopener">VS Code神用法之一：如何用VS Code在Mac环境下优雅地编写latex文档 - CSDN博客</a></p><h3 id="小插件"><a href="#小插件" class="headerlink" title="小插件"></a>小插件</h3><h4 id="Chrome"><a href="#Chrome" class="headerlink" title="Chrome"></a>Chrome</h4><p>Chrome下（也许其他浏览器也可以），在遇到网页中用mathjax编辑的公式时，右键可选show math as，点击tex commands，直接蹦出来latex命令（第一次发现是在cross validated上找统计推断一道题的解答时候发现的），谁用谁说好。</p><p><img src="http://i4.fuimg.com/640680/e7de5ce7e69e471b.png" alt="Markdown"></p><h4 id="Mathpix-Snipping-Tool"><a href="#Mathpix-Snipping-Tool" class="headerlink" title="Mathpix Snipping Tool"></a>Mathpix Snipping Tool</h4><p>最近发现的，基于先进的 <del>人工智能</del>图像识别技术，截图，识别出公式，然后提供latex 命令，谁用谁说好。</p><h1 id="论文管理"><a href="#论文管理" class="headerlink" title="论文管理"></a>论文管理</h1><p>排版部分的话上面已经讲得很清楚了，正儿八经的论文不用LaTeX写真的合适嘛？当然不合适，CS和EE应该没有人不用LaTeX了吧，也就公式少、不喜欢折腾的人还愿意面对着几十页word动不动卡死崩溃却不愿意学点新东西了。话说现在各种资源这么多，THU的本科论文博士论文LaTeX模板都有，拿过来往里面填充运行就行了，孟孟不会LaTeX都能在美赛的时候边问边摸索用美赛的LaTeX模板把文章排出来，可见现在成本真的很低了。</p><h2 id="Zotero"><a href="#Zotero" class="headerlink" title="Zotero"></a>Zotero</h2><p>论文管理强烈推荐zotero，貌似略小众，但是功能很强，索引很强，管理很细致，尤其是chrome插件简直无敌，什么都能推过去。</p><p><a href="https://zhuanlan.zhihu.com/p/31852030" target="_blank" rel="noopener">Zotero开箱指南</a></p><p>这个资源还是讲了很多东西的，包括一些插件，如何插文献，包括修改引文样式<a href="https://www.zotero.org/support/zh/styles" target="_blank" rel="noopener">zh:styles    Zotero Documentation</a>，<a href="https://zhuanlan.zhihu.com/p/31326415" target="_blank" rel="noopener">四步实现自定义Zotero参考文献格式</a>等等，支持word插入以及LaTeX插入。</p><p>比如<a href="https://retorque.re/zotero-better-bibtex/" target="_blank" rel="noopener">Better BibTeX for Zotero</a>，看起来可以更好管理LaTeX的文献插入，以后再折腾。</p><p><a href="https://zhuanlan.zhihu.com/reftools" target="_blank" rel="noopener">RefTools</a><br><a href="https://www.jianshu.com/p/68f0e4134b04" target="_blank" rel="noopener">参考文献管理——简易Zotero教程 - 简书</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;三年前当我还是个什么都不懂的小屁孩的时候，就听闻了高大上的LaTeX，还去听了图书馆某工科博士开的LaTeX入门讲解，听的天花乱坠毫无头绪，回去兴冲冲地装上，运行，出现bug，然后傻眼了，解决不了，我现在还记得当时在百度文科搜到一篇讲解文章，教人一点点开始，第一次生成出来一句话（真是愚蠢的却又不得不这么教的教程）。然后放弃了这个大坑。&lt;/p&gt;
&lt;p&gt;后来慢慢发现，其实就是个简单的工具而已，适应了基本的思维方式，命令熟练，特别是搜索能力变强可以快速debug之后，latex和markdown真的是完美的排版神器，尤其是在mac版的word经常崩而且公式极其丑陋且卡顿且长篇文章格式会让人非常恶心的情况下，我已经放弃了word，这一年除了老师要求必须交word的作业以及让人毫无兴趣的作业，基本都是用latex或markdown写的，这两个东西极大的方便了我的生活。&lt;/p&gt;
&lt;p&gt;在这里分享一下各种相关资源和知识&lt;/p&gt;
    
    </summary>
    
      <category term="techniques" scheme="http://james20141606.github.io/categories/techniques/"/>
    
      <category term="research" scheme="http://james20141606.github.io/categories/techniques/research/"/>
    
    
      <category term="techniques" scheme="http://james20141606.github.io/tags/techniques/"/>
    
      <category term="research" scheme="http://james20141606.github.io/tags/research/"/>
    
      <category term="paper" scheme="http://james20141606.github.io/tags/paper/"/>
    
      <category term="formatting" scheme="http://james20141606.github.io/tags/formatting/"/>
    
  </entry>
  
  <entry>
    <title>GitBook intro &amp; Grandpa&#39;s Bio</title>
    <link href="http://james20141606.github.io/2018/05/04/gitbook/"/>
    <id>http://james20141606.github.io/2018/05/04/gitbook/</id>
    <published>2018-05-03T18:01:38.000Z</published>
    <updated>2018-05-04T16:07:17.261Z</updated>
    
    <content type="html"><![CDATA[<h2 id="写书必备的工具"><a href="#写书必备的工具" class="headerlink" title="写书必备的工具"></a>写书必备的工具</h2><p>GitBook是写书的必备工具，之所以比LaTeX强，在于GitBook有强大的后盾，不仅可以本地排版，还可以挂在gitbook网站上，还可以做一些嵌入自己网页的设计(借助github pages之类的)。可以说是非常有用了，这次梳理了一下gitbook尤其是本地版的适用，以排版爷爷的回忆录为例，回忆录的一系列文章也更新在了博客和gitbook页面。</p><p>之前用过一些，比如：</p><ul><li>帮老板写过书里的一章(<a href="https://lulab.gitbooks.io/bioinfo/content/5%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%95%B4%E5%90%88----%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%92%8C%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/51.html" target="_blank" rel="noopener">5.1 eMaize Challenge  · 生物信息学实践</a>)</li><li>training课的一些notes(<a href="https://lulab.gitbooks.io/bioinfo-training-2018/content/" target="_blank" rel="noopener">Introduction · Bioinfo Training - Shared - 2018</a>)，后来又移到github的wiki上了(<a href="https://github.com/lulab/training/wiki" target="_blank" rel="noopener">Home · lulab/training Wiki · GitHub</a>)</li><li>还把爷爷的回忆录也用markdown整理了一遍放到gitbook上(<a href="https://james20141606.gitbooks.io/grandpa-autobiography/content/" target="_blank" rel="noopener">爷爷回忆录</a>)，（一文多用，之前也已经放到blog上面了），排版比word辛辛苦苦排来排去随便一弄就全乱了是强太多了。<a id="more"></a>最近gitbook正好做了一个重大的<a href="https://www.gitbook.com/" target="_blank" rel="noopener">改版</a>，看起来是不仅希望大家写书，而是做一个更全面的平台？试用了一下，相当自动化和友好，看起来是希望大家以后写各种API的手册之类的都用它来做好了。</li></ul><p><img src="http://i4.fuimg.com/640680/36e1abf41b49870e.png" alt="Markdown"></p><p>当然gitbook有很差的缺点，就是实在太慢太慢，翻墙后还是慢，还好它有本地版的，可以在本地编辑好再同步。以及大多数人markdown本来就是在本地编好的，很多程序员可能还是在用命令行版本的gitbook。</p><p>因为网页版的没有办法调整下载的pdf的中文字体，而我想给爷爷的回忆录的pdf字体修改一下，于是还是得用命令行版的gitbook。</p><h3 id="资源"><a href="#资源" class="headerlink" title="资源"></a>资源</h3><p>找到的几个教程<br><a href="http://www.chengweiyang.cn/gitbook/basic-usage/README.html" target="_blank" rel="noopener">使用 | GitBook 简明教程</a><br><a href="http://zhaoda.net/2015/11/09/gitbook-plugins/" target="_blank" rel="noopener">Gitbook 的使用和常用插件</a><br><a href="https://www.jianshu.com/p/f38d8ff999cb" target="_blank" rel="noopener">使用 Gitbook 打造你的电子书 - 简书</a></p><h2 id="step-by-step"><a href="#step-by-step" class="headerlink" title="step by step"></a>step by step</h2><p>gitbook 的基本用法非常简单，基本上就只有两步：</p><ul><li>使用 gitbook init 初始化书籍目录</li><li>使用 gitbook serve 编译书籍</li><li><h3 id="初级"><a href="#初级" class="headerlink" title="初级"></a>初级</h3></li></ul><p>下面将结合一个非常简单的实例，来介绍 gitbook 的基本用法。</p><h4 id="gitbook-init"><a href="#gitbook-init" class="headerlink" title="gitbook init"></a>gitbook init</h4><p>首先，创建如下目录结构：</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$ tree book/</span><br><span class="line">book/</span><br><span class="line">├── README.md</span><br><span class="line">└── SUMMARY.md</span><br><span class="line"><span class="number">0</span> directories, <span class="number">2</span> files</span><br></pre></td></tr></table></figure><p>README.md 和 SUMMARY.md 是两个必须文件，README.md 是对书籍的简单介绍：<br><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ cat book/README.md </span><br><span class="line"><span class="section"># README</span></span><br><span class="line">This is a book powered by [<span class="string">GitBook</span>](<span class="link">https://github.com/GitbookIO/gitbook</span>).</span><br></pre></td></tr></table></figure></p><p>SUMMARY.md 是书籍的目录结构。内容如下：</p><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">$ cat book/SUMMARY.md </span><br><span class="line"><span class="section"># SUMMARY</span></span><br><span class="line"><span class="bullet">* </span>[<span class="string">Chapter1</span>](<span class="link">chapter1/README.md</span>)</span><br><span class="line">  * [<span class="string">Section1.1</span>](<span class="link">chapter1/section1.1.md</span>)</span><br><span class="line">  * [<span class="string">Section1.2</span>](<span class="link">chapter1/section1.2.md</span>)</span><br><span class="line"><span class="bullet">* </span>[<span class="string">Chapter2</span>](<span class="link">chapter2/README.md</span>)</span><br></pre></td></tr></table></figure><p>创建了这两个文件后，使用 gitbook init，它会为我们创建 SUMMARY.md 中的目录结构。<br><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">$ cd book</span><br><span class="line">$ gitbook init</span><br><span class="line">$ tree</span><br><span class="line">.</span><br><span class="line">├── README.md</span><br><span class="line">├── SUMMARY.md</span><br><span class="line">├── chapter1</span><br><span class="line">│   ├── README.md</span><br><span class="line">│   ├── section1.<span class="number">1</span>.md</span><br><span class="line">│   └── section1.<span class="number">2</span>.md</span><br><span class="line">└── chapter2</span><br><span class="line">    └── README.md</span><br><span class="line"><span class="number">2</span> directories, <span class="number">6</span> files</span><br></pre></td></tr></table></figure></p><h4 id="gitbook-serve"><a href="#gitbook-serve" class="headerlink" title="gitbook serve"></a>gitbook serve</h4><p>书籍目录结构创建完成以后，就可以使用 gitbook serve 来编译和预览书籍了：</p><figure class="highlight gams"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="symbol">$</span> gitbook serve</span><br><span class="line"><span class="function"><span class="title">Press</span></span> CTRL+C to quit ...</span><br><span class="line"></span><br><span class="line">Live reload server started on port: <span class="number">35729</span></span><br><span class="line"><span class="function"><span class="title">Starting</span></span> build ...</span><br><span class="line">Successfully built!</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="title">Starting</span></span> server ...</span><br><span class="line">Serving book on http:<span class="comment">//localhost:4000</span></span><br></pre></td></tr></table></figure><p>gitbook serve 命令实际上会首先调用 gitbook build 编译书籍，完成以后会打开一个 web 服务器，监听在本地的 4000 端口。</p><p>现在，可以用浏览器打开 <a href="http://127.0.0.1:4000" target="_blank" rel="noopener">http://127.0.0.1:4000</a> 查看书籍的效果，如下图：</p><p><img src="http://i4.fuimg.com/640680/393c5bd3f8debba7.png" alt="Markdown"></p><p>现在，gitbook 为我们创建了书籍目录结构后，就可以向其中添加真正的内容了，文件的编写使用 markdown 语法，在文件修改过程中，每一次保存文件，gitbook serve 都会自动重新编译，所以可以持续通过浏览器来查看最新的书籍效果~</p><p>另外，用户还可以下载 gitbook 编辑器，做到所见即所得的编辑，如下图所示：</p><p><img src="http://i4.fuimg.com/640680/18392c162ed87862.png" alt="Markdown"></p><p>Gitbook editor 就是gitbook的本地版本，对于gitbook这个反人类的老年网站还是有意义的，当然MWeb之类的markdown编辑器也够用了，轻量级使用感觉MWeb足够好了。</p><h4 id="常用命令"><a href="#常用命令" class="headerlink" title="常用命令"></a>常用命令</h4><figure class="highlight jboss-cli"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#install</span></span><br><span class="line">sudo npm install gitbook-cli -g</span><br><span class="line">gitbook -V</span><br><span class="line">npm i ebook-convert  <span class="comment">#solve warning:InstallRequiredError: "ebook-convert" is not installed.</span></span><br><span class="line">gitbook init <span class="string">//</span>初始化目录文件</span><br><span class="line">gitbook serve</span><br><span class="line">gitbook <span class="keyword">help</span> <span class="string">//</span>列出gitbook所有的命令</span><br><span class="line">gitbook <span class="params">--help</span> <span class="string">//</span>输出gitbook-cli的帮助信息</span><br><span class="line">gitbook build <span class="string">//</span>生成静态网页</span><br><span class="line">gitbook serve <span class="string">//</span>生成静态网页并运行服务器</span><br><span class="line">gitbook build <span class="params">--gitbook=2</span>.0.1 <span class="string">//</span>生成时指定gitbook的版本, 本地没有会先下载</span><br><span class="line">gitbook <span class="keyword">ls</span> <span class="string">//</span>列出本地所有的gitbook版本</span><br><span class="line">gitbook ls-remote <span class="string">//</span>列出远程可用的gitbook版本</span><br><span class="line">gitbook fetch 标签/版本号 <span class="string">//</span>安装对应的gitbook版本</span><br><span class="line">gitbook update <span class="string">//</span>更新到gitbook的最新版本</span><br><span class="line">gitbook uninstall 2.0.1 <span class="string">//</span>卸载对应的gitbook版本</span><br><span class="line">gitbook build <span class="params">--log=debug</span> <span class="string">//</span>指定log的级别</span><br><span class="line">gitbook build <span class="params">--debug</span> <span class="string">//</span>输出错误信息</span><br><span class="line">gitbook pdf</span><br></pre></td></tr></table></figure><h3 id="高级"><a href="#高级" class="headerlink" title="高级"></a>高级</h3><p>只做到这些的话就没必要用命令行版的gitbook了，当然是为了做一些个性化定制，包括对book.json的配置以及使用插件。</p><h4 id="book-json"><a href="#book-json" class="headerlink" title="book.json"></a>book.json</h4><p>gitbook 在编译书籍的时候会读取书籍源码顶层目录中的 book.js 或者 book.json，这里以 book.json 为例，参考 <a href="https://github.com/GitbookIO/gitbook" target="_blank" rel="noopener">Gitbook 文档</a>可以知道，book.json 支持一系列配置：</p><p>经过一番折腾，往book.json里放置如下内容<br><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    <span class="attr">"author"</span>: <span class="string">"Binglin CHEN &lt;xp-chen14@mails.tsinghua.edu.cn&gt;"</span>,</span><br><span class="line">    <span class="attr">"description"</span>: <span class="string">"An Autobiography of Chen Binglin"</span>,</span><br><span class="line">    <span class="attr">"extension"</span>: <span class="literal">null</span>,</span><br><span class="line">    <span class="attr">"generator"</span>: <span class="string">"site"</span>,</span><br><span class="line">    <span class="attr">"links"</span>: &#123;</span><br><span class="line">        <span class="attr">"sharing"</span>: &#123;</span><br><span class="line">            <span class="attr">"all"</span>: <span class="literal">null</span>,</span><br><span class="line">            <span class="attr">"facebook"</span>: <span class="literal">null</span>,</span><br><span class="line">            <span class="attr">"google"</span>: <span class="literal">null</span>,</span><br><span class="line">            <span class="attr">"twitter"</span>: <span class="literal">null</span>,</span><br><span class="line">            <span class="attr">"weibo"</span>: <span class="literal">null</span></span><br><span class="line">        &#125;,</span><br><span class="line">        <span class="attr">"sidebar"</span>: &#123;</span><br><span class="line">            <span class="attr">"James's Blog"</span>: <span class="string">"http://james20141606.github.io"</span></span><br><span class="line">        &#125;</span><br><span class="line">    &#125;,</span><br><span class="line">    <span class="attr">"output"</span>: <span class="literal">null</span>,</span><br><span class="line">    <span class="attr">"pdf"</span>: &#123;</span><br><span class="line">        <span class="attr">"fontSize"</span>: <span class="number">20</span>,</span><br><span class="line">        <span class="attr">"footerTemplate"</span>: <span class="literal">null</span>,</span><br><span class="line">        <span class="attr">"headerTemplate"</span>: <span class="literal">null</span>,</span><br><span class="line">        <span class="attr">"margin"</span>: &#123;</span><br><span class="line">            <span class="attr">"bottom"</span>: <span class="number">36</span>,</span><br><span class="line">            <span class="attr">"left"</span>: <span class="number">62</span>,</span><br><span class="line">            <span class="attr">"right"</span>: <span class="number">62</span>,</span><br><span class="line">            <span class="attr">"top"</span>: <span class="number">36</span></span><br><span class="line">        &#125;,</span><br><span class="line">        <span class="attr">"pageNumbers"</span>: <span class="literal">false</span>,</span><br><span class="line">        <span class="attr">"paperSize"</span>: <span class="string">"b5"</span></span><br><span class="line">    &#125;,</span><br><span class="line">    <span class="attr">"plugins"</span>: [],</span><br><span class="line">    <span class="attr">"title"</span>: <span class="string">"Autobiography of Chen Binglin"</span>,</span><br><span class="line">    <span class="attr">"variables"</span>: &#123;&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>再次编译并预览<br><figure class="highlight armasm"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="symbol">gitbook</span> <span class="keyword">build</span></span><br><span class="line"><span class="keyword">gitbook </span>serve</span><br></pre></td></tr></table></figure></p><p>生成pdf<br>先下载calibre，当年捣鼓kindle就很熟悉的工具了，做一个软链接到bin下，然后生成pdf<br><figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">ln -s <span class="regexp">/Applications/</span>calibre.app<span class="regexp">/Contents/</span>MacOS<span class="regexp">/ebook-convert /u</span>sr<span class="regexp">/local/</span>bin</span><br><span class="line">gitbook pdf . mypdf.pdf</span><br></pre></td></tr></table></figure></p><p>By the way，顺便做了个快捷键，每次build再生成pdf还得等几十秒很麻烦<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">alias</span> gitbookbp=<span class="string">'gitbook build &amp;&amp; gitbook pdf .'</span></span><br></pre></td></tr></table></figure></p><h4 id="封面"><a href="#封面" class="headerlink" title="封面"></a>封面</h4><p>为了美观，还要做个封面</p><p>需要先安裝 Canvas<br><figure class="highlight mipsasm"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">brew </span><span class="keyword">install </span>pkg-<span class="built_in">config</span> cairo pango libpng <span class="keyword">jpeg </span>giflib</span><br><span class="line">npm <span class="keyword">install </span>canvas</span><br></pre></td></tr></table></figure></p><p>至 nodejs gitbook 安裝目錄下的 node_modules 目錄<br><figure class="highlight crystal"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd /usr/local/<span class="class"><span class="keyword">lib</span>/<span class="title">node_modules</span>/<span class="title">gitbook</span>-<span class="title">cli</span>/<span class="title">node_modules</span></span></span><br><span class="line">npm install gitbook-plugin-autocover</span><br></pre></td></tr></table></figure></p><p>这个方法行不通，尝试先在book.json添加autocover，用gitbook install 安装</p><p>依然报错</p><p>尝试：<br>modify the package.json of YOURBOOKDIR/node_modules/gitbook-plugin-autocover the line:<br><figure class="highlight 1c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">"canvas"</span>: <span class="string">"1.3.15"</span></span><br></pre></td></tr></table></figure></p><p>to<br><figure class="highlight leaf"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">"canvas-prebuilt": "latest" <span class="function"><span class="keyword">#</span><span class="params">(哪个蠢货干嘛不写<span class="variable">latest</span>？)</span></span></span><br></pre></td></tr></table></figure></p><p>and then run npm install in the gitbook-plugin-autocover</p><p>編輯 book.json<br><figure class="highlight prolog"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">"plugins"</span>: [<span class="string">"autocover"</span>]</span><br></pre></td></tr></table></figure></p><p><strong>使用方法</strong><br><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  <span class="attr">"output"</span>: <span class="literal">null</span>,</span><br><span class="line">  <span class="attr">"generator"</span>: <span class="string">"site"</span>,</span><br><span class="line">  <span class="attr">"title"</span>: <span class="string">"Autobiography of Chen Binglin"</span>,</span><br><span class="line">  <span class="attr">"plugins"</span>: [<span class="string">"autocover"</span>],</span><br><span class="line">  <span class="attr">"pluginsConfig"</span>: &#123;</span><br><span class="line">      <span class="attr">"autocover"</span>: &#123;</span><br><span class="line">          <span class="attr">"title"</span>: <span class="string">"Autobiography of Chen Binglin"</span>,</span><br><span class="line">          <span class="attr">"author"</span>: <span class="string">"Chen Binglin"</span>,</span><br><span class="line">          <span class="attr">"font"</span>: &#123;</span><br><span class="line">              <span class="attr">"size"</span>: <span class="literal">null</span>,</span><br><span class="line">              <span class="attr">"family"</span>: <span class="string">"Impact"</span>,</span><br><span class="line">              <span class="attr">"color"</span>: <span class="string">"#FFF"</span></span><br><span class="line">          &#125;,</span><br><span class="line">          <span class="attr">"size"</span>: &#123;</span><br><span class="line">              <span class="attr">"w"</span>: <span class="number">1800</span>,</span><br><span class="line">              <span class="attr">"h"</span>: <span class="number">2360</span></span><br><span class="line">          &#125;,</span><br><span class="line">          <span class="attr">"background"</span>: &#123;</span><br><span class="line">              <span class="attr">"color"</span>: <span class="string">"#09F"</span></span><br><span class="line">          &#125;</span><br><span class="line">      &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>當重新啟動 gitbook serve 以後會在書本的靜態網頁資料夾中發現 (_book/) 兩個檔案，cover.jpg、cover_small.jpg</p><p>发现封面效果极差，，还是用其他软件设计好了，，，<br><a href="https://www.canva.com/" target="_blank" rel="noopener">超级简单易用的平面设计软件 – Canva</a>这个看起来不错</p><h4 id="插件"><a href="#插件" class="headerlink" title="插件"></a>插件</h4><p>没有功夫仔细捣鼓了</p><p><strong>碎碎念：</strong></p><p>最后，其实可以在GitHub pages直接发布书籍，不过对我感觉实用性不大，毕竟每篇拆开当做博客显得更充实（主要是懒），附资源，有空可折腾，不过对于缺少时间的强迫症来说意义真不大。<a href="http://www.chengweiyang.cn/gitbook/github-pages/README.html" target="_blank" rel="noopener">发布到 GitHub Pages | GitBook 简明教程</a></p><p>最后的最后，放一下重新排版的书，B5，fontsize 20，老年人友好。<br><div class="row"><iframe src="https://drive.google.com/file/d/1hY-yITp5yhy5kjASxIR2cHCbkMOsJ7Dq/preview" style="width:100%; height:550px"></iframe></div></p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;写书必备的工具&quot;&gt;&lt;a href=&quot;#写书必备的工具&quot; class=&quot;headerlink&quot; title=&quot;写书必备的工具&quot;&gt;&lt;/a&gt;写书必备的工具&lt;/h2&gt;&lt;p&gt;GitBook是写书的必备工具，之所以比LaTeX强，在于GitBook有强大的后盾，不仅可以本地排版，还可以挂在gitbook网站上，还可以做一些嵌入自己网页的设计(借助github pages之类的)。可以说是非常有用了，这次梳理了一下gitbook尤其是本地版的适用，以排版爷爷的回忆录为例，回忆录的一系列文章也更新在了博客和gitbook页面。&lt;/p&gt;
&lt;p&gt;之前用过一些，比如：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;帮老板写过书里的一章(&lt;a href=&quot;https://lulab.gitbooks.io/bioinfo/content/5%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%95%B4%E5%90%88----%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%92%8C%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/51.html&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;5.1 eMaize Challenge  · 生物信息学实践&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;training课的一些notes(&lt;a href=&quot;https://lulab.gitbooks.io/bioinfo-training-2018/content/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Introduction · Bioinfo Training - Shared - 2018&lt;/a&gt;)，后来又移到github的wiki上了(&lt;a href=&quot;https://github.com/lulab/training/wiki&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Home · lulab/training Wiki · GitHub&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;还把爷爷的回忆录也用markdown整理了一遍放到gitbook上(&lt;a href=&quot;https://james20141606.gitbooks.io/grandpa-autobiography/content/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;爷爷回忆录&lt;/a&gt;)，（一文多用，之前也已经放到blog上面了），排版比word辛辛苦苦排来排去随便一弄就全乱了是强太多了。
    
    </summary>
    
      <category term="techniques" scheme="http://james20141606.github.io/categories/techniques/"/>
    
    
      <category term="techniques" scheme="http://james20141606.github.io/tags/techniques/"/>
    
      <category term="markdown" scheme="http://james20141606.github.io/tags/markdown/"/>
    
      <category term="syntax" scheme="http://james20141606.github.io/tags/syntax/"/>
    
      <category term="composing" scheme="http://james20141606.github.io/tags/composing/"/>
    
  </entry>
  
  <entry>
    <title>On Wittgenstein&#39;s Picture Theory of Meaning</title>
    <link href="http://james20141606.github.io/2018/05/03/tlp/"/>
    <id>http://james20141606.github.io/2018/05/03/tlp/</id>
    <published>2018-05-03T15:19:27.000Z</published>
    <updated>2018-05-03T15:30:24.771Z</updated>
    
    <content type="html"><![CDATA[<h4 id="My-first-naive-understanding-of-picture-theory"><a href="#My-first-naive-understanding-of-picture-theory" class="headerlink" title="My first naive understanding of picture theory:"></a>My first naive understanding of picture theory:</h4><p>I feel that I am always very slow at catching up with ideas in philosophers’ minds. In my child I am slow at understanding, now I always feel that I have more doubt about the ideas to restrict me to understand the ideas. So I try to understand the picture theory by reading some Chinese’s  thoughts. Then read more thoughts in English.</p><p><strong>PDF version</strong>:<br><div class="row"><iframe src="https://drive.google.com/file/d/1TIhBW_rq754AtFNdfnS0HfbjznXGSdvY/preview" style="width:100%; height:550px"></iframe></div></p><a id="more"></a><p>In Wittgenstein’s early thinking — from the TLP era — he conceived of the world in terms of facts rather than objects (from Russel’s Logical Atomism). Objects are simple: a ‘red ball’ is an object that has the property of ‘redness’ and an assortment of properties that are associated with ‘ball-ness’. Facts, by contrast, are states of affairs that involve relationships between objects. e.g.: A ‘red ball’ is an object,<br>‘John sees a red ball’ is a fact, drawing a relationship between an object ‘John’ and an object ‘red ball.’<br>Facts are carried by propositions in language — the proposition “John sees a red ball” conveys the fact that John sees a red ball — and propositions are pictures of the state of affairs that is being conveyed. Like visual pictures, propositions can be created that are more detailed or less detailed, that highlight some relationships and downplay or omit others, that present idealized forms of facts or realistic forms. Thus, I can say:</p><ul><li>“James sees a red ball”</li><li>“The boy James sees a bouncing red playground ball”</li><li>“James sees the redness of the ball in the setting sun”</li></ul><p>These are three ways of drawing slightly different logical pictures of the basic fact in point 1. Wittgenstein uses this as a way of taking about the relationship of language to the real world; it is a correspondence theory of language.</p><p>On Wittgenstein’s view, the world consists entirely of facts. (Tractatus 1.1), <em>The world is the totality of facts, not of things.</em> Human beings are aware of the facts by virtue of our mental representations or thoughts, which are most fruitfully understood as picturing the way things are. (Tractatus 2.1),<em>We make to ourselves pictures of facts.</em> These thoughts are, in turn, expressed in propositions, whose form indicates the position of these facts within the nature of reality as a whole and whose content presents the truth-conditions under which they correspond to that reality. (Tractatus 4),<em>The thought is the significant proposition.</em> Everything that is true—that is, all the facts that constitute the world—can in principle be expressed by atomic sentences. Imagine a comprehensive list of all the true sentences. They would picture all of the facts there are, and this would be an adequate representation of the world as a whole.</p><h4 id="Is-it-a-trick-to-use-picture"><a href="#Is-it-a-trick-to-use-picture" class="headerlink" title="Is it a trick to use picture?"></a>Is it a trick to use picture?</h4><p>Wittgenstein argues that propositions, as facts are images, just like musical scores are images of music or letters are images of spoken language. In other words, from an image point of view, the propositional relationship to reality, the relationship between music scores and music, and the relationship between pronunciation symbols and spoken language are exactly the same.   They are all images. And, for this kind of image relationship, it is understandable by recalling hieroglyphs. Because the alphabetic characters evolved from hieroglyphics, as an image, its essence is still hieroglyphs. This view is equally suitable for music score as an image and proposition as an image. Thus, Wittgenstein’s logical image theory is essentially the same as the pictographic character. What is different is that Wittgenstein uses the word “hieroglyphs” in the original sense, not metaphorically.</p><p>To Wittgenstein, the proposition only shows the fact that the mark is just a mark object, just like a pictogram representing the object that symbolizes it. In simple terms, the image is only a symbolic relationship, not a reflection relationship. On the other hand, images depict their own objects from outside as if the hieroglyphs describe the shape of their own objects. So between the proposition and the fact, the sign and the fact are only similar in appearance. Image-based theory is a kind of pictographic theory. Therefore, Wittgenstein’s logic image theory has the essential characteristics of formalism and agnosticism. </p><p>It suddenly occurs to me that Wittgenstein is really a genius by defining the relationship of proposition and fact by pictures. It seems that he always has the most genius institution to simplify complex and convoluted mysterious. He tries to solve a very very hard problem involving language (and mind) by turning into a picture/plot/image problem. Let’s just have a look about their differences using the view nearly 100 years ago:</p><h5 id="AI’s-view"><a href="#AI’s-view" class="headerlink" title="AI’s view:"></a>AI’s view:</h5><h2 id="——"><a href="#——" class="headerlink" title="——"></a>——</h2><p>We should talk about some details of AI’s achievement and problems to understand Wittgenstein’s insight.</p><p>AI has a long history, but the event made AI back to public’s eyes is AlphaGO beats the one of the best Go players in spring in 2016. But the AI revives happened a little earlier in the academic area. At around 2012 a brand new method called deep learning finally became mature and beats all the other models in computer vision competition. The competition aims at recognize and classify the picture. For example: a child at 3 years old can recognize a painting and point out that there is a cat on it. But it is<br>impossible for computers to do so. But with the deep learning model, the computer can do the amazing work just like human being. Later more and more work and achievements got into public’s view. We can see AI everywhere: automatic driving, face recognition and machine translation. And the concerning about AI will do some harmful things like replace human and kill human when they obtain the intelligence.</p><p>So will they? I’d like to talk about another famous example called Turing test. It is proposed by Allen Turing to accurately definite the actual artificial intelligence. It can be simply said like: a man is outside of a room, the room may contain either a human or a computer, but the man outside of the room doesn’t know. Then they will communicate through a screen. And after some kind of time(maybe 25 mins), the man is asked to tell if the man in the room is a actual man or a computer. If the computer cheated the man and made him think that it is ‘a man or woman’, then we will say the computer passed the Turing test. Unfortunately there is no computer passed the Turing test yet(though there are plenty of them claims they had, but they use some tricks which is no admitted by the other scientists.)</p><p>So what can we learn from it? We can know that Turing believes that language is the ultimate way to examine the actual intelligence of the a so called AI. Actually today’s deep learning model has gained some name they don’t deserve. The public have many wrong ideas about it, it is actually more like the artificial idiot. It can solve so little problems. One of the hardest it still did very bad is called natural language processing(NLP). It is another very hard yet interesting area. Many scientists aims to develop good models to recognize the meaning of sentence and passage, but it is very hard. Many big companies like Google and Microsoft also hire many experts aims to solve the hardest question in AI area.</p><p>We can use some data to see it more clearly: many deep learning models can solve the image recognition problem pretty good, achieving the accuracy at around 99% which is better than human. But the NLP problem is much harder. A dataset  provided by Stanford University containing many sentences for sentiment analysis(classify the sentences into several kinds of categories according to their sentimental meanings). The state of art model on these dataset can only achieve ~50% accuracy. Which is not satisfying. Actually even human can’t do it very well comparing to image recognition. Part of the reason maybe the image contains much more information than the single sentence. And a sentence contains so many levels of meanings which is even very hard for human to recognize and understand.</p><p>So in order to achieve actual intelligence. To make AI more like human intelligence. The best practice and test is natural language problem. The language is the representation of intelligence. If we want to know about intelligence, we should know about language more. Solving language problem is surely not the language itself, the experience, custom, belief, philosophy behind it makes it really complex.</p><p>So language is really amazing and interesting and hard to understand and use. We can hope that one day the ‘AI’ gains the philosophy behind it. But I think it will happen after we gain more knowledge of language and our brain.</p><h2 id="——-1"><a href="#——-1" class="headerlink" title="——"></a>——</h2><p>That is a little long thoughts about the AI’s limits. Which is really inspiring in our situation: Amazingly, machine are really good at image recognition and classification, they even get better performance than human these days. Lecun, one of the great experts in deep learning, mimic cat’s visual system in machine and make the AI as good as man in image tasks. But machines are really, really bad at natural language processing(<strong>NLP</strong>) tasks. We all admit that NLP is much harder than computer vision problems. Recently many researchers do some interesting application using computer vision related model(<strong>CNN</strong>) to solve many non-visual tasks, including <strong>NLP</strong>! It is quite inspiring. Since we have lots of experience and power in solving computer vision problem, the CNN model is powerful especially at extracting features hidden under everything(including picture and languages), it is natural to use image-related methods to solve language problems. After thinking about that, I am thrilled that Wittgenstein also use the same <strong>trick</strong>. I think he must understand how weak we are facing the language problems, but we are really good solving problems related to images. They have more features, they can extend our understanding in a more comfortable way. Especially for us ordinary people with much less intuition than him, picture the proposition as image is really helpful. It seems that Wittgenstein finds a powerful, logical tools to answer how people’s understanding can be expressed and the inherent relationship between the proposition and reality</p><h3 id="References"><a href="#References" class="headerlink" title="References"></a>References</h3><p>1 <a href="http://www.philosophypages.com/hy/6s.htm#trac" target="_blank" rel="noopener">Ludwig Wittgenstein: Analysis of Language</a><br>2 Terrone, E. (2013). Wittgenstein’s Picture Theory of Pictures. Aisthesis. Pratiche, Linguaggi E Saperi Dell’Estetico, 6(1), 275-290. doi:10.13128/Aisthesis-12852<br>3 <a href="https://en.wikipedia.org/wiki/Picture_theory_of_language" target="_blank" rel="noopener">Picture theory of language - Wikipedia</a></p><h2 id="Additional-Basic-data-mining-of-TLP"><a href="#Additional-Basic-data-mining-of-TLP" class="headerlink" title="Additional: Basic data mining of TLP"></a>Additional: Basic data mining of TLP</h2><p>I also spent some time analyzing the context of TLP using python:</p><h3 id="prepare-data"><a href="#prepare-data" class="headerlink" title="prepare data"></a>prepare data</h3><figure class="highlight xl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> jieba</span><br><span class="line"><span class="keyword">import</span> os, codecs  </span><br><span class="line">from collections <span class="keyword">import</span> Counter  </span><br><span class="line"><span class="keyword">import</span> matplotlib.pylab <span class="keyword">as</span> plt</span><br><span class="line">plt.style.use(<span class="string">'ggplot'</span>)</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> matplotlib</span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"><span class="keyword">with</span> codecs.open(<span class="string">'text.txt'</span>, <span class="string">'r'</span>, <span class="string">'utf8'</span>) <span class="keyword">as</span> f:  </span><br><span class="line">    txt = f.read()</span><br><span class="line">def get_words(txt):  </span><br><span class="line">    seg_list = jieba.cut(txt)  </span><br><span class="line">    c = Counter()  </span><br><span class="line">    <span class="keyword">for</span> x <span class="built_in">in</span> seg_list:  </span><br><span class="line">        <span class="keyword">if</span> len(x)&gt;<span class="number">1</span> <span class="built_in">and</span> x != <span class="string">'\r\n'</span>:  </span><br><span class="line">            c[x] += <span class="number">1</span>  </span><br><span class="line">    return c  # c.most_common(<span class="number">100</span>)</span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:  </span><br><span class="line">    <span class="keyword">with</span> codecs.open(<span class="string">'text.txt'</span>, <span class="string">'r'</span>, <span class="string">'utf8'</span>) <span class="keyword">as</span> f:  </span><br><span class="line">        txt = f.read()  </span><br><span class="line">    a = get_words(txt)</span><br><span class="line">dat = a.most_common(<span class="number">1000</span>)</span><br><span class="line">countlist = np.ndarray([<span class="number">1000</span>,<span class="number">2</span>]).astype(<span class="string">'str'</span>)</span><br><span class="line"><span class="keyword">for</span> i <span class="built_in">in</span> range(<span class="number">1000</span>):</span><br><span class="line">    countlist[i][<span class="number">0</span>] = dat[i][<span class="number">0</span>]</span><br><span class="line">    countlist[i][<span class="number">1</span>] = dat[i][<span class="number">1</span>]</span><br><span class="line">wordlist = np.array(re.split(<span class="string">' |; |, |\*|\n'</span>,txt))</span><br><span class="line">index_54 = [<span class="number">9</span>,<span class="number">11</span>,<span class="number">13</span>,<span class="number">14</span>,<span class="number">16</span>,<span class="number">17</span>,<span class="number">21</span>,<span class="number">23</span>,<span class="number">25</span>,<span class="number">26</span>,<span class="number">28</span>,<span class="number">29</span>,<span class="number">32</span>,<span class="number">33</span>,<span class="number">35</span>,<span class="number">37</span>,<span class="number">39</span>,<span class="number">40</span>,<span class="number">43</span>,<span class="number">44</span>,<span class="number">45</span>,<span class="number">51</span>,<span class="number">53</span>,<span class="number">61</span>,<span class="number">63</span>,<span class="number">64</span>,<span class="number">66</span>,<span class="number">67</span>,<span class="number">69</span>,<span class="number">70</span>,<span class="number">72</span>,<span class="number">76</span>,<span class="number">77</span>,<span class="number">80</span>,<span class="number">81</span>,<span class="number">82</span>,<span class="number">84</span>,<span class="number">85</span>,<span class="number">86</span>,<span class="number">88</span>,<span class="number">89</span>,<span class="number">90</span>,<span class="number">91</span>,<span class="number">95</span>,<span class="number">96</span>,<span class="number">98</span>,<span class="number">99</span>,<span class="number">101</span>,<span class="number">103</span>,<span class="number">105</span>,<span class="number">106</span>,<span class="number">107</span>,<span class="number">109</span>,<span class="number">110</span>]</span><br><span class="line">countlist_ = []</span><br><span class="line"><span class="keyword">for</span> i <span class="built_in">in</span> range(<span class="number">1000</span>):</span><br><span class="line">    countlist_.append(countlist[i,<span class="number">0</span>]+<span class="string">': '</span>+countlist[i,<span class="number">1</span>])</span><br><span class="line">pd.DataFrame(np.array(countlist_)[:<span class="number">200</span>].reshape(<span class="number">20</span>,<span class="number">10</span>))</span><br></pre></td></tr></table></figure><h3 id="words-counts-most-frequent"><a href="#words-counts-most-frequent" class="headerlink" title="words counts, most frequent"></a>words counts, most frequent</h3><p><img src="http://i4.fuimg.com/640680/cf8ade4c4e8d7656.png" alt="Markdown"></p><figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">count</span> = <span class="number">27</span></span><br><span class="line">fig,ax=plt.subplots(<span class="number">1</span>,figsize=(<span class="number">20</span>,<span class="number">10</span>))</span><br><span class="line">ax.bar(<span class="built_in">range</span>(<span class="built_in">count</span>),countlist[:<span class="built_in">count</span>,<span class="number">1</span>].astype(<span class="string">'int'</span>))</span><br><span class="line">ax.set_xticks(<span class="built_in">range</span>(<span class="built_in">count</span>))</span><br><span class="line">ax.set_xticklabels(countlist[:<span class="built_in">count</span>,<span class="number">0</span>])</span><br><span class="line">ax.set_title(<span class="string">'most frequent words'</span>)</span><br></pre></td></tr></table></figure><p><img src="http://i4.fuimg.com/640680/403967dbf8791a89.png" alt="Markdown"></p><h3 id="most-frequent-words-with-specific-meaning"><a href="#most-frequent-words-with-specific-meaning" class="headerlink" title="most frequent words with specific meaning"></a>most frequent words with specific meaning</h3><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">count = <span class="number">30</span></span><br><span class="line">index = index_54[:count]</span><br><span class="line">fig,ax=plt.subplots(<span class="number">1</span>,figsize=(<span class="number">20</span>,<span class="number">10</span>))</span><br><span class="line">barlist = plt.bar(range(count),countlist[index,<span class="number">1</span>].astype(<span class="string">'int'</span>))</span><br><span class="line"><span class="keyword">for</span> <span class="selector-tag">i</span> <span class="keyword">in</span> np.arange(<span class="number">0</span>,<span class="number">2</span>):</span><br><span class="line">    barlist[i].set_color(<span class="string">'r'</span>)</span><br><span class="line"><span class="keyword">for</span> <span class="selector-tag">i</span> <span class="keyword">in</span> np.arange(<span class="number">2</span>,<span class="number">5</span>):</span><br><span class="line">    barlist[i].set_color(<span class="string">'b'</span>)</span><br><span class="line"><span class="keyword">for</span> <span class="selector-tag">i</span> <span class="keyword">in</span> np.arange(<span class="number">5</span>,<span class="number">12</span>):</span><br><span class="line">    barlist[i].set_color(<span class="string">'g'</span>)</span><br><span class="line"><span class="keyword">for</span> <span class="selector-tag">i</span> <span class="keyword">in</span> np.arange(<span class="number">12</span>,<span class="number">26</span>):</span><br><span class="line">    barlist[i].set_color(<span class="string">'m'</span>)</span><br><span class="line"><span class="keyword">for</span> <span class="selector-tag">i</span> <span class="keyword">in</span> np.arange(<span class="number">26</span>,count):</span><br><span class="line">    barlist[i].set_color(<span class="string">'y'</span>)</span><br><span class="line">ax.set_xticks(range(count))</span><br><span class="line">ax.set_xticklabels(countlist[index,<span class="number">0</span>],fontsize=<span class="number">15</span>)</span><br><span class="line">ax.set_title(<span class="string">'Most Frequent Words in TLP'</span>,fontsize=<span class="number">30</span>)</span><br><span class="line">plt.setp(ax.get_xticklabels(), rotation=<span class="number">30</span>, horizontalalignment=<span class="string">'right'</span>)</span><br><span class="line">fig.tight_layout()</span><br></pre></td></tr></table></figure><p><img src="http://i4.fuimg.com/640680/1488a2a32bc0e559.png" alt="Markdown"></p><h3 id="words-frequency-by-chapter"><a href="#words-frequency-by-chapter" class="headerlink" title="words frequency by chapter"></a>words frequency by chapter</h3><figure class="highlight fortran"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">chapterind = [<span class="number">460</span>,<span class="number">559</span>,<span class="number">2285</span>,<span class="number">4855</span>,<span class="number">10438</span>,<span class="number">17882</span>,<span class="number">22970</span>]</span><br><span class="line"><span class="keyword">namelist</span> = countlist[<span class="built_in">index</span>,<span class="number">0</span>]</span><br><span class="line">def count_frequent(chap,<span class="built_in">count</span>):</span><br><span class="line">    freqlist =[]</span><br><span class="line">    <span class="keyword">if</span> chap &lt;<span class="number">6</span>:</span><br><span class="line">        for i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">count</span>):</span><br><span class="line">            freqlist.append(np.<span class="keyword">where</span>(wordlist[chapterind[chap]:chapterind[chap+<span class="number">1</span>]] ==<span class="keyword">namelist</span>[i])[<span class="number">0</span>].<span class="built_in">shape</span>[<span class="number">0</span>])</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        for i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">count</span>):</span><br><span class="line">            freqlist.append(np.<span class="keyword">where</span>(wordlist[chapterind[chap]:] ==<span class="keyword">namelist</span>[i])[<span class="number">0</span>].<span class="built_in">shape</span>[<span class="number">0</span>])</span><br><span class="line">    <span class="keyword">return</span> np.array(freqlist)</span><br><span class="line">freq_var = np.ndarray([<span class="number">7</span>,<span class="number">30</span>])</span><br><span class="line">for i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">7</span>):</span><br><span class="line">    freq_var[i] = count_frequent(i,<span class="number">30</span>)</span><br><span class="line">from sklearn.preprocessing <span class="keyword">import</span> MinMaxScaler</span><br><span class="line">scaler = MinMaxScaler()</span><br><span class="line">transformed = scaler.fit_transform(freq_var)</span><br><span class="line">fig,ax=plt.subplots(<span class="number">1</span>,figsize =(<span class="number">10</span>,<span class="number">10</span>))</span><br><span class="line">ax.matshow(transformed.T ,cmap =<span class="string">'jet'</span>)</span><br><span class="line">ax.set_title(<span class="string">'30 key words fluctuation in 7 chapters'</span>)</span><br><span class="line">ax.set_xticks(<span class="built_in">range</span>(<span class="number">7</span>))</span><br><span class="line">ax.set_yticks(<span class="built_in">range</span>(<span class="number">30</span>))</span><br><span class="line">ax.set_yticklabels(<span class="keyword">namelist</span>)</span><br></pre></td></tr></table></figure><p><strong>You can see the words variation in different chapters</strong>. For example, sign appears most in chapter 3, the chapter talking about the logical picture of facts.  In another example you can see picture appears most in chapter 2. Since chapter four is the longest chapter, most words appears more times in it.</p><p><img src="http://i4.fuimg.com/640680/0caf8e3a241751bd.png" alt="Markdown"></p><figure class="highlight stata"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">fig,ax=plt.subplots(1,figsize =(20,10))</span><br><span class="line">#ax.<span class="keyword">plot</span>(freq_var[:,:10])</span><br><span class="line"><span class="keyword">count</span> =10</span><br><span class="line"><span class="keyword">for</span> x,y <span class="keyword">in</span> <span class="keyword">zip</span>(freq_var[:,:<span class="keyword">count</span>].T,namelist[:<span class="keyword">count</span>]):</span><br><span class="line">    plt.<span class="keyword">plot</span>(x,<span class="keyword">label</span> =y)</span><br><span class="line">plt.title(str(<span class="keyword">count</span>)+' key words fluctuation <span class="keyword">in</span> 24 chapters')</span><br><span class="line">plt.legend()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p>It is another plot stating the same fact as the previous heatmap.</p><p><img src="http://i4.fuimg.com/640680/dc2b19befbe552ac.png" alt="Markdown"></p><h3 id="Calculate-different-words’-distances"><a href="#Calculate-different-words’-distances" class="headerlink" title="Calculate different words’ distances"></a>Calculate different words’ distances</h3><p>I also think about how to depict different words’ relationship. One easiest way I think of is calculating two words minimum distance in the book whenever they appears. <strong>For example:</strong><br><figure class="highlight nix"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">def calculate_distance(ind1,ind2):</span><br><span class="line">    <span class="attr">pos1</span> = np.where(<span class="attr">wordlist==namelist[ind1])[0]</span></span><br><span class="line">    <span class="attr">pos2</span> = np.where(<span class="attr">wordlist==namelist[ind2])[0]</span></span><br><span class="line">    num1 ,<span class="attr">num2</span> = pos1.shape[<span class="number">0</span>],pos2.shape[<span class="number">0</span>]</span><br><span class="line">    <span class="keyword">if</span> num1&gt;num2:</span><br><span class="line">        <span class="attr">small</span> = num2</span><br><span class="line">        <span class="attr">large</span> = num1</span><br><span class="line">        <span class="attr">lararr</span> = pos1</span><br><span class="line">        <span class="attr">smarr</span> = pos2</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="attr">small</span> = num1</span><br><span class="line">        <span class="attr">large</span> = num2</span><br><span class="line">        <span class="attr">lararr</span> = pos2</span><br><span class="line">        <span class="attr">smarr</span> = pos1</span><br><span class="line">    <span class="attr">disarr</span> = np.ndarray([small,large])  <span class="comment">#each line calculate the small set's ith word's and large set's every words distance</span></span><br><span class="line">    <span class="attr">arr1=</span> np.repeat(smarr,large).reshape(-<span class="number">1</span>,large)</span><br><span class="line">    <span class="attr">arr2=</span> np.repeat(lararr,small).reshape(-<span class="number">1</span>,small).T</span><br><span class="line">    <span class="attr">mindis</span> = np.min(np.abs(arr2-arr1),<span class="attr">axis=1)</span></span><br><span class="line">    return mindis</span><br><span class="line">def draw_dist_count(ind1,ind2):</span><br><span class="line">    fig,<span class="attr">ax=plt.subplots(1,figsize=(20,10))</span></span><br><span class="line">    ax.bar(range(calculate_distance(<span class="number">0</span>,<span class="number">1</span>).shape[<span class="number">0</span>]),calculate_distance(<span class="number">0</span>,<span class="number">1</span>),<span class="attr">color='g')</span></span><br><span class="line">    ax.set_title('Minimum Distance of '+namelist[ind1]+<span class="string">" and "</span>+namelist[ind2])</span><br><span class="line">draw_dist_count(<span class="number">0</span>,<span class="number">1</span>)</span><br></pre></td></tr></table></figure></p><p>It is the minimum distance of two words: <strong>picture and objects</strong>, we can see at the first half of the book, the two words have closer distance, which means they have higher chance appearing together or nearby. But in the last half, apparently the author didn’t arrange them  appearing together.</p><p><img src="http://i4.fuimg.com/640680/dd3d1fca192d56eb.png" alt="Markdown"></p><p>We can also see one word’s distance with many other words: <strong>logical and other words</strong><br><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">fig,ax=plt.subplots(<span class="number">4</span>,<span class="number">2</span>,figsize=(<span class="number">20</span>,<span class="number">20</span>))</span><br><span class="line"><span class="keyword">for</span> <span class="selector-tag">i</span> <span class="keyword">in</span> range(<span class="number">4</span>):</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">2</span>):</span><br><span class="line">        ax[<span class="selector-tag">i</span>,j].bar(range(calculate_distance(<span class="number">2</span>*i+j,<span class="number">8</span>)<span class="selector-class">.shape</span>[<span class="number">0</span>]),calculate_distance(<span class="number">2</span>*i+j,<span class="number">8</span>))</span><br><span class="line">        ax[<span class="selector-tag">i</span>,j].set_title(<span class="string">'Minimum Distance of '</span>+namelist[<span class="number">9</span>]+<span class="string">" and "</span>+namelist[<span class="number">2</span>*i+j])</span><br></pre></td></tr></table></figure></p><p><img src="http://i4.fuimg.com/640680/88bdbaae024ff88c.png" alt="Markdown"></p><p>We can also overview one word’s distance(relationship) with others using hist or boxplot<br><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">fig,ax=plt.subplots(<span class="number">4</span>,<span class="number">2</span>,figsize=(<span class="number">20</span>,<span class="number">20</span>))</span><br><span class="line"><span class="keyword">for</span> <span class="selector-tag">i</span> <span class="keyword">in</span> range(<span class="number">4</span>):</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">2</span>):</span><br><span class="line">        ax[<span class="selector-tag">i</span>,j].hist(calculate_distance(<span class="number">0</span>,<span class="number">1</span>+<span class="number">2</span>*i+j),bins =<span class="number">50</span>,<span class="attribute">color</span>=<span class="string">'b'</span>,alpha=<span class="number">0.4</span>)</span><br><span class="line">        ax[<span class="selector-tag">i</span>,j].set_title(<span class="string">'Minimum Distance of '</span>+namelist[<span class="number">0</span>]+<span class="string">" and "</span>+namelist[<span class="number">1</span>+<span class="number">2</span>*i+j])</span><br></pre></td></tr></table></figure></p><p><img src="http://i4.fuimg.com/640680/0ff4b00256b64e7a.png" alt="Markdown"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">dist_data = &#123;&#125;</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> np.arange(<span class="number">1</span>,<span class="number">20</span>):</span><br><span class="line">    dist_data[i] = calculate_distance(<span class="number">0</span>,i)</span><br><span class="line">dataframe_dxp = pd.concat((pd.DataFrame(&#123;namelist[i]:dist_data[i]&#125;) <span class="keyword">for</span> i <span class="keyword">in</span> np.arange(<span class="number">1</span>,<span class="number">20</span>)),axis=<span class="number">1</span>)</span><br><span class="line"><span class="keyword">import</span> seaborn <span class="keyword">as</span> sns</span><br><span class="line">fig, ax = plt.subplots(figsize=(<span class="number">100</span>,<span class="number">20</span>))</span><br><span class="line">sns.boxplot(data =dataframe_dxp,ax=ax,boxprops=dict(alpha=<span class="number">.5</span>),color=<span class="string">'g'</span>)</span><br><span class="line">ax.set_title(<span class="string">u'Proposition and others'</span>,fontsize=<span class="number">120</span>)</span><br><span class="line">ax.set_xticks(range(<span class="number">19</span>))</span><br><span class="line">ax.set_xticklabels(namelist[<span class="number">1</span>:<span class="number">20</span>],fontsize=<span class="number">80</span>)</span><br><span class="line">plt.setp(ax.get_xticklabels(), rotation=<span class="number">30</span>, horizontalalignment=<span class="string">'right'</span>)</span><br><span class="line">fig.tight_layout()</span><br></pre></td></tr></table></figure><p><img src="http://i4.fuimg.com/640680/d2e029996f53858b.png" alt="Markdown"></p><p>That’s all about my basic data mining and analysis of TLP. It took me a lot of time writing codes to count and plot. I believe in the future we can use data mining, natural language processing to do analysis more automatically. What’s more, it may do some really exciting and serious study instead of my basic plotting. We may use the powerful so called “artificial intelligence” tool to mine thousands of books and materials to analyze the hidden ideas which is omitted by human due to our limitation of memorizing. Thus we can understand more about the author and the ideas behind the book.</p>]]></content>
    
    <summary type="html">
    
      &lt;h4 id=&quot;My-first-naive-understanding-of-picture-theory&quot;&gt;&lt;a href=&quot;#My-first-naive-understanding-of-picture-theory&quot; class=&quot;headerlink&quot; title=&quot;My first naive understanding of picture theory:&quot;&gt;&lt;/a&gt;My first naive understanding of picture theory:&lt;/h4&gt;&lt;p&gt;I feel that I am always very slow at catching up with ideas in philosophers’ minds. In my child I am slow at understanding, now I always feel that I have more doubt about the ideas to restrict me to understand the ideas. So I try to understand the picture theory by reading some Chinese’s  thoughts. Then read more thoughts in English.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;PDF version&lt;/strong&gt;:&lt;br&gt;

	&lt;div class=&quot;row&quot;&gt;
		&lt;iframe src=&quot;https://drive.google.com/file/d/1TIhBW_rq754AtFNdfnS0HfbjznXGSdvY/preview&quot; style=&quot;width:100%; height:550px&quot;&gt;&lt;/iframe&gt;
	&lt;/div&gt;


&lt;/p&gt;
    
    </summary>
    
      <category term="thoughts" scheme="http://james20141606.github.io/categories/thoughts/"/>
    
    
      <category term="philosophy" scheme="http://james20141606.github.io/tags/philosophy/"/>
    
      <category term="Wittgenstein" scheme="http://james20141606.github.io/tags/Wittgenstein/"/>
    
      <category term="thoughts" scheme="http://james20141606.github.io/tags/thoughts/"/>
    
  </entry>
  
  <entry>
    <title>记一下人生第一次GRE</title>
    <link href="http://james20141606.github.io/2018/04/28/gre/"/>
    <id>http://james20141606.github.io/2018/04/28/gre/</id>
    <published>2018-04-28T11:49:36.000Z</published>
    <updated>2018-04-29T02:28:36.118Z</updated>
    
    <content type="html"><![CDATA[<p>记录一下2018.4.28一战裸考GRE的经历，verbal 154，quant 164，总体来看是惊喜中带着遗憾，首先语言部分154分？我可是单词书都没翻过，GRE单词根本没空背，题型都不知道，结果语言部分竟然可以一次过？简直像托福一样是纸老虎嘛，但是quant部分考的得我无地自容，做题的时候不屑于初中水平的考题，都是很快秒杀然后提前提交，然而数学分出来之后让我目瞪口呆，竟然扣了六分，，如果数学部分考满分的话，岂不是一次就过了GRE？简短地总结一下，一场因为疲劳和微恙略感麻木的带着惊喜和遗憾的考试。</p><a id="more"></a><h2 id="考前"><a href="#考前" class="headerlink" title="考前"></a>考前</h2><h3 id="预期时间"><a href="#预期时间" class="headerlink" title="预期时间"></a>预期时间</h3><p>一年前听林祖迪说了一句他暑研回来才考的托福和GRE，我想想蛮有道理，早考还得专门准备，在我看来是吃亏大了， 我理想的状态是一分钟别花。</p><h4 id="由托福来的经验"><a href="#由托福来的经验" class="headerlink" title="由托福来的经验"></a>由托福来的经验</h4><p>当年年幼无知，托福还报了个新东方班，现在想想心疼不已，花了十六天听四个老师瞎扯讲故事的风格真是贻害无穷，学到的东西寥寥，而且后来我认清了一个道理：早点学太亏了，准备半天，完全浪费时间，大二寒假我还想着好好学托福，结果到最后都只背了单词，然后开学看了八个多小时的李文勍的视频，觉得受益颇丰，第一次考了100分。一个学期没学托福，但是生活大爆炸全集听了很多遍，写代码读各种文档网站加上上课进一步练了阅读速度和单词，用英语记日记练了写作，期末完直接托福裸考，110，由此得出经验，早早准备托福真是愚蠢，在清华的环境下，加上自己的无形训练，自然而然的就高分了。</p><h4 id="更改预期时间"><a href="#更改预期时间" class="headerlink" title="更改预期时间"></a>更改预期时间</h4><p>突然更改时间是大三下开始，大三寒假基本都处于半养病状态，第一周也养了几天病，然后进入一段例行焦虑期，当时暑研没找好，学堂班的项目不想去做湿实验，自己想找的迟迟没发邮件，又听闻周围几个同学用寒假学了学GRE去考了一次，于是心里一动，觉得有必要在暑研前考一次“先感受感受”，免得暑研回来直接兵荒马乱了。于是挑选一番报了个北大4月28号的，北大考场还是很好的，离清华最近，骑车就过去了，上次裸考托福感觉很好，于是感觉选考场很满意。</p><h3 id="复习准备"><a href="#复习准备" class="headerlink" title="复习准备"></a>复习准备</h3><p>这块儿其实可以空白了，，因为完全没准备。说实话是有点自负的，而且实在是不想相信新东方之流以及微信公众号还有几个微信小助手各种狂轰滥炸的课程和经验分享，平时也忙得要死，是实在抽不出时间准备。</p><h4 id="背单词"><a href="#背单词" class="headerlink" title="背单词"></a>背单词</h4><p>都说GRE单词极其难，确实如此，看了看还是很受打击的，又没空背，很多人讲要你命3000词要刷个十几遍，这个很经典，我下了app，打印了单词，准备了几本单词书，但是都没有看，，别说背一遍了，看都没看，，，</p><h4 id="verbal部分"><a href="#verbal部分" class="headerlink" title="verbal部分"></a>verbal部分</h4><p>单词都没背就不说更进一步的准备了，之前买过好几本verbal的填空和阅读书，没看过一眼，下载过整理过很多网上资源，没有打开看过，事实上在上考场之前我都不知道verbal有几种题型考什么。。。（现在知道了，一空两空三空双选和阅读）</p><h4 id="quant部分"><a href="#quant部分" class="headerlink" title="quant部分"></a>quant部分</h4><p>这部分就更更更不用说了，，，由于受到主流观点的“误导”：都说GRE verbal极其变态，quant部分都是初中及以下题目，大家都是满分，因此我连看都没看，题型是完全不知道，觉得初中数学题也没什么练的，数学名词？平时数学课都英文的，应该没问题吧？（现在想想，代数和几何部分其实很多名词还是很不熟的）</p><h4 id="作文"><a href="#作文" class="headerlink" title="作文"></a>作文</h4><p>别提了，知道有issue和argument两种题型，而且官方的题库都有，但是别说一道题一道题准备了，看都没看过，也不知道这两种题会考什么，有什么区别，任何套路也没准备。</p><h2 id="考试"><a href="#考试" class="headerlink" title="考试"></a>考试</h2><p>综上所述，这次考试完全没有抱着要通过的心态，决定先试一次水，因此真是难得的让我惊讶的淡定，连题型都没搞清楚就上考场了。<br>4.28是个不好的考试时间，正好期中考试周，刚考完几门，累得要死，一堆大作业，因此完全没时间复习GRE，毕竟也不是一次性考试，于是也没想着要复习，考试前一天又感觉不舒服，又晕又发热，完全看不进去书，而且GRE的准备确实是个庞大工程，我是觉得一晚上也没什么可看的了，于是重温了一遍16年G7，看了看詹姆斯，重温了一遍左邻右舍，在实验室磨蹭到十一点半，买了小面包，顺手买了一罐红牛，发现自行车还在六教停着，又拐到六教取了车，回去洗漱洗漱看看手机，睡下来都一点多了。</p><p>早上晕晕乎乎的到了北大，发现需要红牛撑过四个小时，开始做题，前两个小时先是两篇作文然后一个语言一个数学模块。写作文的时候才知道这两种题型是什么，，完全没有套路地口语化地瞎扯，然后做verbal，发现真的很难，填空也难，阅读也比托福的难，主要是时间不够，但是竟然没有出现经验帖子里说的时间不够就放弃一些题的现象，数学部分感觉真简单，35分钟时间十几分钟就做完了，提前出去休息了。</p><p>10分钟break回来，继续两个语言section一个数学section，这次语言的更难了，这说明第一个做的还不错，据说第二个语言section如果越做越简单，说明第一个语言section是肯定做崩了的，但是依然没有出现做不完，因为通过第一个section了解了GRE语言部分在考什么，第二个section做的熟练了一些，依稀有些怀疑，第二个section不会简单了吧，难道第一个section做的很烂？然后打消念头，第二个还是很难的，不过确实做的快了一些。然后心里对GRE的尊重和畏惧变成快消失殆尽了，似乎和托福一样是纸老虎，而且阅读也太喜欢出科技类文章了，简直是送分的，读着太顺了，觉得这对其他学生很不公平（大概懂了孟孟为什么托福考不好了，同理）。</p><p>第二个数学section明显比第一个难了，开始后悔没有提前背一下数学部分专有名词（我是搜集了相关资料的），因为有几个题无法确定某几个词的意思。而且感觉数学的出题风格很乖，get不到有些题的点</p><p>最后考到了快两点，点提交，现场出分很刺激，看到verbal分惊讶了一下，154，这裸考都154了，岂不是不用再考了，一次通过？再看数学，164，，，感觉非常丢人，说好的都满分169呢，，，没想到竟然是在初中数学上栽跟头了，，，</p><p>回去搜到了这个帖子</p><p><a href="https://www.zhihu.com/question/24063337" target="_blank" rel="noopener">如何提高GRE数学成绩（普通gre，非sub）？ - 知乎</a><br>也许这个知乎回答是“受害者偏差？‘’，反正看了一遍之后发现自己处理数学上太naive了，没想到ETS这么狡猾，竟然有很多题要选“无法确定”？这个选项在中国就是凑数的好嘛。所以数学部分确实得刷点题看看ETS的坑在哪里，靠实力也会被拐到坑里吧。</p><p>当然这次对待GRE的态度本来就是naive的，完全没想准备，但是语言部分是知道大家都说很难得准备几个月没想到裸考之后发现不过如此，平时的水平竟然是完全够了，而数学部分是败给了没经验，看起来还是得做做ETS给的参考题，看看他们会挖什么坑。要是知道会这样，提前刷一个小时数学题，我不就和GRE直接拜拜了？现在就等着暑研之后再裸考一次好了。</p><p>由此可见托福和GRE的水平实在是不过如此，得益于英语的底子（其实没有，退步最大的时候，保送完大概一年的时间节点，英语已经感觉很烂了），加上生院的培养方案以及统辅的课程（比起基本中文的工科院系，生院的课程体系还是很锻炼大家的英语水平的），加上每日环境，基本工作和学习环境都切换到了英文，加上娱乐环境：无限循环美剧，应付托福和GRE已经绰绰有余了。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;记录一下2018.4.28一战裸考GRE的经历，verbal 154，quant 164，总体来看是惊喜中带着遗憾，首先语言部分154分？我可是单词书都没翻过，GRE单词根本没空背，题型都不知道，结果语言部分竟然可以一次过？简直像托福一样是纸老虎嘛，但是quant部分考的得我无地自容，做题的时候不屑于初中水平的考题，都是很快秒杀然后提前提交，然而数学分出来之后让我目瞪口呆，竟然扣了六分，，如果数学部分考满分的话，岂不是一次就过了GRE？简短地总结一下，一场因为疲劳和微恙略感麻木的带着惊喜和遗憾的考试。&lt;/p&gt;
    
    </summary>
    
      <category term="school work" scheme="http://james20141606.github.io/categories/school-work/"/>
    
    
      <category term="school" scheme="http://james20141606.github.io/tags/school/"/>
    
      <category term="language" scheme="http://james20141606.github.io/tags/language/"/>
    
      <category term="exam" scheme="http://james20141606.github.io/tags/exam/"/>
    
  </entry>
  
  <entry>
    <title>Solution of MCM Problem C</title>
    <link href="http://james20141606.github.io/2018/04/21/mcm/"/>
    <id>http://james20141606.github.io/2018/04/21/mcm/</id>
    <published>2018-04-21T01:15:19.000Z</published>
    <updated>2018-05-03T18:05:17.519Z</updated>
    
    <content type="html"><![CDATA[<hr><p>It is our solution of <strong>2018 MCM contest</strong>, our result got <strong>Meritorious award</strong>. By the way, another news makes me 10 times happier than my own award: My girlfriend Mengmeng also got Meritorious Award!<br><div class="row"><iframe src="https://drive.google.com/file/d/1dsWsVDi-VES_-OSU2MX2PceZ4GOeYXlv/preview" style="width:100%; height:550px"></iframe></div><br><a id="more"></a><br>I have put all the codes and related stuff in <a href="https://github.com/james20141606/MCM2018problemC" target="_blank" rel="noopener">GitHub</a><br>Here is the <strong>final paper</strong>:<br><div class="row"><iframe src="https://drive.google.com/file/d/1qpGsc65yvT8BGrGisMkxNSOspTGXFo6G/preview" style="width:100%; height:550px"></iframe></div></p><h2 id="Abstract-of-Paper"><a href="#Abstract-of-Paper" class="headerlink" title="Abstract of Paper"></a>Abstract of Paper</h2><p>In this paper, we construct two evaluation systems: Renewable Energy Indicator (REI) and Network Instability Indicator (NII). By using data mining methods and developing two models, we succeed in visualizing, characterizing, evaluating and predicting the energy flow network and the use of renewable energy.\par</p><p>First, we process the data. We do data screening to select two groups of variables to characterize the energy profile. As some data is missing, we do imputation based on existing data and data from other reliable sources. The data constructing key indicators are normalized with all fifty states. To illustrate energy profile, we also do data visualization with vivid diagrams like Sankey diagram and radar diagram.\par</p><p>Second, we construct two evaluation systems REI and NII to trace and compare the energy profiles of four states and their evolution. NII is derived from a Constrained-Ridge model to characterize the structural stability of energy flow. To specify on renewable energy, we define REI with five key indicators. We use combined Analytic Hierarchy Process(AHP) and Entropy Weight Method(EWM) to construct REI, which characterizes the profile for use of cleaner, renewable energy. We then compare REI and NII of four states and Arizona performs best.\par</p><p>Third, we use several time series models to predict the energy profile of each state in the future. Structural profile of energy flow NII is predicted with Constrained-Ridge model, and renewable energy profile REI is predicted with Autoregressive Integrated Moving Average Model(ARIMA) and Long Short Term Memory(LSTM). Based on the comparison and prediction, we determine the renewable energy usage target for four states and propose several feasible actions.\par</p><p>Finally, we conduct sensitivity analysis of our model. We try several machine learning models to predict 5 key indicators, and use independent validation datasets to evaluate their performances. We also do a perturb-and-profile test to evaluate Constrained-Ridge model’s power to characterize network profile.</p><p>Our model is reasonable and legible with theoretical and data support. The model can be easily applied to characterize the energy profile of renewable energy and energy flow network after data training.</p><h2 id="Data"><a href="#Data" class="headerlink" title="Data"></a>Data</h2><p>Used data in data file, produced data in produced data</p><h2 id="Codes-and-method"><a href="#Codes-and-method" class="headerlink" title="Codes and method"></a>Codes and method</h2><p>For quick work in limited time, all codes are written in jupyter notebook, which is a good interactive environment easier to plot and analyze data.<br>We have implemented some algorithm and some visualization method to analyze data.  We use AHP and EWM to construct REI, use ARIMA, Machine Learning model and Deep Learning model to predict time series data. We construct a Restricted Ridge model, using optimization method to depict the network relationship. We use sankey plot, radar plot, and many ordinary plots to visualize data vividly.</p><h2 id="Key-point-manually-feature-selection"><a href="#Key-point-manually-feature-selection" class="headerlink" title="Key point: manually feature selection"></a>Key point: manually feature selection</h2><p>We use a network to extract key features to depict the energy profile. The network is visualized through sankey plot.</p>]]></content>
    
    <summary type="html">
    
      &lt;hr&gt;
&lt;p&gt;It is our solution of &lt;strong&gt;2018 MCM contest&lt;/strong&gt;, our result got &lt;strong&gt;Meritorious award&lt;/strong&gt;. By the way, another news makes me 10 times happier than my own award: My girlfriend Mengmeng also got Meritorious Award!&lt;br&gt;

	&lt;div class=&quot;row&quot;&gt;
		&lt;iframe src=&quot;https://drive.google.com/file/d/1dsWsVDi-VES_-OSU2MX2PceZ4GOeYXlv/preview&quot; style=&quot;width:100%; height:550px&quot;&gt;&lt;/iframe&gt;
	&lt;/div&gt;


&lt;br&gt;
    
    </summary>
    
      <category term="projects" scheme="http://james20141606.github.io/categories/projects/"/>
    
    
      <category term="codes" scheme="http://james20141606.github.io/tags/codes/"/>
    
      <category term="contest" scheme="http://james20141606.github.io/tags/contest/"/>
    
      <category term="math model" scheme="http://james20141606.github.io/tags/math-model/"/>
    
  </entry>
  
  <entry>
    <title>Linear Regression Assignment 8</title>
    <link href="http://james20141606.github.io/2018/04/20/line3/"/>
    <id>http://james20141606.github.io/2018/04/20/line3/</id>
    <published>2018-04-20T02:01:19.000Z</published>
    <updated>2018-04-20T02:53:27.442Z</updated>
    
    <content type="html"><![CDATA[<p>The seventh assignment of Linear Regression. The assignment is written in Rmarkdown, a smart syntax supported by RStudio helping with formula, plot visualization and plugin codes running.<br><a id="more"></a><br>You may also find the <a href="https://github.com/james20141606/somethingmore/tree/master/linear_regression_pdf" target="_blank" rel="noopener"><strong>PDF Version</strong></a> of this assignment. Which is more recommended. <strong>Or here</strong>:<br><div class="row"><iframe src="https://drive.google.com/file/d/1JQjkq4iBhUmCgrJ9ijq9lxTrsroQKeZC/preview" style="width:100%; height:550px"></iframe></div></p><h1 id="1"><a href="#1" class="headerlink" title="1"></a>1</h1><h2 id="a"><a href="#a" class="headerlink" title="a"></a>a</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"># read the data</span><br><span class="line">setwd(&apos;~/Desktop/三春/5线性回归分析/作业/HW8/&apos;)</span><br><span class="line">dat&lt;-read.csv(&quot;hw8.csv&quot;)</span><br><span class="line">X1&lt;-dat$x1</span><br><span class="line">X2&lt;-dat$x2</span><br><span class="line">X3&lt;-dat$x3</span><br><span class="line">X4&lt;-dat$x4</span><br><span class="line">Y&lt;-dat$y</span><br><span class="line"># plot stem and leaf plots</span><br><span class="line">stem(X1)</span><br><span class="line">stem(X2)</span><br><span class="line">stem(X3)</span><br><span class="line">stem(X4)</span><br></pre></td></tr></table></figure><p>It seems that X3 has a denser concentration and the following boxplot supports it. X1 has two outliers. X2 is asymmetric</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">library(vioplot)</span><br><span class="line">vioplot(X1,X2,X3,X4,col=&quot;gold&quot;)</span><br><span class="line">boxplot(X1,X2,X3,X4)</span><br></pre></td></tr></table></figure><h2 id="b"><a href="#b" class="headerlink" title="b"></a>b</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">pairs(~X1+X2+X3+X4+Y,data=dat, </span><br><span class="line">   main=&quot;Scatterplot Matrix&quot;)</span><br><span class="line">cor(dat)</span><br></pre></td></tr></table></figure><p>obviously X3 and X4 has high correlation.</p><h2 id="c"><a href="#c" class="headerlink" title="c"></a>c</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Fit = lm(Y~X1+X2+X3+X4, data=dat)</span><br><span class="line">anova(Fit)</span><br><span class="line">summary(Fit)</span><br></pre></td></tr></table></figure><p>$\hat Y = -124.38 + 0.30 x_1 + 0.05 x_2 + 1.31 x_3 + 0.52 x_4$<br>It seems X2 should be excluded from the model since the p-value=0.4038.</p><h1 id="2"><a href="#2" class="headerlink" title="2"></a>2</h1><h2 id="a-1"><a href="#a-1" class="headerlink" title="a"></a>a</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">library(leaps)</span><br><span class="line">best &lt;- function(model, ...) </span><br><span class="line">&#123;</span><br><span class="line">  subsets &lt;- regsubsets(formula(model), model.frame(model), ...)</span><br><span class="line">  subsets &lt;- with(summary(subsets),</span><br><span class="line">                  cbind(p = as.numeric(rownames(which)), which, adjr2))</span><br><span class="line"></span><br><span class="line">  return(subsets)</span><br><span class="line">&#125;  </span><br><span class="line">round(best(Fit, nbest = 6), 4)</span><br></pre></td></tr></table></figure><p>The four best subset regression models are</p><div class="table-container"><table><thead><tr><th>subset</th><th>$R^2_{a,p}$</th></tr></thead><tbody><tr><td>x1, x3, x4</td><td>0.956</td></tr><tr><td>x1,x2,x3,x4</td><td>0.955</td></tr><tr><td>x1,x3</td><td>0.927</td></tr><tr><td>x1,x2,x3</td><td>0.925</td></tr></tbody></table></div><h2 id="b-1"><a href="#b-1" class="headerlink" title="b"></a>b</h2><p>There are $C_p$ Criterion, #AIC_p# and #SBC_p# which can be used as criterion to select the best model. They all place penalties for adding predictors.</p><h1 id="3"><a href="#3" class="headerlink" title="3"></a>3</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">library(MASS)</span><br><span class="line">Null = lm(Y ~ 1, dat)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">addterm(Null, scope = Fit, test=&quot;F&quot;)</span><br><span class="line">NewMod = update( Null, .~. + X3)</span><br><span class="line">addterm( NewMod, scope = Fit, test=&quot;F&quot; )</span><br><span class="line">NewMod = update( NewMod, .~. + X1)</span><br><span class="line">dropterm(NewMod , test = &quot;F&quot;)</span><br><span class="line">addterm( NewMod, scope = Fit, test=&quot;F&quot; )</span><br><span class="line">NewMod = update( NewMod, .~. + X4)</span><br><span class="line">dropterm( NewMod, test = &quot;F&quot; )</span><br><span class="line">addterm( NewMod, scope = Fit, test=&quot;F&quot; )</span><br></pre></td></tr></table></figure><ul><li>As shown, start with no predictors, X3 is chosen because of smallest p-value.</li><li>Then regressing y on x3 and additional one predictor, the result shows that X1 has the smallest p-value (1.578e-06&lt; 0.05). Therefore X1 can be included in the model. In the same time a test is given to see if x3 should be dtropped. Since p-value (6.313e-13&lt;0.10), X3 is retained.</li><li>Then regressing y on X3, X1 and any one of the rest two, it shows that X4 has the smallest p-value (0.0007354 &lt; 0.05) and hence being included in the model.In the same time a test is given to see if x3 or x1 should be dtropped. Since both of their p-value &lt; 0.10, they are both retained.</li><li>Finally, regressing y on all four predictors and x2 isn’t significant to be included (0.4038 &gt; 0.05). Thus it is deleted from the model.</li><li>The best subset of predictor variables to predict job proficiency is (x1,x3,x4)</li></ul><h2 id="b-2"><a href="#b-2" class="headerlink" title="b"></a>b</h2><p>The model evaluated using the forward stepwise regression shows the same result as earlier chosen variables under the criteria of adjusted R square.</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;The seventh assignment of Linear Regression. The assignment is written in Rmarkdown, a smart syntax supported by RStudio helping with formula, plot visualization and plugin codes running.&lt;br&gt;
    
    </summary>
    
      <category term="school work" scheme="http://james20141606.github.io/categories/school-work/"/>
    
    
      <category term="codes" scheme="http://james20141606.github.io/tags/codes/"/>
    
      <category term="R" scheme="http://james20141606.github.io/tags/R/"/>
    
      <category term="assignment" scheme="http://james20141606.github.io/tags/assignment/"/>
    
      <category term="statistics" scheme="http://james20141606.github.io/tags/statistics/"/>
    
      <category term="linear regression" scheme="http://james20141606.github.io/tags/linear-regression/"/>
    
  </entry>
  
  <entry>
    <title>Reading Notes of Wittgenstein&#39;s Bio</title>
    <link href="http://james20141606.github.io/2018/04/17/Reading-Notes-of-Wittgenstein-s-Bio/"/>
    <id>http://james20141606.github.io/2018/04/17/Reading-Notes-of-Wittgenstein-s-Bio/</id>
    <published>2018-04-17T14:37:17.000Z</published>
    <updated>2018-04-20T02:56:36.088Z</updated>
    
    <content type="html"><![CDATA[<p>Some notes and thoughts concerning <strong>Ludwig Wittgenstein: The Duty of Genius</strong> by Monk, Ray. To be honest, I first found Wittgenstein very mysterious and great (part of the reason is his wonderful german name). But now I feel little disappointed. I see more a crazy and strange man rather than a genius. He may be a genius, but if it is because his madness and defect (I know it is wrong to say like this, but I can always have such feelings that some man is “great” just because he is strange or even mad), I don’t think it is a great thing. Also I am always confused about philosopher, they always think themselves have the right to judge everything, it is really ridiculous. So I really hate many of his thoughts concerning language, science and maths.<br><a id="more"></a><br><div class="row"><iframe src="https://drive.google.com/file/d/1T24zGxWcLN4W33lQm1CbFhy_yFUMzih0/preview" style="width:100%; height:550px"></iframe></div></p><h1 id="RELUCTANT-PROFESSOR"><a href="#RELUCTANT-PROFESSOR" class="headerlink" title="RELUCTANT PROFESSOR"></a>RELUCTANT PROFESSOR</h1><p>My part is CHAP 20, the reluctant professor. This chapter contains many things, including WITTGENSTEIN work and life after his return to Cambridge. He gives some lectures and courses about aesthetics, science, religion, mathematics. He also struggles with his love with Francis and experience many guilt after Francis death. It helps to better understand WITTGENSTEIN.</p><h2 id="Return-to-Cambridge"><a href="#Return-to-Cambridge" class="headerlink" title="Return to Cambridge"></a>Return to Cambridge</h2><p>Anschluss( [‘ɑ:nʃ lu:s) is one of the reason WITTGENSTEIN return to Cambridge ([vitɡən’ʃtain]). His attempts to find a niche in life outside academia has been at best inconclusive. His savings, of £300 or £400, would not have lasted a lifetime. Eventually, he would have had to have found some paid employment.<br>That is, as he had put it to Moore in 1930, <strong>he would have had to have found someone who had a use for the sort of goods he produced. And the place where these goods were in most demand was, inevitably, in academic life, and particularly in Cambridge</strong>.<br>So at some time or other he would have applied for a lectureship.  However, is that if it had not been for the Anschluss, this would not have been as early as April 1938.<br>This is not only because Wittgenstein was then <strong>not eager to return to teaching</strong>, but also because he was worried about his <strong>relationship with Francis</strong>. </p><ul><li>[x] he was deeply concerned about the sensuality that existed between himself and Francis, and anxious whether, on his part at least, such sensual desires <strong>were compatible with true love.</strong> </li></ul><h2 id="Live-as-Couple"><a href="#Live-as-Couple" class="headerlink" title="Live as Couple"></a>Live as Couple</h2><p>Upon his return he moved into Francis’s lodgings and for over a year they lived <strong>as a couple.</strong> But by 1939 it had deteriorated, and that for the following two years it was only Francis’s undyingly <strong>faithful</strong>, perhaps even <strong>clinging, love</strong> for Wittgenstein that kept it going.<br>Wittgenstein’s love for Francis didn’t survive the physical closeness that he at once <strong>craved and feared</strong>. </p><h2 id="New-Disciples"><a href="#New-Disciples" class="headerlink" title="New Disciples"></a>New Disciples</h2><p>Wittgenstein found a <strong>new generation of disciples</strong>.<br>In order to keep his class down to a size with which he felt comfortable, he <strong>did not announce his lectures in the usual way</strong> in the Cambridge University Recorder. Some students were asked if they would be interested about the classes. No more than about ten students attended</p><p>Unfortunately, one of the fact is that those whom Wittgenstein <strong>influenced most strongly</strong>, particularly in the <strong>1930s</strong> did not enter <strong>academic life</strong>. So a large and important aspect of Wittgenstein’s influence is not reflected in the large body of academic literature that Wittgenstein’s work has inspired. </p><p>During this lecture Wittgenstein told one of the students to stop making notes: </p><p> ~If you write these spontaneous remarks down, some day someone may publish them as my considered opinions. I don’t want that done. For I am talking now freely as my ideas come.~ </p><p>Fortunately, this request was ignored, and notes from these lectures have indeed been published.</p><h1 id="Aesthetics-and-Religion"><a href="#Aesthetics-and-Religion" class="headerlink" title="Aesthetics and Religion"></a>Aesthetics and Religion</h1><h2 id="Attach-Worship-of-Science"><a href="#Attach-Worship-of-Science" class="headerlink" title="Attach Worship of Science"></a>Attach Worship of Science</h2><p>These lectures are <strong>unique</strong> among Wittgenstein’s corpus. They are concerned, not with mathematics or philosophy generally, but with <strong>aesthetics and religious belief.</strong><br>What distinguishes these lectures is their tone. Precisely because he was speaking in a <strong>spontaneous and unguarded manner</strong><br>And his target is more about the <strong>wretched effect that the worship of science and the scientific method has had upon our whole culture.</strong></p><p>He believes that in areas of <strong>thought and life, scientific method is not appropriate</strong>, and efforts trying to make it <strong>so lead to distortion, superficiality and confusion</strong>.<br>Wittgenstein told his audience that what he was doing was ‘persuading people to change their style of thinking’ He said he is really <strong>disgusted with worship of science</strong><br>He gives an example: </p><p>~Jeans has written a book called The Mysterious Universe and I loathe it and call it misleading.3 Take the title … I might say the title The Mysterious Universe includes a kind of idol worship, <strong>the idol being Science and the Scientist</strong>.~</p><h2 id="Rescue-Artistic-Appreciation"><a href="#Rescue-Artistic-Appreciation" class="headerlink" title="Rescue Artistic Appreciation"></a>Rescue Artistic Appreciation</h2><p>He was also trying to rescue questions of artistic appreciation from the idea that there could be <strong>a kind of science of aesthetics</strong>:<br>He said:  </p><p>~You might think Aesthetics is a science telling us what’s beautiful – almost too ridiculous for words.I suppose it ought to include also what sort of coffee tastes well~.</p><p>When  asked about his ‘theory’ of deterioration Wittgenstein answered:</p><p>~‘Do you think I have a theory? Do you think I’m saying what deterioration is? What I do is <strong>describe different things called deterioration</strong>~</p><p>He said that Appreciation often will not consist in saying anything. Appreciation <strong>will be shown</strong></p><ul><li>by actions as often as by words</li><li>by certain gestures of disgust or satisfaction</li><li>by the way we read a work of poetry or play a piece of music</li><li><p>by how often we read or listen to the same piece, and how we do so.<br>These different forms of appreciation <strong>do not have any one thing in common that one can isolate in answer to the question: ‘What is artistic appreciation?’</strong> </p></li><li><p>[x] (They are, rather, linked by a complicated series of ‘family resemblances’. )</p></li></ul><p>So It is <strong>impossible to describe what it consists</strong> because we would have to describe the whole environment. </p><h2 id="Religious-Belief"><a href="#Religious-Belief" class="headerlink" title="Religious Belief"></a>Religious Belief</h2><p>Regarding religious, Wittgenstein did not wish to <strong>see God or to</strong> <strong>find reasons for His existence</strong>. He thought that if he could overcome himself – if a day came when his whole nature ‘bowed down in humble resignation in the dust’ – then God would  come to him and he would be saved.<br>But in his lectures on religious belief he concentrates only on the first part of this convictiont<strong>he denial of the necessity to have reasons for religious beliefs</strong>.<br><strong>He said:</strong></p><p>~‘Russell and the parsons between them have done infinite harm, infinite harm.’~ </p><p>This is because they both have encouraged the idea that a <strong>philosophical justification for religious beliefs is necessary for those beliefs</strong>  So Both of them have fallen victim to the <strong>idol-worship of the scientific style of thinking.</strong> <strong>Religious beliefs are not analogous to scientific theories, and should not be accepted or rejected using the same evidential criteria.</strong><br> Wittgenstein insists that : The kind of experience that can make a man religious, is not at all like the experience of <strong>drawing a conclusion from an experiment or from a collection of data.</strong></p><h1 id="Pride-and-Ambivalence"><a href="#Pride-and-Ambivalence" class="headerlink" title="Pride and Ambivalence"></a>Pride and Ambivalence</h1><h2 id="Apologize-for-Everything"><a href="#Apologize-for-Everything" class="headerlink" title="Apologize for Everything"></a>Apologize for Everything</h2><p>Wittgenstein is determined not to let himself get away with the smallest misbehavior. For example he once wrote a letter to apologize for a very minor mistake:<br><figure class="highlight applescript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">Dear Mrs Stewart</span><br><span class="line">I must apologise <span class="keyword">for</span> an untruth I told you today <span class="keyword">in</span> Miss Pate’s office. </span><br><span class="line">I said <span class="keyword">that</span> I had seen Mrs Thomson recently <span class="keyword">in</span> Birmingham; </span><br><span class="line">&amp; only when I came home this evening <span class="keyword">it</span> occurred <span class="keyword">to</span> <span class="keyword">me</span> <span class="keyword">that</span> this wasn’t <span class="literal">true</span> <span class="keyword">at</span> all.</span><br><span class="line">……</span><br><span class="line">Please forgive <span class="keyword">my</span> stupidity. </span><br><span class="line">Yours sincerely, L. Wittgenstein</span><br></pre></td></tr></table></figure></p><h2 id="Pride-in-Work"><a href="#Pride-in-Work" class="headerlink" title="Pride in Work"></a>Pride in Work</h2><p>However, things are very interesting in philosophy work. Though he tried  to exclude pride from his work, as he put it, <strong>‘to the glory of God’ rather than out of vanity</strong>,<br>yet we find again and again that <strong>‘pride of Lucifer’</strong> occurs most often in philosophy work.<br>He said: </p><p>~I was obliged to learn that my results  variously misunderstood…. This stung my vanity and I had difficulty in quieting it.~ </p><h2 id="Become-Professor"><a href="#Become-Professor" class="headerlink" title="Become Professor"></a>Become Professor</h2><p>After G. E. Moore’s resignation, He decided to apply for the post of Professor of Philosoph<br>The disadvantage is  one of the electors Collingwood was disagree with Wittgenstein’s work. The advantage is that among the electors there was John Keynes.<br>The fact is that, by 1939 he was recognized as the <strong>foremost philosophical genius of his time</strong>. </p><p>~‘To refuse the chair to Wittgenstein’ ‘would be like refusing Einstein a chair of physics.’~ , said C. D. Broad. Broad was not a admirer of Wittgenstein’s work; he was simply stating a fact. </p><p>On 11 February Wittgenstein was duly elected professor. </p><h1 id="Mathematics"><a href="#Mathematics" class="headerlink" title="Mathematics"></a>Mathematics</h1><h2 id="Redescribe-Math"><a href="#Redescribe-Math" class="headerlink" title="Redescribe Math"></a>Redescribe Math</h2><p>After he became a Professor, He decided to rescue math by giving a series of lectures to the subject.</p><p>So his aim was to reinterpret mathematics – to redescribe it was not a fascinating world waiting for mathematicians, but a swamp of philosophical confusions. </p><ul><li>The mathematician Hilbert had once said: ‘No one is going to turn us out of the paradise which Cantor has created.’</li><li>Wittgenstein told his class: ‘I wouldn’t dream of trying to drive anyone out of this paradise’ I would do something quite different: I would try to show you that it is not a paradise so that you’ll leave of your own accord</li></ul><p>The lectures on mathematics is one of Wittgenstein’s general attack on the idol-worship of science. Wittgenstein thought the idolization of science was the <strong>most significant symptom and a contributory cause of the decay of our culture.</strong> </p><h2 id="Argue-with-Turing"><a href="#Argue-with-Turing" class="headerlink" title="Argue with Turing"></a>Argue with Turing</h2><p>One of the lectures audience was one of the greatest mathematicians of the century: Alan Turing.<br>The lectures often developed into a <strong>dialogue</strong> between Wittgenstein and Turing, with the f<strong>ormer attacking and the latter defending the importance of mathematical logic</strong>.<br>The presence of Turing is very essential to the theme of the discussion, once when he announced he would not be attending a certain lecture, Wittgenstein told the class that, therefore, that lecture would have to be ‘somewhat parenthetical’</p><p>He said he would try again and again to show that what is called a <strong>mathematical discovery is actually a mathematical invention</strong>. On his view, nothing for the mathematician to discover. A proof in mathematics does not mean the truth of a conclusion but fix the meaning of certain signs.<br>He thought that <strong>mathematical propositions are grammatical</strong>. </p><p>After some more lectures Turing stopped attending, convinced that if Wittgenstein would not admit a contradiction is a fatal flaw in a system of mathematics, then there could be no common ground between them.<br>I think it is very brave of Turing to attend the classes as the representative of all that Wittgenstein was attacking, surrounded by Wittgenstein’s students and discussing the issues in a way that was unfamiliar to him. </p><h1 id="The-Death-of-Francis"><a href="#The-Death-of-Francis" class="headerlink" title="The Death of Francis"></a>The Death of Francis</h1><h2 id="Death-of-Francis"><a href="#Death-of-Francis" class="headerlink" title="Death of Francis"></a>Death of Francis</h2><p>By the time the second world war broke out, Skinner’s period as an apprentice had come to an end and he returned to Cambridge and he seems to have made an attempt to return to theoretical work<br>He knew that he was losing Wittgenstein’s love. After his return to Cambridge, he and Wittgenstein lived separately</p><p>In 1941 Francis had been taken seriously ill with polio and had been admitted into hospital. On 11 October 1941, Francis died.<br>Wittgenstein’s initial reaction was <strong>one of delicate restraint</strong>. In letters to friends telling them of Francis’s death, he managed a <strong>tone of quiet dignity.</strong> :</p><p>~He died without any pain or struggle entirely peacefully. I was with him. I think he has had one of the happiest lives I’ve known anyone to have, &amp; also the most peaceful death.~ </p><p>By the time of the funeral  his restraint had gone. He behaves like a <strong>‘frightened wild animal’</strong> at the ceremony, after the ceremony he refused to go to the house but  walked around town.</p><p>But Wittgenstein’s guilt over Francis was entirely unconnected with the way in which he had influenced him. It had to do with more <strong>internal matters</strong><br>He wrote: </p><p>~In the last 2 years of his life very often loveless and, in my heart, unfaithful to him. If he had not been so boundlessly gentle and true, I would have become totally loveless towards him.~ </p><p>~Think a great deal about the last time I was with Francis; about my odiousness towards him … I cannot see how I can ever in my life be freed from this guilt~</p><h2 id="Solipsism-Philosophy-and-Love"><a href="#Solipsism-Philosophy-and-Love" class="headerlink" title="Solipsism: Philosophy and Love"></a>Solipsism: Philosophy and Love</h2><p>Compared with other people Wittgenstein love but not get reward, we can see the characteristics of his love: <strong>a certain indifference to the feelings of the other person.</strong> Neither Pinsent nor Marguerite nor Kirk  were in love with him seemed not to affect his love for them. Indeed, it perhaps made <strong>his love easier to give</strong>, for the relationship could be <strong>safe</strong>, in the <strong>splendid isolation of his own feelings</strong>.<br>So at last there is a very important concept about Wittgenstein’s love and philosophical ideas:  <strong>solipsism</strong></p><p>Most of his later work is to against the philosophical solipsism which once attracted his very much. He characterized his later work as <strong>an attempt to show the fly the way out of the fly-bottle</strong>)<br>Its parallel is the emotional solipsism. With Francis that isolation was threatened, and, in the face of that threat, Wittgenstein had withdrawn. I think it is the major reason he behaves really badly towards Francis in his last few years and made him so guilty.</p><p>So that’s all about Chapter 20, about the life when he returned to Cambridge, his thought towards science, aesthetics, religious, mathematics and love. It shows what a complicated man Wittgenstein was, most importantly, the author gives us the parallel comparison about his love and his work. We can have a better understanding how one man’s characteristics can deeply shape him,  especially for such a genius.</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Some notes and thoughts concerning &lt;strong&gt;Ludwig Wittgenstein: The Duty of Genius&lt;/strong&gt; by Monk, Ray. To be honest, I first found Wittgenstein very mysterious and great (part of the reason is his wonderful german name). But now I feel little disappointed. I see more a crazy and strange man rather than a genius. He may be a genius, but if it is because his madness and defect (I know it is wrong to say like this, but I can always have such feelings that some man is “great” just because he is strange or even mad), I don’t think it is a great thing. Also I am always confused about philosopher, they always think themselves have the right to judge everything, it is really ridiculous. So I really hate many of his thoughts concerning language, science and maths.&lt;br&gt;
    
    </summary>
    
      <category term="thoughts" scheme="http://james20141606.github.io/categories/thoughts/"/>
    
    
      <category term="philosophy" scheme="http://james20141606.github.io/tags/philosophy/"/>
    
      <category term="Wittgenstein" scheme="http://james20141606.github.io/tags/Wittgenstein/"/>
    
      <category term="thoughts" scheme="http://james20141606.github.io/tags/thoughts/"/>
    
  </entry>
  
  <entry>
    <title>Assignments of Statistic Inferences</title>
    <link href="http://james20141606.github.io/2018/04/16/Statistic-Inferences/"/>
    <id>http://james20141606.github.io/2018/04/16/Statistic-Inferences/</id>
    <published>2018-04-16T06:33:06.000Z</published>
    <updated>2018-04-16T06:46:36.287Z</updated>
    
    <content type="html"><![CDATA[<p>ALL the assignments of <strong>Statistic Inferences</strong>. The assignment is written in Rmarkdown/LaTeX, LaTeX is a classic formula editor but a little hard to use intensely. Rmarkdown is introduced by TA and it is a smart syntax supported by RStudio helping with formula, plot visualization and plugin codes running.<br><a id="more"></a><br>You may also find the <a href="https://github.com/james20141606/somethingmore/tree/master/statistic_inderences_pdf" target="_blank" rel="noopener"><strong>PDF Version</strong></a> of this assignment. Which is more recommended.</p><p>Apart from all the math and knowledge I learned from SI course, one of the most useful techniques a obtain from the course is definitely “writing skill”. Statistic Inferences is my first statistic minor course. Before this course I rarely use LaTeX or markdown.(Only when I write math model course paper and project report). So I am rusty at writing complex formulas at first. But thanks to this course, I use LaTeX and markdown really proficiently. Now I write nearly every file in LaTeX of markdown. I also want to recommend <a href="http://www.bear-writer.com/" target="_blank" rel="noopener"><strong>Bear</strong></a>, a markdown based notes manager, which is perfect especially for coders. I have transferred all my codes, work, plan to it for nearly two years and it really helps me to be more effective. One of many benefits is I can simply copy all my notes in bear in my posts without changing anything, for hexo and bear both use markdown(as well as Rmarkdown and LaTeX and gitbook and all README file, they all appear in my daily life quite often).</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;ALL the assignments of &lt;strong&gt;Statistic Inferences&lt;/strong&gt;. The assignment is written in Rmarkdown/LaTeX, LaTeX is a classic formula editor but a little hard to use intensely. Rmarkdown is introduced by TA and it is a smart syntax supported by RStudio helping with formula, plot visualization and plugin codes running.&lt;br&gt;
    
    </summary>
    
      <category term="school work" scheme="http://james20141606.github.io/categories/school-work/"/>
    
    
      <category term="codes" scheme="http://james20141606.github.io/tags/codes/"/>
    
      <category term="R" scheme="http://james20141606.github.io/tags/R/"/>
    
      <category term="assignment" scheme="http://james20141606.github.io/tags/assignment/"/>
    
      <category term="statistics" scheme="http://james20141606.github.io/tags/statistics/"/>
    
  </entry>
  
  <entry>
    <title>Linear Regression Assignment 7</title>
    <link href="http://james20141606.github.io/2018/04/16/line2/"/>
    <id>http://james20141606.github.io/2018/04/16/line2/</id>
    <published>2018-04-16T06:01:19.000Z</published>
    <updated>2018-04-20T02:59:10.467Z</updated>
    
    <content type="html"><![CDATA[<p>The seventh assignment of Linear Regression. The assignment is written in Rmarkdown, a smart syntax supported by RStudio helping with formula, plot visualization and plugin codes running.<br><a id="more"></a><br>You may also find the <a href="https://github.com/james20141606/somethingmore/tree/master/linear_regression_pdf" target="_blank" rel="noopener"><strong>PDF Version</strong></a> of this assignment. Which is more recommended. <strong>Or here</strong>:<br><div class="row"><iframe src="https://drive.google.com/file/d/10Oo7Rb7sKNbT7XPEzdGwlfQRO1O42Y0h/preview" style="width:100%; height:550px"></iframe></div></p><h1 id="1"><a href="#1" class="headerlink" title="1"></a>1</h1><h2 id="a"><a href="#a" class="headerlink" title="a"></a>a</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"># read the data</span><br><span class="line">setwd(&apos;~/Desktop/三春/5线性回归分析/作业/HW7/&apos;)</span><br><span class="line">Data&lt;-read.table(&quot;hw7.txt&quot;)</span><br><span class="line">names(Data) = c(&quot;Hours&quot;,&quot;Cases&quot;,&quot;Costs&quot;,&quot;Holiday&quot;)</span><br><span class="line">Fit = lm(Hours~Cases+Costs+Holiday, data=Data)</span><br><span class="line">anova(Fit)</span><br><span class="line">SSTO = sum( anova(Fit)[,2] )</span><br><span class="line">MSE = anova(Fit)[4,3]</span><br><span class="line">SSR = sum( anova(Fit)[1:3,2] )  </span><br><span class="line">MSR = SSR / 3                  </span><br><span class="line">SSE = anova(Fit)[4,2]</span><br></pre></td></tr></table></figure><p>From the table we have: $SSR(X_1) = 136366$,$SSE(X_1,X_2,X_3) = 985530$</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Fit2 = lm(Hours~Cases+Holiday, data=Data)</span><br><span class="line">anova(Fit2)</span><br></pre></td></tr></table></figure><p>$SSR(X_3|X_1) = 2033565$</p><p>$SSR(X_2|X_1,X_3) = SSE(X_1,X_3)-SSE(X_1,X_2,X_3)$ = 992204 - 985530 = 6674</p><h2 id="b"><a href="#b" class="headerlink" title="b"></a>b</h2><script type="math/tex; mode=display">H_0: \beta_2 = 0, H_a: \beta_2 \neq 0. \\</script><p>From a we have:</p><script type="math/tex; mode=display">SSR(X_2|X_1,X_3)  = 6674, SSE(X_1,X_2,X_3) =  985530\\F^* = \frac{(6674/1)}{985530/48} = 0.32491 \\F(0.95,1,48) = 4.04265 \\If \  F^* \leqslant 4.04265 \ conclude H_9, otherwise \ conclude H_a \\P-value = 0.5713</script><h2 id="c"><a href="#c" class="headerlink" title="c"></a>c</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Fit2 = lm(Hours~Cases+Costs, data=Data)</span><br><span class="line">anova(Fit2)</span><br><span class="line">Fit3 = lm(Hours~Costs+Cases, data=Data)</span><br><span class="line">anova(Fit3)</span><br></pre></td></tr></table></figure><p>So we have $SSR(X_2|X_1) +SSR(X_1) = 136366 + 5726 = 11395+130697 =SSR(X_1|X_2) +SSR(X_2)$ </p><p>Yes, it is always true because: $SSR(X_2|X_1)+SSR(X_1) = SSE(X_1) - SSE(X_1,X_2) +SSR(X_1) = SSTO -SSE(X_1,X_2)$</p><p>$SSR(X_1|X_2)+SSR(X_2) = SSE(X_2) - SSE(X_1,X_2) +SSR(X_2) = SSTO -SSE(X_1,X_2)$</p><h1 id="2"><a href="#2" class="headerlink" title="2"></a>2</h1><p>From question1, We have $SSR(X<em>1) = 136366,SSR(X_2) =  5726 , SSR = 2176606,SSTO = 3162136$ \<br>So $R^2</em>{Y<em>1} = 0.0431,R^2</em>{Y<em>2} =0.00181,R^2 = 0.6883$ \<br>From homework6 we have $r_12 = 0.10059216$, so $R^2</em>{12} =0.0101 $ \</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Fit4 = lm(Hours~Costs, data=Data)</span><br><span class="line">anova(Fit4)</span><br><span class="line">Fit5 = lm(Hours~Cases, data=Data)</span><br><span class="line">anova(Fit5)</span><br></pre></td></tr></table></figure><p>$R^2<em>{Y1|2} = \frac{SSR(X_1|X_2)}{SSE(X_2)}$ = 130697/3150741 = 0.04148\<br>$R^2</em>{Y2|1} = \frac{SSR(X_2|X_1)}{SSE(X_1)}$ = 5726/3025770 = 0.001892\</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Fit6 = lm(Hours~Cases+Holiday, data=Data)</span><br><span class="line">anova(Fit6)</span><br></pre></td></tr></table></figure><p>$R^2_{Y2|13} = \frac{SSR(X_2|X_1,X_3)}{SSE(X_1,X_3)}$</p><p>$SSR(X_2|X_1,X_3) = SSE(X_1,X_3)-SSE(X_1,X_2,X_3)$ = 992204 - 985530 = 6674 </p><p>$SSE(X<em>1,X_3)$ = 992204, so $R^2</em>{Y2|13}$ = 6674/992204 = 0.006726</p><h1 id="3"><a href="#3" class="headerlink" title="3"></a>3</h1><h2 id="a-1"><a href="#a-1" class="headerlink" title="a"></a>a</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Fit = lm(Hours~Cases, data=Data)</span><br><span class="line">summary(Fit)</span><br></pre></td></tr></table></figure><p>So regression function is $\hat Y = 4080 +0.0009355X_1$ \</p><h2 id="b-1"><a href="#b-1" class="headerlink" title="b"></a>b</h2><p>The regression function in 6.10a is $Y=0.0007871X_1-13.17X_2+623.6X3+4150$</p><p>The coefficient $\beta_1$ is bigger than coefficient in 6.10a.</p><h2 id="c-1"><a href="#c-1" class="headerlink" title="c"></a>c</h2><p>No, from question 1, $SSR(X_1) = 136366,SSR(X_1|X_2)=130697$. It’s not substantial</p><h2 id="d"><a href="#d" class="headerlink" title="d"></a>d</h2><p>The correlation of $X_1,X_2$ is highest in all predictors, so the $SSR(X_1) and SSR(X_1|X_2)$ don’t have substantial difference.</p><h1 id="4"><a href="#4" class="headerlink" title="4"></a>4</h1><h2 id="a-2"><a href="#a-2" class="headerlink" title="a"></a>a</h2><p>To run a polynomial regression model on one or more predictor variables, it is advisable to first center the variables by subtracting the corresponding mean of each, in order to reduce the intercorrelation among the variables.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">x1 &lt;- Data$Cases - mean(Data$Cases)</span><br><span class="line">x3 &lt;- Data$Holiday - mean(Data$Holiday)</span><br><span class="line">x1sq &lt;- x1^2</span><br><span class="line">x3sq &lt;- x3^2</span><br><span class="line">x1x3 &lt;- x1 * x3</span><br><span class="line">Grocery &lt;- cbind( Data, x1, x3, x1sq, x3sq, x1x3 )</span><br><span class="line">Poly &lt;- lm(  Hours ~ x1 + x3 + x1sq + x3sq + x1x3, data=Grocery )</span><br><span class="line">summary(Poly)</span><br></pre></td></tr></table></figure><p>So the model is $\hat Y = 4367+8.61 \times 10^{-4} X_1+623.7 X_3-1.154 \times 10^{-9}X_1^2 -8.87 \times 10^{-5} X_1 X_3$</p><h2 id="b-2"><a href="#b-2" class="headerlink" title="b"></a>b</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">anova(Poly)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Fit7 &lt;-lm(  Hours ~ x1 + x3, data=Grocery )</span><br><span class="line">anova(Fit7)</span><br><span class="line">qf(0.95,3,46)</span><br></pre></td></tr></table></figure><script type="math/tex; mode=display">H_0: \beta_3,\beta_4,\beta_5 =0, H_a: \text{not all }\beta_k in H_0 = 0\\F^* = \frac{SSR(X_1^2,X_3^2,X_1X_3|X_1,X_3)/3}{SSE(X_1^2,X_3^2,X_1X_3,X_1,X_3)/(n-6)}\\=\frac{(SSE(X_1,X_3)-SSE(X_1^2,X_3^2,X_1X_3,X_1,X_3))/3}{991173/46}\\=\frac{(992204-991173)/3}{991173/46}\\=0.01594945\\F(0.95,3,46) = 2.806845, So \ F^* < F(0.95,3,46), \text{Do not reject H_0.}</script><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pf(0.01594945,3,46)</span><br></pre></td></tr></table></figure><p>p-value = 0.002785933</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;The seventh assignment of Linear Regression. The assignment is written in Rmarkdown, a smart syntax supported by RStudio helping with formula, plot visualization and plugin codes running.&lt;br&gt;
    
    </summary>
    
      <category term="school work" scheme="http://james20141606.github.io/categories/school-work/"/>
    
    
      <category term="codes" scheme="http://james20141606.github.io/tags/codes/"/>
    
      <category term="R" scheme="http://james20141606.github.io/tags/R/"/>
    
      <category term="assignment" scheme="http://james20141606.github.io/tags/assignment/"/>
    
      <category term="statistics" scheme="http://james20141606.github.io/tags/statistics/"/>
    
      <category term="linear regression" scheme="http://james20141606.github.io/tags/linear-regression/"/>
    
  </entry>
  
  <entry>
    <title>Linear Regression Assignment 6</title>
    <link href="http://james20141606.github.io/2018/04/16/line1/"/>
    <id>http://james20141606.github.io/2018/04/16/line1/</id>
    <published>2018-04-16T05:59:19.000Z</published>
    <updated>2018-04-20T02:52:25.194Z</updated>
    
    <content type="html"><![CDATA[<p>The sixth assignment of Linear Regression. The assignment is written in Rmarkdown, a smart syntax supported by RStudio helping with formula, plot visualization and plugin codes running.<br><a id="more"></a><br>You may also find the <a href="https://github.com/james20141606/somethingmore/tree/master/linear_regression_pdf" target="_blank" rel="noopener"><strong>PDF Version</strong></a> of this assignment. Which is more recommended. <strong>Or here</strong>:<br><div class="row"><iframe src="https://drive.google.com/file/d/1-lgCaX4w7FjBMdYdr330UWjqdkMCJtuf/preview" style="width:100%; height:550px"></iframe></div></p><h1 id="1"><a href="#1" class="headerlink" title="1"></a>1</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"># read the data</span><br><span class="line">setwd(&apos;~/Desktop/三春/5线性回归分析/作业/HW6/&apos;)</span><br><span class="line">dat&lt;-read.csv(&quot;hw6.csv&quot;)</span><br><span class="line">cases&lt;-dat$X1</span><br><span class="line">percent&lt;-dat$X2</span><br><span class="line">holiday&lt;-dat$X3</span><br><span class="line">labor&lt;-dat$Y</span><br><span class="line"># plot stem and leaf plots</span><br><span class="line">stem(cases)</span><br><span class="line">stem(percent)</span><br></pre></td></tr></table></figure><p>The plots are above.<br>There seems to be some outliers. For example for $X_2$, the values more than 9 and lower than 5 seem to be outliers. The gaps are obvious.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># plot time plots</span><br><span class="line">time&lt;-1:52</span><br><span class="line">plot(time,dat$X1,xlab=&quot;time (weeks)&quot;,ylab=&quot;X1&quot;)</span><br><span class="line">plot(time,dat$X2,xlab=&quot;time (weeks)&quot;,ylab=&quot;X2&quot;)</span><br><span class="line">plot(time,dat$X3,xlab=&quot;time (weeks)&quot;,ylab=&quot;X3&quot;)</span><br></pre></td></tr></table></figure><p>1) $X_1$ may depent on time. it has a tendency to be larger over time<br>2) $X_2$ is independent of time<br>3) $X_3$ seems independent of time </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">pairs(~cases+percent+holiday+labor,data=dat, </span><br><span class="line">   main=&quot;Simple Scatterplot Matrix&quot;)</span><br><span class="line">cor(dat[,1:4])</span><br></pre></td></tr></table></figure><p>It is obvious that $X_3$ and Y has a very strong correlation(it also makes sense). the others have no significant correlation.</p><h1 id="2"><a href="#2" class="headerlink" title="2"></a>2</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">dat.fit&lt;-lm(labor~cases+percent+holiday)</span><br><span class="line">sm&lt;-summary(dat.fit)</span><br><span class="line">sm</span><br><span class="line">resid1&lt;-dat.fit$residuals</span><br><span class="line">resid1</span><br></pre></td></tr></table></figure><p>a) The regression function is <script type="math/tex">Y=0.0007871X_1-13.17X_2+623.6X3+4150</script><br>$b_1, b_2, b_3$ are unbiased estimates of <script type="math/tex">\beta_1 , \beta_2 , \beta_3</script></p><p>b)<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">boxplot(resid1,ylab=&quot;residuals&quot;, pch=19)</span><br></pre></td></tr></table></figure></p><p>From the boxplot, we can know the median, maximum, minimum, 25 and 75 percent quantile of the residuals.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">plot(labor,resid1)</span><br><span class="line">plot(cases,resid1)</span><br><span class="line">plot(percent,resid1)</span><br><span class="line">plot(holiday,resid1)</span><br><span class="line">plot(cases*percent,resid1)</span><br><span class="line">qqnorm(resid1, main=&quot;Normal Probability Plot&quot;, pch=19)</span><br><span class="line">qqline(resid1)</span><br></pre></td></tr></table></figure><p>c) The plots show that the regression function may not be linear. The residuals change systematically as Y increases, as shown in the first plot. Also the normal probability plot shows that the residuals may not be strictly normally distributed.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">plot(time,resid1)</span><br></pre></td></tr></table></figure><p>d) There does not seem to be any indication that the error terms are correlated.</p><h1 id="3"><a href="#3" class="headerlink" title="3"></a>3</h1><p>a) </p><script type="math/tex; mode=display">H_0:\ \beta_1=\beta_2=\beta_3=0\;\;H_a:\ otherwise</script><script type="math/tex; mode=display">We\;reject\;H_0\;if\;F^*=\frac{MSR}{MSE}>F_{0.95,3,48}</script><p>Based on the result:”F-statistic: 35.34 on 3 and 48 DF,  p-value: 3.316e-12”, we reject H0 and conclude Ha.<br>The p value is 3.316e-12<br>The t-test result from above implies that <script type="math/tex">\beta_1 \; and\; \beta_3</script> are likely to be non-zero but $\beta_2$ may be zero.</p><p>b)<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">confint(dat.fit,c(2,4),level = 1-0.05/4)</span><br></pre></td></tr></table></figure></p><p>The family confidence interval is shown above. The family  confidence coefficient means that when doing many simulations, the proportion of samples which values fall correctly in the cofifence interval.</p><p>c.Coefficient of multiple determination is 0.6883. It can be viewed as a coefficient of simple determination between the responses and the fitted values.</p><h1 id="4"><a href="#4" class="headerlink" title="4"></a>4</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">plot(cases, percent,ylim=c(4,11))</span><br><span class="line">points(400000,7.2,pch=2)</span><br><span class="line">points(400000,9.9,pch=3)</span><br></pre></td></tr></table></figure><p>It is a plot of the two variables: X1 and X2. The cross and triangle represent the two points where predictions are to be made. It can be seen that the triangle lies well within the joint range of the two variables, but the cross seems to be out of the scope of the model.</p><h1 id="5"><a href="#5" class="headerlink" title="5"></a>5</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">new1 &lt;- data.frame(cases=230000,percent=7.5,holiday=0)</span><br><span class="line">new2 &lt;- data.frame(cases=250000,percent=7.3,holiday=0)</span><br><span class="line">new3 &lt;- data.frame(cases=280000,percent=7.1,holiday=0)</span><br><span class="line">new4 &lt;- data.frame(cases=340000,percent=6.9,holiday=0)</span><br><span class="line">predict(dat.fit, new1, se.fit = F, interval = &quot;prediction&quot;, level = 1-0.05/8)</span><br><span class="line">predict(dat.fit, new2, se.fit = F, interval = &quot;prediction&quot;, level = 1-0.05/8)</span><br><span class="line">predict(dat.fit, new3, se.fit = F, interval = &quot;prediction&quot;, level = 1-0.05/8)</span><br><span class="line">predict(dat.fit, new4, se.fit = F, interval = &quot;prediction&quot;, level = 1-0.05/8)</span><br></pre></td></tr></table></figure><p>The intervals are presented above.</p><h1 id="6"><a href="#6" class="headerlink" title="6"></a>6</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">new &lt;- data.frame(cases=282000,percent=7.1,holiday=0)</span><br><span class="line">predict(dat.fit, new, se.fit = T, interval = &quot;prediction&quot;, level = 1-0.05)</span><br><span class="line">mse &lt;- mean(dat.fit$residuals^2)</span><br><span class="line">mse</span><br></pre></td></tr></table></figure><p>We obtained MSE and se.fit.<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">lwr&lt;-4278.365-qt(1-0.05/2,df=48)*sqrt(mse/3+22.83758^2)</span><br><span class="line">upr&lt;-4278.365+qt(1-0.05/2,df=48)*sqrt(mse/3+22.83758^2)</span><br><span class="line">lwr</span><br><span class="line">upr</span><br></pre></td></tr></table></figure></p><p>a. The interval is (4112.088,4444.642)<br>b. Just multiply the interval by 3. We obtain (12336.27,13333.92)</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;The sixth assignment of Linear Regression. The assignment is written in Rmarkdown, a smart syntax supported by RStudio helping with formula, plot visualization and plugin codes running.&lt;br&gt;
    
    </summary>
    
      <category term="school work" scheme="http://james20141606.github.io/categories/school-work/"/>
    
    
      <category term="codes" scheme="http://james20141606.github.io/tags/codes/"/>
    
      <category term="R" scheme="http://james20141606.github.io/tags/R/"/>
    
      <category term="assignment" scheme="http://james20141606.github.io/tags/assignment/"/>
    
      <category term="statistics" scheme="http://james20141606.github.io/tags/statistics/"/>
    
      <category term="linear regression" scheme="http://james20141606.github.io/tags/linear-regression/"/>
    
  </entry>
  
  <entry>
    <title>Linear Regression Assignment 4</title>
    <link href="http://james20141606.github.io/2018/04/16/line0/"/>
    <id>http://james20141606.github.io/2018/04/16/line0/</id>
    <published>2018-04-16T04:59:19.000Z</published>
    <updated>2018-04-20T02:51:21.420Z</updated>
    
    <content type="html"><![CDATA[<p>The fourth assignment of Linear Regression. The assignment is written in Rmarkdown, a smart syntax supported by RStudio helping with formula, plot visualization and plugin codes running.<br><a id="more"></a><br>You may also find the <a href="https://github.com/james20141606/somethingmore/tree/master/linear_regression_pdf" target="_blank" rel="noopener"><strong>PDF Version</strong></a> of this assignment. Which is more recommended. <strong>Or here</strong>:<br><div class="row"><iframe src="https://drive.google.com/file/d/1opWnMpCRo6zNydUWgrIrpeBQenV_tB04/preview" style="width:100%; height:550px"></iframe></div></p><figure class="highlight plain"><figcaption><span>setup, include</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">knitr::opts_chunk$set(echo = TRUE)</span><br></pre></td></tr></table></figure><h1 id="3-6"><a href="#3-6" class="headerlink" title="3.6"></a>3.6</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">hardness&lt;- read.table(&apos;CH01PR22_947709365.txt&apos;)</span><br><span class="line">names(hardness) &lt;- c(&quot;y&quot;,&quot;x&quot;)</span><br><span class="line">is.data.frame(hardness)</span><br><span class="line">hardness.fit &lt;- lm(y~x,data=hardness)</span><br><span class="line">sumtable&lt;-summary(hardness.fit)</span><br><span class="line">sumtable</span><br></pre></td></tr></table></figure><h2 id="a"><a href="#a" class="headerlink" title="a"></a>a</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">boxplot(resid(hardness.fit),main=&quot;Box Plot of Hardness Data&quot;, ylab=&quot;Residuals&quot;)</span><br></pre></td></tr></table></figure><p>The mean of residuals is zero</p><h2 id="b"><a href="#b" class="headerlink" title="b"></a>b</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">yhat &lt;- fitted(hardness.fit); resid &lt;- resid(hardness.fit)</span><br><span class="line">plot(yhat,resid,main=&quot;Plot of residuals against the fitted values Yhat&quot;, </span><br><span class="line">     ylab=&quot;Residuals&quot;,xlab=&apos;yhat&apos;)</span><br></pre></td></tr></table></figure><p>There is one residual equals to 5.575 a little higher than others.</p><h2 id="c"><a href="#c" class="headerlink" title="c"></a>c</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">hardness.stdres = rstandard(hardness.fit)</span><br><span class="line">qqnorm(resid, </span><br><span class="line">       ylab=&quot;Residuals&quot;, </span><br><span class="line">       xlab=&quot;Expected&quot;, </span><br><span class="line">       main=&quot; Normal Probability Plot of Residuals&quot;) </span><br><span class="line">qqline(resid)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">StdErr = summary(hardness.fit)$sigma</span><br><span class="line">n = 16</span><br><span class="line">ExpVals = sapply(1:n, function(k) StdErr * qnorm((k-.375)/(n+.25)))</span><br><span class="line">cor(ExpVals,sort(resid))</span><br></pre></td></tr></table></figure><p>With n=16, from Table B.6, the critical value for the coefficient of correlation between the ordered residuals and the expected values under normality when the distribution of error terms is normal using a 0.05 significance level is 0.941. Since 0.9916733 &gt; 0.941, the assumption of normality appeared reasonable.</p><h1 id="4-5"><a href="#4-5" class="headerlink" title="4.5"></a>4.5</h1><h2 id="a-1"><a href="#a-1" class="headerlink" title="a"></a>a</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">interval &lt;- function(dat)&#123;</span><br><span class="line">  names(dat) &lt;- c(&quot;y&quot;,&quot;x&quot;)</span><br><span class="line">  is.data.frame(dat)</span><br><span class="line">  data.fit &lt;- lm(y~x,data=dat)</span><br><span class="line">  sumtable&lt;-summary(data.fit)</span><br><span class="line">  coef &lt;- sumtable$coefficients</span><br><span class="line">  signif&lt;-1-(1-0.9)/4.0</span><br><span class="line">  tvalue &lt;-qt(signif, 14)</span><br><span class="line">  beta_0 = list(coef[1]-tvalue*coef[3],coef[1]+tvalue*coef[3])</span><br><span class="line">  beta_1 = list(coef[2]-tvalue*coef[4],coef[2]+tvalue*coef[4])</span><br><span class="line">  return(list(beta_0,beta_1))</span><br><span class="line">&#125;</span><br><span class="line">interval(hardness)</span><br></pre></td></tr></table></figure><p>From R result, $b_0 = 168.6, s(b_0)=2.65702, b_1 = 2.03438, s(b_1)=0.09039.$ Since<br>t(0.975, 14) = 2.145, Bonferroni joint confidence intervals for $β_0$ and $β_1$, using a 90% percent family confidence coefficient, are 168.6±2.145(2.65702) = [162.901, 174.299] for $β_0$ and 2.03438±2.145(0.09039) = [1.840, 2.228] for $β_1$. At least 90% of the time, both coefficients will be within the limits stated.</p><h2 id="c-1"><a href="#c-1" class="headerlink" title="c"></a>c</h2><p>The 90% joint confidence interval means that both will be in the interval at least 90% of the time.<br>Restated, at least one of them will be out of the interval no more than 10% of the time. We cannot get more specific than this.</p><h1 id="4-9"><a href="#4-9" class="headerlink" title="4.9"></a>4.9</h1><h2 id="a-2"><a href="#a-2" class="headerlink" title="a"></a>a</h2><p>For Bonferroni, use $b_0+b_1X_j±t(1−0.1/6, 14)s{\hat Y_h}$, with t(1−.10/6, 14) = 2.35982.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">meaninterval &lt;-function(X_h)&#123;</span><br><span class="line">mse &lt;- mean(sumtable$residuals^2)</span><br><span class="line">sYh &lt;- (mse * ( 1/16.0 + (X_h -ave(hardness$x)[1])**2/(sum((hardness$x - ave(hardness$x))**2)/16.0)) )**0.5</span><br><span class="line">beta_0 = coef[1]</span><br><span class="line">beta_1 = coef[3]</span><br><span class="line">list(beta_0 +beta_1*X_h - qt(1-.10/6, 14) * sYh,beta_0 +beta_1*X_h + qt(1-.10/6, 14) * sYh)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>Using this function we have CI of 20,30,40 are [215.1106，228.3704], [245.9163，250.7052], [265.1384，284.6236] respectively.</p><p>The 90% joint confidence interval means that all three mean hardness will be in their respective interval at least 90% of the time.Restated, at least one of them will be out of the interval no more than 10% of the time. We cannot get more specific than this.</p><h1 id="4-12"><a href="#4-12" class="headerlink" title="4.12"></a>4.12</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">galleys.x &lt;- c(7, 12, 10, 10, 14, 25, 30, 25, 18, 10, 4, 6)</span><br><span class="line">cost.y &lt;- c(128, 213, 191, 178, 250 , 446, 540, 457, 324, 177, 75, 107)</span><br></pre></td></tr></table></figure><h2 id="a-3"><a href="#a-3" class="headerlink" title="a"></a>a</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">typos.lm &lt;- lm(cost.y~galleys.x-1)</span><br><span class="line">typos.lm</span><br></pre></td></tr></table></figure><script type="math/tex; mode=display">\hat Y_h=18.03X</script><h2 id="b-1"><a href="#b-1" class="headerlink" title="b"></a>b</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">plot(galleys.x,cost.y,xlab= &quot;Galleys&quot;, ylab=&quot;Cost&quot;)</span><br><span class="line">abline(typos.lm)</span><br></pre></td></tr></table></figure><p>It appears that the model fits good</p><h2 id="c-2"><a href="#c-2" class="headerlink" title="c"></a>c</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">historical.norm &lt;- data.frame(galleys.x=1)</span><br><span class="line">alpha &lt;- 0.02</span><br><span class="line">typos.int &lt;- predict.lm(typos.lm, newdata = historical.norm , interval = &quot;confidence&quot;, level = 1-alpha)</span><br><span class="line">typos.int</span><br></pre></td></tr></table></figure><p>Alternatives: $H<em>0:E[Y]=β</em>{10}=17.50$ </p><p>$H<em>0:E[Y]≠β</em>{10}=17.50$  </p><p>CI: $17.81226≤E[Y_h]≤18.24435$ </p><p>Decision rule:</p><p>If $β_{10}$ falls within the confidence interval for $E[Y_h]$, conclude $H_0$;</p><p>If $β_{10}$ does not fall within the confidence interval for $E[Y_h]$, conclude $H_A$ </p><p>Conclusion:</p><p>Since 17.50&lt;17.81226; therefore, accept $H_A$.</p><h2 id="d"><a href="#d" class="headerlink" title="d"></a>d</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">newdata.galleys &lt;- data.frame(galleys.x=10)</span><br><span class="line">typos.pred &lt;- predict(typos.lm, newdata.galleys, level=0.98, interval = &quot;predict&quot;, se.fit = TRUE)  </span><br><span class="line">typos.pred</span><br></pre></td></tr></table></figure><p>$\hat Y_h=180.283$</p><p>s[pred]=4.506806</p><p>180.283±2.738769(4.506806)</p><p>$167.8441≤Y_{h(new)}≤192.722$</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;The fourth assignment of Linear Regression. The assignment is written in Rmarkdown, a smart syntax supported by RStudio helping with formula, plot visualization and plugin codes running.&lt;br&gt;
    
    </summary>
    
      <category term="school work" scheme="http://james20141606.github.io/categories/school-work/"/>
    
    
      <category term="codes" scheme="http://james20141606.github.io/tags/codes/"/>
    
      <category term="R" scheme="http://james20141606.github.io/tags/R/"/>
    
      <category term="assignment" scheme="http://james20141606.github.io/tags/assignment/"/>
    
      <category term="statistics" scheme="http://james20141606.github.io/tags/statistics/"/>
    
      <category term="linear regression" scheme="http://james20141606.github.io/tags/linear-regression/"/>
    
  </entry>
  
  <entry>
    <title>Multivariate Statistics Assignment 3</title>
    <link href="http://james20141606.github.io/2018/04/16/multi0/"/>
    <id>http://james20141606.github.io/2018/04/16/multi0/</id>
    <published>2018-04-16T04:59:19.000Z</published>
    <updated>2018-04-20T02:55:01.954Z</updated>
    
    <content type="html"><![CDATA[<p>The third assignment of Multivariate Statistics. The assignment is written in Rmarkdown, a smart syntax supported by RStudio helping with formula, plot visualization and plugin codes running.<br><a id="more"></a><br>You may also find the <a href="https://github.com/james20141606/somethingmore/tree/master/linear_regression_pdf" target="_blank" rel="noopener"><strong>PDF Version</strong></a> of this assignment. Which is more recommended. <strong>Or here</strong>:<br><div class="row"><iframe src="https://drive.google.com/file/d/128itrGhENm_IZbs-si0gkS4X5NHdDfMU/preview" style="width:100%; height:550px"></iframe></div></p><h1 id="1"><a href="#1" class="headerlink" title="1"></a>1</h1><p>density function for the multivariate normal distribution:</p><script type="math/tex; mode=display">f(x; \mu, \Sigma) =\frac{1}{(2\pi)^{p/2}|\Sigma|^{1/2}}e^{-\frac{1}{2}(x-\mu)^{'}\Sigma^{-1}(x-\mu)}</script><p>the likelihood function for the two independent sample:</p><script type="math/tex; mode=display">L(\mu_1,\mu_2,\Sigma) = \Pi_{i=1}^{n_1}f(X_i;\mu_1,\Sigma)\Pi_{j=1}^{n_2}f(X_j;\mu_2,\Sigma) \\=L(\mu_1,\Sigma)L(\mu_2,\Sigma)</script><p>So the likelihood function can be defined as:</p><script type="math/tex; mode=display">L(\mu_1,\mu_2,\Sigma) = \frac{1}{(2\pi)^{Np/2}|\Sigma|^{N/2}}exp(  -\frac{1}{2}tr[  \Sigma^{-1} (\sum_{i=1}^{n_1}\Phi_1(X_i)+\sum_{j=1}^{n_2}\Phi_2(X_j))  ]   ) \\\Phi_i(x) = (x-\mu_i)(x-\mu_i)^{'}</script><p>Using MLE, the maximum is:</p><script type="math/tex; mode=display">\hat \Sigma = \frac{1}{n_1+n_2} (\sum_{i=1}^{n_1} \Phi_1(X_i)+\sum_{j=1}^{n_2}\Phi_2(X_j)) \\=\frac{1}{n_1+n_2}[(n_1-1)S_1+(n_2-1)S_2] = \frac{n_1+n_2-1}{n_1+n_2}S_{pooled}</script><h1 id="2"><a href="#2" class="headerlink" title="2"></a>2</h1><h2 id="a"><a href="#a" class="headerlink" title="(a)"></a>(a)</h2><p>It can be calculated that:</p><script type="math/tex; mode=display">\bar x=[4.64,45.4,9.965]^{T}\\S=\left[\begin{matrix}2.879 &10.01 &-1.81 \\10.01&199.788&-5.64 \\-1.81&-5.64&3.682\end{matrix}\right]</script><p>we can calculate S’s eigen value and its respective eigen vector:</p><script type="math/tex; mode=display">value = [ 200.46209264,    1.31804876,    4.56885859]\\e_1^{'} = [-0.05084165, -0.82194341, -0.56729548] \\e_2^{'} = [-0.99828327,  0.02528569,  0.0528313 ] \\e_3^{'} = [ 0.02907988, -0.56900761,  0.82181792] \\</script><p>The axes of the region are:</p><script type="math/tex; mode=display">\sqrt \lambda_i \sqrt {\frac{p(n-1)}{n(n-p)}F_{p,n-p}(\alpha)} \\\frac{p(n-1)}{n(n-p)}F_{p,n-p}(\alpha) = \frac{19 \times 3}{17 \times 20} \times F_{3,17}(0.1) = 0.167 \times 2.44 = 0.409</script><p>so the axes’ lengths are:</p><script type="math/tex; mode=display">\sqrt {200.46209264} \times \sqrt {0.409} = 9.055 \\\sqrt {1.31804876} \times \sqrt {0.409} = 0.734 \\\sqrt {4.56885859} \times \sqrt {0.409} = 1.367</script><p>The directions of each aixs is determined by its corresponding eigen vector shown above.</p><h2 id="b"><a href="#b" class="headerlink" title="(b)"></a>(b)</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"># read the data</span><br><span class="line">setwd(&apos;~/Desktop/三春/3多元统计分析/作业/作业3-1,3-2/&apos;)</span><br><span class="line">dat&lt;-read.csv(&quot;data.csv&quot;)</span><br><span class="line">x1&lt;-dat$x1</span><br><span class="line">x2&lt;-dat$x2</span><br><span class="line">x3&lt;-dat$x3</span><br><span class="line">qqnorm(x1, main=&quot;Normal Probability Plot&quot;, pch=19)</span><br><span class="line">qqline(x1)</span><br><span class="line">qqnorm(x2, main=&quot;Normal Probability Plot&quot;, pch=19)</span><br><span class="line">qqline(x2)</span><br><span class="line">qqnorm(x3, main=&quot;Normal Probability Plot&quot;, pch=19)</span><br><span class="line">qqline(x3)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">plot(x1,x2)</span><br><span class="line">plot(x1,x3)</span><br><span class="line">plot(x2,x3)</span><br></pre></td></tr></table></figure><p>It seems that each variable’s normality is fine and they don’t have a significant relationship with each other, so the multivariate normal assumption seems justied.</p><h1 id="3"><a href="#3" class="headerlink" title="3"></a>3</h1><script type="math/tex; mode=display">T^2 = \sqrt n (\bar X -\mu_0)^{'}{(\frac{\sum_{j=1}^{n}(X_j-\bar X)(X_j-\bar X)^{'}}{n-1})^{-1}} \sqrt n (\bar X -\mu_0)\\\text{So it can be calculated that }T^2 = 9.74 \\\frac{p(n-1)}{(n-p)}F_{p,n-p}(\alpha) = \frac{19 \times 3}{17} \times F_{3,17}(0.05) = \frac{19 \times 3}{17} \times 3.20 = 10.729\</script><p>the confidence region is defined as :</p><script type="math/tex; mode=display">(\bar x_i - \sqrt {\frac{p(n-1)}{n(n-p)} F_{p,n-p}(\alpha)} \sqrt{\frac{s_{ii}}{n}},\bar x_i + \sqrt {\frac{p(n-1)}{n(n-p)} F_{p,n-p}(\alpha)} \sqrt{\frac{s_{ii}}{n}}) \\F_{p,n-p}(\alpha) = F_{3,17}(0.05) =3.20,  \ \bar x_1 = 4.64, \ \bar x_2 = 45.4,\ \bar x_3 =9.965 \\s_{11} = 2.879, \ s_{22} = 199.788, \  s_{33} = 3.628</script><p>so the three regions are:</p><script type="math/tex; mode=display">(\bar x_1 - \sqrt {\frac{p(n-1)}{n(n-p)} F_{p,n-p}(\alpha)} \sqrt{\frac{s_{11}}{n}},\bar x_1 + \sqrt {\frac{p(n-1)}{n(n-p)} F_{p,n-p}(\alpha)} \sqrt{\frac{s_{11}}{n}}) \\= [3.397, 5.882]\\(\bar x_2 - \sqrt {\frac{p(n-1)}{n(n-p)} F_{p,n-p}(\alpha)} \sqrt{\frac{s_{22}}{n}},\bar x_2 + \sqrt {\frac{p(n-1)}{n(n-p)} F_{p,n-p}(\alpha)} \sqrt{\frac{s_{22}}{n}}) \\= [35.047, 55.752]\\(\bar x_3 - \sqrt {\frac{p(n-1)}{n(n-p)} F_{p,n-p}(\alpha)} \sqrt{\frac{s_{22}}{n}},\bar x_3 + \sqrt {\frac{p(n-1)}{n(n-p)} F_{p,n-p}(\alpha)} \sqrt{\frac{s_{33}}{n}}) \\= [8.569, 11.360]\\</script><h2 id="b-1"><a href="#b-1" class="headerlink" title="(b)"></a>(b)</h2><p>The Bonferroni region is defined as :</p><script type="math/tex; mode=display">[\bar x_p -t_{n-1}(\frac{\alpha}{2p})\sqrt{\frac{s_{pp}}{n}}, \ \bar x_p +t_{n-1}(\frac{\alpha}{2p})\sqrt{\frac{s_{pp}}{n}}] \\t_{19}(0.0083) = 2.625106</script><p>so the Bonferroni regions are:</p><script type="math/tex; mode=display">[3.644, 5.635] \\[37.103, 53.696] \\[8.846, 11.083]</script><p>which are smaller than $T^2$ confidence region because it focus on single confidence interval.</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;The third assignment of Multivariate Statistics. The assignment is written in Rmarkdown, a smart syntax supported by RStudio helping with formula, plot visualization and plugin codes running.&lt;br&gt;
    
    </summary>
    
      <category term="school work" scheme="http://james20141606.github.io/categories/school-work/"/>
    
    
      <category term="codes" scheme="http://james20141606.github.io/tags/codes/"/>
    
      <category term="R" scheme="http://james20141606.github.io/tags/R/"/>
    
      <category term="assignment" scheme="http://james20141606.github.io/tags/assignment/"/>
    
      <category term="statistics" scheme="http://james20141606.github.io/tags/statistics/"/>
    
      <category term="linear regression" scheme="http://james20141606.github.io/tags/linear-regression/"/>
    
  </entry>
  
  <entry>
    <title>春日の恋想</title>
    <link href="http://james20141606.github.io/2018/04/15/peomofmeng/"/>
    <id>http://james20141606.github.io/2018/04/15/peomofmeng/</id>
    <published>2018-04-15T05:54:30.000Z</published>
    <updated>2018-04-16T06:20:20.616Z</updated>
    
    <content type="html"><![CDATA[<p>就像pdf文档里所说的那样，这组诗作于2013年春⽇在郑州外国语学校就读期间，我十七岁，孟孟年方二八，正是豆蔻年华，如今看起来有些羞涩的文字却是那个时候的真情流露，并不觉得夸张。人们总是喜欢用青涩形容自己年轻的岁月，我却一直秉持着大胆地做自己想做的事情的原则，因此青春中做了很多美好的值得回味的，不落于俗套的美好事情。这一路来所做所想，都是自己所爱，能够在自己多年来心仪的学校继续和多年来心爱的姑娘经历生活的美好与平淡，实在是人生的幸运。<br><a id="more"></a><br>原诗曾记录整理于高中的电⼦词典中，并⼀⼀抄录于⼿折玫瑰中赠予孟孟。那个春天每天晚上一朵纸折的玫瑰，如今还能回忆起春风沉醉，空气温暖，眼眸明亮，笑容可人，这些是刻在记忆里了，以及这些文字，在2018 年1 ⽉30 ⽇整理旧寝室的物品的时候，发现还完好无损地躺在我的电子词典中，于是导出，重新用LaTeX排了版，如今也放在这里，希望可以和记忆一起永恒。</p><div class="row"><iframe src="https://drive.google.com/file/d/1lNCiSdpfv9scaQ1fHKt49UWHLv59rKjv/preview" style="width:100%; height:550px"></iframe></div>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;就像pdf文档里所说的那样，这组诗作于2013年春⽇在郑州外国语学校就读期间，我十七岁，孟孟年方二八，正是豆蔻年华，如今看起来有些羞涩的文字却是那个时候的真情流露，并不觉得夸张。人们总是喜欢用青涩形容自己年轻的岁月，我却一直秉持着大胆地做自己想做的事情的原则，因此青春中做了很多美好的值得回味的，不落于俗套的美好事情。这一路来所做所想，都是自己所爱，能够在自己多年来心仪的学校继续和多年来心爱的姑娘经历生活的美好与平淡，实在是人生的幸运。&lt;br&gt;
    
    </summary>
    
      <category term="life" scheme="http://james20141606.github.io/categories/life/"/>
    
    
      <category term="life" scheme="http://james20141606.github.io/tags/life/"/>
    
      <category term="girl friend" scheme="http://james20141606.github.io/tags/girl-friend/"/>
    
      <category term="poem" scheme="http://james20141606.github.io/tags/poem/"/>
    
  </entry>
  
  <entry>
    <title>Data Mining of Deng Era</title>
    <link href="http://james20141606.github.io/2018/04/15/datamining/"/>
    <id>http://james20141606.github.io/2018/04/15/datamining/</id>
    <published>2018-04-14T16:55:43.000Z</published>
    <updated>2018-04-15T08:26:50.362Z</updated>
    
    <content type="html"><![CDATA[<p>期中作业之一是写一篇邓小平时代的读后感，说实话这种书是实在没空读了，虽然粗略地翻了几章，十分吸引人，但是这两年真的越来越讨厌写文科式的论文，瞎胡诌凑字数曾经也是我作为理科生的优势，但是这一两年对这种风格的文章：东拼西凑，无病呻吟，迷茫又自负的写作非常地厌恶。因为就想玩点花样，做点简单的文本数据挖掘凑凑字数，虽然多花了很多时间，但是毕竟很有意思，有意思的事情就不算浪费时间对吧，没有意思的事情，哪怕一分钟也是对生命的浪费呢。<br><a id="more"></a><br>中文分词是个很好玩的事情，但是jieba和THULAC之类的工具已经把中文分词和词性标注之类的变得很简单，最折腾的，花了我很久时间的是这本中文材料。。。matplotlib本身不支持英文绘图，python的encoding方式也让我折腾了很久，竟然做了很久装卸各种包的工作，，，直到最后奇葩的matplotlib就是找不到字体，不管在本地还是在几个服务器上竟然都不行，于是只好测试好代码让斌斌帮忙在他的账户跑一下。下面简单记一下过程和代码，最后再把自己胡乱拼凑的论文也扔上。</p><p>代码也放到<a href="https://github.com/james20141606/somethingmore/datamining_dxp" target="_blank" rel="noopener">GitHub</a>上了,里面附有jupyter版本的代码和可以直接运行产生各种类型图片的代码。</p><h1 id="对邓小平时代的分词与词频统计"><a href="#对邓小平时代的分词与词频统计" class="headerlink" title="对邓小平时代的分词与词频统计"></a>对邓小平时代的分词与词频统计</h1><h2 id="数据准备"><a href="#数据准备" class="headerlink" title="数据准备"></a>数据准备</h2><p><strong>convert to UTF-8 format</strong><br><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">file ip.txt</span><br><span class="line"><span class="comment">#use vim to change encoding format</span></span><br><span class="line">:<span class="builtin-name">set</span> <span class="attribute">fileencoding</span>=utf-8</span><br></pre></td></tr></table></figure></p><p>首先是找到txt版本的邓小平时代资源，用utf-8编码，方便后续处理。</p><h2 id="分词与词频统计"><a href="#分词与词频统计" class="headerlink" title="分词与词频统计"></a>分词与词频统计</h2><p>想做词频统计分析，就得对文本进行分词，中文和英文不同，英文单词是孤立的，而中文单词需要人工分开，这里我找了一个比较经典的分词方法，<strong>Jieba分词</strong>，对整本书进行了分词处理，把每句话都给分开，存储了各个名词，并且顺便统计了一下出现频次前10000的所有词语。因此我通过代码可以获取以下<strong>两个文件</strong>：</p><p>被分词分开的全书“词汇”，按顺序一个个存储起来，以及对各个词汇出现频次的统计文件。接下来就可以对数据进行进一步的分析。</p><p><strong>Use Jieba for Chinese words partition</strong><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os, codecs  </span><br><span class="line"><span class="keyword">import</span> jieba  </span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> Counter </span><br><span class="line"><span class="keyword">import</span> matplotlib.pylab <span class="keyword">as</span> plt</span><br><span class="line">plt.style.use(<span class="string">'ggplot'</span>)</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> matplotlib</span><br><span class="line">plt.rcParams[<span class="string">'font.style'</span>] = <span class="string">u'normal'</span></span><br><span class="line">plt.rcParams[<span class="string">'font.family'</span>] = <span class="string">u'Microsoft YaHei'</span></span><br><span class="line"><span class="keyword">with</span> codecs.open(<span class="string">'output.txt'</span>, <span class="string">'r'</span>, <span class="string">'utf8'</span>) <span class="keyword">as</span> f:  </span><br><span class="line">    txt = f.read() </span><br><span class="line">seg_list = jieba.cut(txt) </span><br><span class="line">c = Counter()  </span><br><span class="line"><span class="keyword">for</span> x <span class="keyword">in</span> seg_list:  </span><br><span class="line">    <span class="keyword">if</span> len(x)&gt;<span class="number">1</span> <span class="keyword">and</span> x != <span class="string">'\r\n'</span>:  </span><br><span class="line">        c[x] += <span class="number">1</span></span><br><span class="line">np.savetxt(<span class="string">'count10000.txt'</span>,np.array(c.most_common(<span class="number">10000</span>)),fmt=<span class="string">'%s'</span>)</span><br><span class="line">data = np.loadtxt(<span class="string">'count10000.txt'</span>,dtype=<span class="string">'str'</span>)</span><br><span class="line"><span class="keyword">with</span> codecs.open(<span class="string">'output.txt'</span>, <span class="string">'r'</span>, <span class="string">'utf8'</span>) <span class="keyword">as</span> f:  </span><br><span class="line">    txt = f.read() </span><br><span class="line">wordlist = np.array(txt.split(<span class="string">' '</span>))</span><br><span class="line"><span class="comment">#wordlist.shape</span></span><br><span class="line">countlist = []</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">10000</span>):</span><br><span class="line">    countlist.append(data[i,<span class="number">0</span>]+<span class="string">': '</span>+str(data[i,<span class="number">1</span>]))</span><br><span class="line">pd.DataFrame(np.array(countlist)[:<span class="number">200</span>].reshape(<span class="number">20</span>,<span class="number">10</span>)).head()</span><br></pre></td></tr></table></figure></p><p>然后选取最靠前的200个词语制出来一张表格，从这个表格里还是可以看出一些信息量的，还是很有趣的。比如毛泽东作为中国近现代史的第一人物，是本书除了邓小平之外绕不开的第二号人物。干部一词也反复出现，在中国这是个非常重要的词语，很多东西都取决于干部之间的博弈和关系。北京作为政治中心和中国的代名词，自然也反复出现，而国家和地区层面，美国，苏联、日本和中国台湾也榜上有名，广东作为非常重要的试验地点，被提及的频率也相当的高。人物上，胡耀邦、陈云、赵紫阳、周恩来也都出现了多次。军队、学生等关键词也出现次数不少。</p><p>除此之外还有年份也引人关注，比如1975、1977、1979、1980、1989等关键节点也都帮上有名。</p><p><img src="http://i4.bvimg.com/640680/f405c02d19042f6b.png" alt="Markdown"></p><h2 id="结果可视化"><a href="#结果可视化" class="headerlink" title="结果可视化"></a>结果可视化</h2><p><strong>除此之外，我还对一些非常重要的关键词画了一些可视化的图，这里选取一些放上来。</strong></p><h3 id="bar-plot"><a href="#bar-plot" class="headerlink" title="bar plot"></a>bar plot</h3><figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">namelist = [<span class="string">u'邓小平'</span>,<span class="string">u'中国'</span>,<span class="string">u'毛泽东'</span>,<span class="string">u'工作'</span>,<span class="string">u'干部'</span>,<span class="string">u'问题'</span>,<span class="string">u'北京'</span>,<span class="string">u'美国'</span>,<span class="string">u'领导人'</span>,<span class="string">u'会议'</span>,<span class="string">u'经济'</span>,<span class="string">u'关系'</span>,<span class="string">u'香港'</span>,<span class="string">u'1975'</span>,<span class="string">u'领导'</span>,<span class="string">u'胡耀邦'</span>,<span class="string">u'苏联'</span>,<span class="string">u'政治'</span>,<span class="string">u'支持'</span>,</span><br><span class="line"><span class="string">u'军队'</span>,<span class="string">u'陈云'</span>,<span class="string">u'政策'</span>,<span class="string">u'赵紫阳'</span>,<span class="string">u'周恩'</span>,<span class="string">u'讲话'</span>,<span class="string">u'学生'</span>,<span class="string">u'华国锋'</span>,<span class="string">u'改革'</span>,<span class="string">u'日本'</span>]</span><br><span class="line">index_25 = [<span class="number">0</span>,<span class="number">1</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">6</span>,<span class="number">7</span>,<span class="number">8</span>,<span class="number">11</span>,<span class="number">12</span>,<span class="number">14</span>,<span class="number">16</span>,<span class="number">20</span>,<span class="number">21</span>,<span class="number">23</span>,<span class="number">26</span>,<span class="number">27</span>,<span class="number">28</span>,<span class="number">31</span>,<span class="number">33</span>,<span class="number">37</span>,<span class="number">39</span>,<span class="number">40</span>,<span class="number">42</span>,<span class="number">46</span>,<span class="number">47</span>,<span class="number">48</span>,<span class="number">49</span>]</span><br><span class="line">count = <span class="number">27</span></span><br><span class="line">fig,ax=plt.subplots(<span class="number">1</span>,figsize=(<span class="number">20</span>,<span class="number">10</span>))</span><br><span class="line">ax.bar(range(count),data[index_25,<span class="number">1</span>].astype(<span class="string">'int'</span>),color = <span class="string">'b'</span>)</span><br><span class="line"><span class="comment">#ax.bar(range(count),data[:count,1].astype('int'))</span></span><br><span class="line">ax.set_xticks(range(count))</span><br><span class="line">ax.set_xticklabels(namelist)</span><br><span class="line"><span class="comment">#plt.savefig('tst.png')</span></span><br><span class="line">ax.set_title(str(count)+<span class="string">' key words frequency in book'</span>)</span><br></pre></td></tr></table></figure><p>比如这个显示前二十个关键词的bar plot，可以发现相当有趣的现象，在一本书中的关键词分布竟然也挺像幂率分布，某两三个关键词频次非常高，然后是一堆比较重要的关键词，这个也很有趣。<br><img src="http://i4.bvimg.com/640680/9190c33461d494bd.png" alt="Markdown"></p><h3 id="fluctuation"><a href="#fluctuation" class="headerlink" title="fluctuation"></a>fluctuation</h3><p>接下来我还画了重要词汇再不同章节的变化图。这个的难点是要先获取每一章的起始和结束的位置（不是书本的页码，而是自己分割出来的“单词表”上的位置）<br><figure class="highlight maxima"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">chapterind = <span class="built_in">np</span>.<span class="built_in">array</span>([<span class="number">16590</span>,  <span class="number">31267</span>,  <span class="number">54053</span>,<span class="number">69769</span>,  <span class="number">90171</span>, <span class="number">104745</span>,<span class="number">121010</span>, <span class="number">138136</span>,  <span class="number">147048</span>,  <span class="number">161724</span>,<span class="number">170963</span>,  <span class="number">193593</span>, <span class="number">206502</span>, <span class="number">214129</span>,<span class="number">230193</span>, </span><br><span class="line">                <span class="number">245828</span>,  <span class="number">260400</span>,<span class="number">285768</span>, <span class="number">303284</span>, <span class="number">324922</span>, <span class="number">337101</span>, <span class="number">349241</span>, <span class="number">362426</span>, <span class="number">377184</span>])-<span class="number">1</span></span><br><span class="line">def count_frequent(chap):</span><br><span class="line">    freqlist =[]</span><br><span class="line">    <span class="keyword">if</span> chap &lt;<span class="number">23</span>:</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">27</span>):</span><br><span class="line">            freqlist.<span class="built_in">append</span>(<span class="built_in">np</span>.where(wordlist[chapterind[chap]:chapterind[chap+<span class="number">1</span>]] ==namelist[i])[<span class="number">0</span>].shape[<span class="number">0</span>])</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">27</span>):</span><br><span class="line">            freqlist.<span class="built_in">append</span>(<span class="built_in">np</span>.where(wordlist[chapterind[chap]:] ==namelist[i])[<span class="number">0</span>].shape[<span class="number">0</span>])</span><br><span class="line">    <span class="built_in">return</span> <span class="built_in">np</span>.<span class="built_in">array</span>(freqlist)</span><br><span class="line"></span><br><span class="line">freq_var = <span class="built_in">np</span>.ndarray([<span class="number">24</span>,<span class="number">27</span>])</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">24</span>):</span><br><span class="line">    freq_var[i] = count_frequent(i)</span><br><span class="line"></span><br><span class="line">from sklearn.preprocessing import MinMaxScaler</span><br><span class="line">scaler = MinMaxScaler()</span><br><span class="line">transformed = scaler.fit_transform(freq_var)</span><br><span class="line"></span><br><span class="line">fig,ax=plt.subplots(<span class="number">1</span>,figsize =(<span class="number">10</span>,<span class="number">10</span>))</span><br><span class="line">ax.matshow(transformed.T ,cmap ='jet')</span><br><span class="line">ax.set_title('<span class="number">27</span> <span class="built_in">key</span> words fluctuation <span class="keyword">in</span> <span class="number">24</span> chapters')</span><br><span class="line">ax.set_xticks(<span class="built_in">range</span>(<span class="number">24</span>))</span><br><span class="line">ax.set_yticks(<span class="built_in">range</span>(<span class="number">27</span>))</span><br><span class="line">ax.set_yticklabels(namelist)</span><br></pre></td></tr></table></figure></p><h4 id="heatmap"><a href="#heatmap" class="headerlink" title="heatmap"></a>heatmap</h4><p>经过一番折腾就可以统计出来27个关键词在24章的词频的变化，然后先画了一个<strong>heatmap热力图</strong>，这里为了避免某些关键词，比如邓小平出现频次太多影响到其他关键词的颜色，对每行做了归一化的处理（Minmaxscale）。<br><img src="http://i4.bvimg.com/640680/231ecc794e04d4d7.png" alt="Markdown"></p><p>这个图每一行是一个关键词，每一列是一章。信息量也是蛮大的，比如毛泽东在前面几章出现频次极其的高，后面由于趋势的原因，提的渐渐少了很多，变化相当明显。再比如支持一词，在后面的章节出现很多，可以推理强调邓小平受到他人支持以及支持他人推进改革的次数不少。学生这个关键词在19-21章出现非常多，闭着眼睛也知道这几张在讲什么（政治的潮起潮落、北京之春和天安门事件）。总之用heatmap图的方法也可以粗略地对关键词，尤其是关键词在每章中的变化做一些分析，更加细致的分析可以通过索引回一开始产生的全书词汇找到前后文再仔细看。</p><h4 id="折线图"><a href="#折线图" class="headerlink" title="折线图"></a>折线图</h4><p>接下来又绘制了一个更加直观的折线图，展示不同关键词在不同章节的变化情况，但是由于混杂在一起，可能不如热力图易读。<br><figure class="highlight stata"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">fig,ax=plt.subplots(1,figsize =(20,10))</span><br><span class="line">#ax.<span class="keyword">plot</span>(freq_var[:,:10])</span><br><span class="line"><span class="keyword">count</span> =10</span><br><span class="line"><span class="keyword">for</span> x,y <span class="keyword">in</span> <span class="keyword">zip</span>(freq_var[:,:<span class="keyword">count</span>].T,namelist[:<span class="keyword">count</span>]):</span><br><span class="line">    plt.<span class="keyword">plot</span>(x,<span class="keyword">label</span> =y)</span><br><span class="line">plt.title(str(<span class="keyword">count</span>)+' key words fluctuation <span class="keyword">in</span> 24 chapters')</span><br><span class="line">plt.legend()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure></p><p><img src="http://i4.bvimg.com/640680/c79d691ce875618e.png" alt="Markdown"></p><h2 id="进一步分析"><a href="#进一步分析" class="headerlink" title="进一步分析"></a>进一步分析</h2><h3 id="定义关键词之间的关系"><a href="#定义关键词之间的关系" class="headerlink" title="定义关键词之间的关系"></a>定义关键词之间的关系</h3><p>之前做的是一些基本的分析，我又思考了一下，能不能怎样表示一下两个关键词之间的关系呢？因为时间仓促，我也没有查找资料，就自己定义了某种衡量方法：</p><p>想衡量两个关键词的关系，以邓小平和毛泽东为例，他们分别出现了四千多次和两千多次，分布在全书中的各个位置，我想看他们的关系，就是看他们是否会出现的比较近，或者很多时候没有什么关系。于是我考虑去计算两个关键词的“<strong>最近邻距离</strong>”。接下来就是如何定义这个最近邻距离。因为两个关键词的数量不一致，以个数少的作为基准，已经可以知道这个词语在我生成的词汇表的具体位置，因此我分别找到毛泽东出现的两千多个位置，然后搜索每个位置最近的邓小平这个词汇出现的位置，然后获得他们的距离。这样就可以衡量出两个关键词在每个位置的最近距离了。</p><p>虽然听起来这个过程十分的繁琐，需要大量的搜索，但是通过把循环和搜索问题变成矩阵的运算（反正位置都是数字），就可以非常快地计算出任意两个关键词的距离分布了，我给定义成了<strong>calculate_distance</strong>函数。</p><figure class="highlight nix"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">def calculate_distance(ind1,ind2):</span><br><span class="line">    <span class="attr">pos1</span> = np.where(<span class="attr">wordlist==namelist[ind1])[0]</span></span><br><span class="line">    <span class="attr">pos2</span> = np.where(<span class="attr">wordlist==namelist[ind2])[0]</span></span><br><span class="line">    num1 ,<span class="attr">num2</span> = pos1.shape[<span class="number">0</span>],pos2.shape[<span class="number">0</span>]</span><br><span class="line">    <span class="keyword">if</span> num1&gt;num2:</span><br><span class="line">        <span class="attr">small</span> = num2</span><br><span class="line">        <span class="attr">large</span> = num1</span><br><span class="line">        <span class="attr">lararr</span> = pos1</span><br><span class="line">        <span class="attr">smarr</span> = pos2</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="attr">small</span> = num1</span><br><span class="line">        <span class="attr">large</span> = num2</span><br><span class="line">        <span class="attr">lararr</span> = pos2</span><br><span class="line">        <span class="attr">smarr</span> = pos1</span><br><span class="line">    <span class="attr">disarr</span> = np.ndarray([small,large])  <span class="comment">#each line calculate the small set's ith word's and large set's every words distance</span></span><br><span class="line">    <span class="attr">arr1=</span> np.repeat(smarr,large).reshape(-<span class="number">1</span>,large)</span><br><span class="line">    <span class="attr">arr2=</span> np.repeat(lararr,small).reshape(-<span class="number">1</span>,small).T</span><br><span class="line">    <span class="attr">mindis</span> = np.min(np.abs(arr2-arr1),<span class="attr">axis=1)</span></span><br><span class="line">    return mindis</span><br><span class="line"></span><br><span class="line">def draw_dist_count(ind1,ind2):</span><br><span class="line">    fig,<span class="attr">ax=plt.subplots(1,figsize=(20,10))</span></span><br><span class="line">    ax.bar(range(calculate_distance(<span class="number">0</span>,<span class="number">1</span>).shape[<span class="number">0</span>]),calculate_distance(<span class="number">0</span>,<span class="number">1</span>),<span class="attr">color='g')</span></span><br><span class="line">    ax.set_title('Minimum Distance of '+namelist[ind1]+<span class="string">" and "</span>+namelist[ind2])</span><br><span class="line">draw_dist_count(<span class="number">0</span>,<span class="number">1</span>)</span><br></pre></td></tr></table></figure><h4 id="绘图"><a href="#绘图" class="headerlink" title="绘图"></a>绘图</h4><p>这里就拿邓小平和毛泽东两个关键词举例，我按照顺序画了出来，毛泽东出现的两千多次里，每个毛泽东与最近的一个邓小平的位置距离。值越小说明这两个关键词越靠近，要是值为1的话就说明他们挨着（不过对于名词来说一般中间至少隔着一个介词）。<strong>这样就可以看到任意两个关键词的关系随书的文字的紧张的变化情况。</strong></p><p>可以看到700到1400左右，两个词的距离明显较近，说明在这部分文字中，两人发生了更为密切的联系，而500左右的距离有的非常远，说明这部分是各讲各的故事，两个人还没有交集。</p><p><img src="http://i4.bvimg.com/640680/a421383dc7da2619.png" alt="Markdown"></p><p>同样的调用计算距离和绘图的函数，可以查看任意两个关键词的距离并按顺序绘制其值。</p><p>下面一次性展示了同一个关键词和其他好几个关键词的距离图。</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">fig,ax=plt.subplots(<span class="number">4</span>,<span class="number">2</span>,figsize=(<span class="number">20</span>,<span class="number">20</span>))</span><br><span class="line"><span class="keyword">for</span> <span class="selector-tag">i</span> <span class="keyword">in</span> range(<span class="number">4</span>):</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">2</span>):</span><br><span class="line">        ax[<span class="selector-tag">i</span>,j].bar(range(calculate_distance(<span class="number">0</span>,<span class="number">1</span>+<span class="number">2</span>*i+j)<span class="selector-class">.shape</span>[<span class="number">0</span>]),calculate_distance(<span class="number">0</span>,<span class="number">1</span>+<span class="number">2</span>*i+j))</span><br><span class="line">        ax[<span class="selector-tag">i</span>,j].set_title(<span class="string">'Minimum Distance of '</span>+namelist[<span class="number">0</span>]+<span class="string">" and "</span>+namelist[<span class="number">1</span>+<span class="number">2</span>*i+j])</span><br></pre></td></tr></table></figure><p><img src="http://i4.bvimg.com/640680/4414d03a5b228e77.png" alt="Markdown"></p><h4 id="hist-plot"><a href="#hist-plot" class="headerlink" title="hist plot"></a>hist plot</h4><p>接下来还画了一下距离的<strong>分布图</strong>，就是把上面的图中的距离统计一下他们的分布。</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">fig,ax=plt.subplots(<span class="number">4</span>,<span class="number">2</span>,figsize=(<span class="number">20</span>,<span class="number">20</span>))</span><br><span class="line"><span class="keyword">for</span> <span class="selector-tag">i</span> <span class="keyword">in</span> range(<span class="number">4</span>):</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">2</span>):</span><br><span class="line">        ax[<span class="selector-tag">i</span>,j].hist(calculate_distance(<span class="number">0</span>,<span class="number">1</span>+<span class="number">2</span>*i+j),bins =<span class="number">50</span>,<span class="attribute">color</span>=<span class="string">'b'</span>,alpha=<span class="number">0.4</span>)</span><br><span class="line">        ax[<span class="selector-tag">i</span>,j].set_title(<span class="string">'Minimum Distance of '</span>+namelist[<span class="number">0</span>]+<span class="string">" and "</span>+namelist[<span class="number">1</span>+<span class="number">2</span>*i+j])</span><br></pre></td></tr></table></figure><p><img src="http://i4.bvimg.com/640680/3c6048942654bc2c.png" alt="Markdown"></p><p>这种图感觉就丢失很多信息了，看不出来随着书籍的发展，两个名词的关系的变化。当然如果做得更细致，可以用某些指标刻画一下这种距离图，更好地衡量两个指标的关系，用可视化的方法当然是更直观的。</p><h4 id="boxplot"><a href="#boxplot" class="headerlink" title="boxplot"></a>boxplot</h4><p>最后是<strong>Boxplot</strong>，这是另一种直观显示距离分布的图。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">dist_data = &#123;&#125;</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> np.arange(<span class="number">1</span>,<span class="number">20</span>):</span><br><span class="line">    dist_data[i] = calculate_distance(<span class="number">0</span>,i)</span><br><span class="line">dataframe_dxp = pd.concat((pd.DataFrame(&#123;namelist[i]:dist_data[i]&#125;) <span class="keyword">for</span> i <span class="keyword">in</span> np.arange(<span class="number">1</span>,<span class="number">20</span>)),axis=<span class="number">1</span>)</span><br><span class="line"><span class="keyword">import</span> seaborn <span class="keyword">as</span> sns</span><br><span class="line">fig, ax = plt.subplots(figsize=(<span class="number">100</span>,<span class="number">20</span>))</span><br><span class="line">sns.boxplot(data =dataframe_dxp,ax=ax,boxprops=dict(alpha=<span class="number">.5</span>),color=<span class="string">'g'</span>)</span><br><span class="line">ax.set_title(<span class="string">u'Dengxiaoping and others'</span>,fontsize=<span class="number">80</span>)</span><br><span class="line">ax.set_xticks(range(<span class="number">19</span>))</span><br><span class="line">ax.set_xticklabels(namelist[<span class="number">1</span>:<span class="number">20</span>],fontsize=<span class="number">80</span>)</span><br><span class="line">fig.savefig(<span class="string">'boxplot.png'</span>)</span><br></pre></td></tr></table></figure></p><p><img src="http://i4.bvimg.com/640680/5ef0af735a848eca.png" alt="Markdown"></p><p>可以看到邓小平和好几个关键词的距离的分布，每一个box就是一个分布的统计，当然也和分布图一样，这样一画就<strong>拉平了</strong>不同关键词之间的差异了。</p><p>其实对文本挖掘还有<strong>词性标注、情感分析</strong>等更多方法，包括归纳段落或篇章的主题等等，目前都有很多统计模型和机器学习方法可以做。不过在尝试的过程中，我还是感觉到这只是很基本的辅助方法，更重要的还在于人文历史政治学科的专家们对书籍做仔细的解读，<strong>挖掘历史细节中的关键信息是人最擅长的</strong>，比机器强大的多的地方，不过在卷帙浩繁的历史典籍中，面对成千上万的书籍时，快速挖掘书籍的要点，分析出来一些有趣的东西，也许机器能够帮上大忙。</p><h1 id="附：读后感"><a href="#附：读后感" class="headerlink" title="附：读后感"></a>附：读后感</h1><p>这部分明显能感觉到自己笔触的迟滞和笨重，真的是很久不写这种风格的论文，表达得略显凌乱，也有可能是最近总是熬夜，写的时间也正是脑子乱乱的时候，又没有一个规范约束自己，因此写的相当不守规矩。</p><p>这次读后感分成两个部分，一个是常规的读后感想，另一部分是出于兴趣所做的一些对邓小平时代一书的基本的文本挖掘。</p><p>第一部分是我在读书时的一些感悟，尤其是比较了一些香港版和大陆版的不同之后，也产生了一些感悟。</p><p>经过查询，我发现大陆版正文较港版删节约5.3万字，其中包括“邓小平时代的关键人物”一文约2.6万字。这部分其实还相当有趣，我还专门仔细找了其中提到的几个人物的一些更多的史料，发现能读到很多令人震惊的，被有意掩盖的历史。大陆版的几个大篇幅忽略的内容包括天安门事件、邓小平南巡前后对改革停滞的不满、邓小平子女的腐败传闻等敏感话题并未避而不谈。其实这几部分也不是秘密了，我想有一些想法的民众也都能从各种地方搜集来蛛丝马迹，但是从删节中还是能感觉到一条清晰的审查红线。如果删除的内容更真实、接近历史的真相的话，确实相当让人开阔眼界，比如叶剑英病重，邓小平并未去看望，毛泽东对周恩来的打压讨厌却又离不开，在其死后冷漠的态度。这就很颠覆大家的“被培养起来的观念”，还有比如印象中对陈云和邓小平在经济问题上很好地合作的观念，也被作者纠正：陈云比起邓小平的大刀阔斧，要保守稳健很多，因此产生很多分歧，比如不去广东视察。另一个肯定被删（没有核实）的就是南巡时候的珠海会议，不留情面地批评了有点向往毛泽东的观念的江泽民，甚至威胁要取代他。这种分歧的事情大陆版应该也不会保留的。至于89年的风波，就没有什么可讨论的啦。</p><p>我总感觉，一字之差可能一篇文章意义就全变了，中国人最懂笔义春秋之法，也就格外注意这点。邓小平时代确实披露了很多史料，让中国人有机会了解自己的事情（听起来有点滑稽），但是在搜索的过程中，还是难免注意到，香港版的邓小平时代在好几个网站被列为“大陆禁书”。可想而之其中的一些关键问题，可能被隐去了。为什么会被隐去呢，我想我自己有些体会，当我初中读到上上任领导人的传记中的部分内容，感到一阵阵的震惊、震撼、深思、迷茫和一种成长成熟的不愉悦感的时候，以及日后有意无意读到搜到的各种真假故事的时候，都会隐隐体会到为什么有些历史和真实被隐去了。</p><p>读历史和任务传记，尤其是夹杂在历史洪流中的人物群像时，最让人不适的阅读体验之一就是作者对任务的评价不得不因为篇幅所限而显得相当“专断”，比如“雖然趙紫陽做人和藹可親，但一些同事認為他有點兒不合群，喜歡為自己著想。文革開始時趙紫陽讓他的部下抵抗紅衛兵，可是令部下氣憤的是，趙本人很快就把自己辦公室的鑰匙交給了紅衛兵。”这样的话语，让人很难真正对赵紫阳这样一个略显神秘的人物有更多的了解和判断，读起来还是一团历史的迷雾，当然对历史做价值判断本来就是危险且未必能断得清楚的，而如果能尽可能多得了解到公正的史料，也许就能想的更清楚一点，更全面一点，更多地祛除个人的感情，比如作者就并没有站在自由派的立场上一边倒地吹捧赵紫阳，而是指出了他自私自我的一面，这样的词语可能还是可以甚至有点受到大陆的欢迎的，而相反的，喜欢报复、邪恶狡猾之类很难见到的形容毛泽东的词语当然被删除干净。我相信人是无比复杂的，也绝不是多么高尚的，无私的，一个忘我的奉献一切的无私的人就真的是像神一样的所有人应该努力追逐的样子吗，我觉得不是，要说刨去了物质欲望，无私的奉献、为祖国的奉献、也无非是精神上的享受而已，不同类型的人难以体会另外的人的享受和愉悦的点，把事情渲染地极端又美好真的是件好事吗，需要的时候就圣洁如神明，一旦出事了一个个都不干净，这样的过程一再出现的话，恐怕就得不断填补一个巨大的漏洞，或者封住所有的缺口，封住所有信息的渠道，这样好吗。</p><p>从这里想到的东西绝不是在批评当权者和领导层，而是感到作为自私的利己的人类而言也许会永远存在的现象，我们当然不会承认自己的自私，也都指责别人的自私，在这个过程中有心照不宣也有大声互斥，我想共产党有的时候让世界上的很多人抵触和担忧就在于，一定要极端地宣称某些事情，有的时候大家都心照不宣的事情，可以揭露的事情，一定要坚持极端地说出来，这样未必是好的。然而反过来想，就算允许人们议论，揭短，又有何意义呢，对国家发展、经济增长、人民福利有什么意义呢？一下子也许还有很多负面的例子，也许很多时候我们就是这样安慰自己来进行善意的谎言的过程吧。有的时候一次性揭开历史的各种真相也未必是好事，对大多数人来说会是一个失去信仰的，难以接受的过程，就看美国也不过是借着民主自由人权的外衣干了很多坏事，包括这两天的叙利亚的空袭，为了自己的自私和利益，一套冠冕堂皇的理由好像自己也愿意相信，当然中国不信，中国说自己和平崛起不愿战争的说法，外国也不信，所以我们是否真的要探讨一下，话题开放的尺度究竟在哪里，“见过世面”、书读的多一点的人总是嚷嚷着开放，讲出真相，但是真的好吗，我记得也许是毛泽东曾经说过，知识分子什么都懂，就是不懂两点，吃不饱会饿，打仗会死人，我想书读得多了，很多道理也未必想的清楚，也只有毛泽东邓小平这样的人物，也许才能做一个比较好的决断？也许一个自诩读书万卷的知识分子人类良心真正有机会去管理人民的时候，也会发现这套管理是尽量好的办法？虽然也会有很多问题：我们避免不了伟人的错误，比如邓小平的全面物价改革可能造成的问题，我们避免不了无外部监督的权力带来的严重腐败，但这都源于每个人本身的缺点，虽然中国历史数千年来都因为通行一套文字而拥有巨量的经验，但是看起来还是很难解决好这样的问题。</p><p>本书让我感触颇深的另一点在于英雄人物的复杂性，这两段话让我感触极深：<br>~“基辛格11月訪華後，毛澤東為了與美國打交道，轉而依靠鄧小平這個在對抗蘇聯時十分堅定的人。1973年12月，鄧小平遵照指示參加了政治局批周的會議。無論在法國、在上海做地下工作期間還是1950年代初在北京一起工作時，周恩來就像鄧小平的兄長。但是毛澤東有理由希望鄧小平會和自己而不是周恩來站在一起。鄧小平在1940年代的整風運動中就站在毛澤東的一邊，周恩來卻沒有。自從1931年鄧小平被批為「毛派頭子」後，他就一直緊跟毛澤東，並在1950年代得到了毛的重用。1956年以後鄧小平成了黨的總書記，他和周恩來的關係在黨內事務上有時變得很尷尬：周恩來在黨內排名上高於鄧小平，可是他要向負責黨內日常事務的鄧小平彙報工作和接受指示。[2-82]周恩來在文革期間也沒有保護鄧小平。[2-83]~<br>~鄧小平心裏很清楚，「兩位小姐」會把他在批周會議上的發言彙報給毛主席。會議臨近結束時，鄧小平對周恩來說：「你現在的位置離主席只有一步之遙，別人都是可望而不可即，而你卻是『可望而可即』，希望你自己能夠十分警惕這一點。」[2-84]這些話表面上並不惡毒，卻暗藏殺機。鄧小平實際上是在暗示，周恩來想架空毛澤東，篡奪毛的地位。「兩位小姐」把鄧小平的發言和態度彙報給毛澤東後，毛非常興奮，立刻把鄧小平叫去談話。”~</p><p>我印象很深，小时候喜欢问父亲，这个人是好人还是坏人，父亲总会告诫我，不要用好坏去区分一个人。但是不用好坏去区分总会很头疼，很费脑子，让人很难受。毛泽东这个人的复杂性，很多历史事实和资料都能让人们略知一二，邓小平时代一书让我体会的更加深切，尤其是邓小平文革后期找到机会重新拾起权力阶段的故事，顺带让我了解到周恩来与毛泽东的分歧，而按顺序读起来，总是让人感慨颇深，一会儿对某人充满同情，一会儿又更加深切地感受到人物关系的复杂，让人对这些历史巨人产生了掺杂的混合的情绪，不知道同情谁好，不知道支持谁好，更别说谁是好人谁是坏人。可见评价历史真是件十分难的事情，如果客观地叙述事实，就只是记录罢了，而历史终归要掺入主观的观念，从这点上看，我反而觉得不要删节，让有心的读者多读读更好，为什么要把人维护得如此正面而甚至虚假呢，让人更加深切地体会到人性、人生和历史的复杂，更加谨慎地获取和得到自己的观点，岂不是更好？</p><p>大陆版删除的关键人物一章，读起来相当有意思，虽然在我看来这一章里篇幅所限也并没有很细致的描述，可能是不希望人们太多的关注人物背后的交情以及过多思考人性的复杂，故而这章也被删除了，我觉得其中一些写的不错，包括读到了赵紫阳的个人性格和可能带来的局限性，比如过分自爱，性格较为保守，对于管理经济倒是非常擅长。从很多人物和邓小平的关系看来，邓小平确实在管理人，尤其是管理领袖级人物上非常有一套，虽然不及毛泽东的本事，把开国功臣们掌握得牢牢的，但是也能不需要居于最前方就可以实际掌控大局，我觉得邓小平在很多地方上是借鉴了毛泽东的，而且去除了一些掌控欲，也有可能是由于一代英雄们都已经谢幕，相对掌控起来也更加容易，让邓小平有机会居于稍微靠后的位置，就牢牢掌控着一切局势，而且我感觉他和毛泽东后期一样，在挑人上都有意选择一些并未在权力中心浸染很久的人，比如王洪文、华国锋、赵紫阳和江泽民等等，赵紫阳是他出访尼泊尔路过四川一番交谈最终调到北京，而江泽民，我记得在各处看到，大概是邓小平喜欢春节在上海度过，加上八九年的风波，江泽民处理得很好，就空降到政治局成了领导者。这种选人的风格给邓小平带来了很多主动性，也许也有未来避免在历史上少背点锅的可能也说不好。做实验的过程中，都特别强调试探，邓小平可以多一些试探的余地，更多地观察再做决定，加上自己实际的军队和政治上的掌控权和表面领导人的生疏，可以让自己牢牢地掌管真正的核心决策。我记得江泽民曾经对毛泽东的一套更加喜欢，对市场化的进程感到过快，邓小平于是南下巡防，声称反对改革开放就是反对社会主义，由此压制了另一套想法的滋生。</p><p>但是这种掌管权力的方法也未必是完全好的，想想邓小平其实在一个相当“美好”的历史时期，活到最后的就是胜利者，邓小平73年说自己还能干二十年，结果真的干到十四大退到幕后，这种后发优势熬倒了很多元老，导致在大清洗之后的权力真空期获得了很多权力，恐怕接下来的两任都在某些时间段并不舒服，有很多桎梏，到这一任才有了更加集中的权力。这里面有的时候也可以看出人性的有趣之处。干部在反对个人集权的时候又希望自己能够有更多的权力，当年反对个人集权，是反对毛泽东权力过大，随意分配权力，两位小姐、造反派头头竟然能高过周恩来，反对的是权力危害到了我，而当自己可以有权力的时候，有人会愿意不要吗，恐怕也是很少见的。所以我觉得人们在意的可能更多的是权力能不能更多地为自己所用，以及其他的权力不要危害到自己，最好在一种比较好的平衡中，但是平衡也未必会永远存在，也未必是好事。看起来邓小平用权力的集中做了很多伟大的改革，毛泽东也做出了一系列彪炳千秋的伟业和载入史册的糊涂事，如今权力也更加集中起来，如果思路明晰的话，我觉得权力集中是能够做大事的，否则光是吵吵嚷嚷争论不休就什么都做不了了。只是权力集中虽然有可能带来很多的回报，也有更大的风险，争吵不休而止步不前面临的是缓慢的毁灭，一意孤行地推进则有走向荣光和加速衰败两种更加极端的选择。</p><p>这本书讲了很多改革和开放的故事，看起来是讲述了一个政党的领导人如何自我改革，把自己变得更加兼容并包，在术的层面不断学习进步的过程，但是大胆地一说，总是能感觉到在政治和管理问题上，管理者有极其强烈的控制欲，这当然很重要，必须承认，如果不是这样，也许我们已经几次陷入严重到可以亡国的风险，这种经验教训可能也让共产党更加谨慎小心，让改革开放这个词语显得更加多面性：从西方的角度，我们的所谓改革开放，也就是做到人家常规的程度，甚至都远远不够，而共产党在这个过程中要不断地审视各种局势，更加小心翼翼。我的感觉是，共产党相比于其他政党，更喜欢和强调管教，但是又不是喜欢按照法律管理，而是希望有更大的自由裁定权，记得一位高层曾经说过，法律不要立的太细致，这样才有解释的空间。管制与压抑的原理是什么？这恐怕是很多人心中的无法言说的疑问，不管会乱，有很多例子，所以我们就愿意并满足于上交更多的权利？比如自由获取信息、翻阅墙壁、表达言论的权利。某些做法使人很容易往不好的一面联想，虽然这也许并非管理者与人民的本意，但是实在是与神圣纯洁的宣传所矛盾。</p><p>最近对快手、抖音、内涵段子等的查封，以及其“段友”、“抖友”发展出来的有一定结构的组织，让管理者更加紧绷，很多人说这些地方都是垃圾，价值观歪曲，封的好，但是也有人心中充满了疑问，以及经典的“他们向…发难，而我没有发声，最终他们向我而来，没有人替我说话了”的担忧。管理者的这种一贯的，一刀切的、有点精神洁癖和完美主义者喜欢的掌控感与清净感让人赞许又害怕，让包括我在内的很多人处于矛盾：激动并自豪于中国人的成就、又觉得这来源于中国人传统的吃苦与奋进，又觉得这是对自己大大压抑和压榨的结果，觉得我们并没有跳出某种循环，让人迷茫于很多事情和自己的意义，感慨于人类历史和经验的复杂和不足够：看不清楚究竟什么是正确的路，估计不出来自己和国人和世界所处的情况、条件等等。</p><p>以上已经做了很多带有敏感词的评论，在我看来邓小平无疑是个奇人，尤其是作为开国领袖级人物，在分割明显的76年之后，又在领袖群体凋落的时代支撑并扭转了中国的大势，这是千秋功业，邓小平在73年的时候说出自己还能再干二十年，实在是几代人的幸运。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;期中作业之一是写一篇邓小平时代的读后感，说实话这种书是实在没空读了，虽然粗略地翻了几章，十分吸引人，但是这两年真的越来越讨厌写文科式的论文，瞎胡诌凑字数曾经也是我作为理科生的优势，但是这一两年对这种风格的文章：东拼西凑，无病呻吟，迷茫又自负的写作非常地厌恶。因为就想玩点花样，做点简单的文本数据挖掘凑凑字数，虽然多花了很多时间，但是毕竟很有意思，有意思的事情就不算浪费时间对吧，没有意思的事情，哪怕一分钟也是对生命的浪费呢。&lt;br&gt;
    
    </summary>
    
      <category term="techniques" scheme="http://james20141606.github.io/categories/techniques/"/>
    
      <category term="data science" scheme="http://james20141606.github.io/categories/techniques/data-science/"/>
    
    
      <category term="codes" scheme="http://james20141606.github.io/tags/codes/"/>
    
      <category term="statistics" scheme="http://james20141606.github.io/tags/statistics/"/>
    
      <category term="data mining" scheme="http://james20141606.github.io/tags/data-mining/"/>
    
      <category term="matplotlib" scheme="http://james20141606.github.io/tags/matplotlib/"/>
    
  </entry>
  
  <entry>
    <title>Extract Countries&#39; Commercial Data</title>
    <link href="http://james20141606.github.io/2018/04/12/economics/"/>
    <id>http://james20141606.github.io/2018/04/12/economics/</id>
    <published>2018-04-12T15:49:57.000Z</published>
    <updated>2018-04-12T16:10:43.156Z</updated>
    
    <content type="html"><![CDATA[<p>It is a brief pipeline to extract data from datasets in <a href="http://139.129.209.66:8000/d/daedafb854/" target="_blank" rel="noopener">here</a></p><p>The work is from my cute girl friend, who know nothing about code but brag to her mentor she can do it.</p><p>In this work, I use R, Bash and Python to extract different countries different indicators in different years. The data have some property: big, not unified(.RData or .csv), some have mistakes. It is very sparse so it waste many storage. And the conversion of Rdata to csv leads some mistakes, so it needs very careful examination and check work. At first I want to store all of them in HDF5 for better IO, but people in my girl friend’s working team are’t familiar with codes, so I store them in csv. I also think about later work(for example, basic statistical work, model the interaction and time series, maybe a hierarchical time series machine learning model), but I am too busy to help my little bragger to do all kinds of things.<br><a id="more"></a><br>Here are codes I wrote to extract and organize data: <a href="https://github.com/james20141606/economics" target="_blank" rel="noopener">https://github.com/james20141606/economics</a></p><h4 id="wget-to-extract-data"><a href="#wget-to-extract-data" class="headerlink" title="wget to extract data"></a>wget to extract data</h4><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="selector-tag">getdata</span><span class="selector-class">.sh</span></span><br></pre></td></tr></table></figure><h4 id="check-row-and-col-names-for-further-extraction"><a href="#check-row-and-col-names-for-further-extraction" class="headerlink" title="check row and col names for further extraction"></a>check row and col names for further extraction</h4><h5 id="before-2001"><a href="#before-2001" class="headerlink" title="before 2001"></a>before 2001</h5><p>use awk to read row and columns in csv, then compare them with std names<br><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">#first</span><br><span class="line">extractnames.sh</span><br><span class="line">#second</span><br><span class="line">refer to codes <span class="keyword">in</span> analyze_dim_name<span class="selector-class">.ipynb</span>:检查<span class="number">1995</span>-<span class="number">2000</span>年的行和列名</span><br><span class="line"><span class="selector-id">#run</span> and check</span><br></pre></td></tr></table></figure></p><p><strong>There are some wrong files, need to examine them later</strong></p><h5 id="after-2001"><a href="#after-2001" class="headerlink" title="after 2001"></a>after 2001</h5><p>Use R to extract row and columns and compare<br><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="selector-id">#first</span> run Rscript to get row and <span class="attribute">columns</span></span><br><span class="line">run extractrowandcol.R</span><br><span class="line">#second</span><br><span class="line">refer to codes <span class="keyword">in</span> analyze_dim_name<span class="selector-class">.ipynb</span>:<span class="number">2001</span>以后的，用R脚本转出来csv，同样的方式读取并判断</span><br><span class="line"><span class="selector-id">#run</span> and check</span><br></pre></td></tr></table></figure></p><p><strong>All the files have exactly the same structure</strong></p><h2 id="extract-data-concerning-CHINA"><a href="#extract-data-concerning-CHINA" class="headerlink" title="extract data concerning CHINA"></a>extract data concerning CHINA</h2><h3 id="analysis"><a href="#analysis" class="headerlink" title="analysis"></a>analysis</h3><p><strong>The data dimension is: </strong> 1435<em>1435</em>41<br>If use RData to extract some matrix to analyze its row and column names,  the  automatically saved names have mistakes. So we use the previous plot to inspire us and find the true data structure:</p><p><img src="http://i1.bvimg.com/640680/0ba5f17f200bc207.png" alt="Markdown"></p><p>The first big block is <strong>to</strong> the first country(AUS)</p><p>So the column names in first big block are:   X&gt;AUS</p><p>When row and columns names match there are values, so there are only values in diagonal. </p><p><strong>(That’s the main reason the R data file is big: the minimum number is :41<em>41</em>35<em>35, but RData have 1435</em>1435*41, 41 fold redundancy)</strong></p><h3 id="How-to-find-a-country"><a href="#How-to-find-a-country" class="headerlink" title="How to find a country"></a>How to find a country</h3><h4 id="after-2001-1"><a href="#after-2001-1" class="headerlink" title="after 2001"></a>after 2001</h4><p>The data we use is RData, use Rscript to extract data</p><h5 id="to-CHINA"><a href="#to-CHINA" class="headerlink" title="to CHINA"></a>to CHINA</h5><p>Locate the country，<strong>CHN is 7th</strong></p><p><strong>The seventh block are all countries to CHINA</strong><br><strong>dat[,,7]</strong>  the matrix dimension is 1435*1435<br><figure class="highlight subunit"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">datt&lt;-dat[,,7]</span><br><span class="line">datt[1<span class="string">+35</span>*(i<span class="string">-1</span>):35*i,1<span class="string">+35</span>*(i<span class="string">-1</span>):35*i]</span><br></pre></td></tr></table></figure></p><p>Use for loop, <strong>use a array:35<em>（35</em>41）</strong> to store<br>save to <strong>tochn.csv</strong><br>Rows:c1-c35<br>Columns: every 35 columns are a same country<br><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Run extract2001.R</span><br><span class="line"><span class="selector-id">#output</span> <span class="keyword">in</span> out directory</span><br></pre></td></tr></table></figure></p><h5 id="CHINA-to"><a href="#CHINA-to" class="headerlink" title="CHINA to"></a>CHINA to</h5><p><strong>each 1435*1435 block’s seventh mini block is China to another country</strong><br><strong>dat[211:245,211:245,i]</strong><br><figure class="highlight dockerfile"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">Run</span><span class="bash"> extract2001.R</span></span><br></pre></td></tr></table></figure></p><p>Use for loop, <strong>use a array:35<em>（35</em>41）</strong>to store<br><strong>save to chnto.csv</strong></p><p>Rows:c1-c35</p><p>Columns: every 35 columns are a same country</p><h4 id="before-2001-1"><a href="#before-2001-1" class="headerlink" title="before 2001"></a>before 2001</h4><p>There are some exceptions we do not deal with at first</p><p>The data we use only has csv, so use python to extract</p><h5 id="to-CHINA-1"><a href="#to-CHINA-1" class="headerlink" title="to CHINA"></a>to CHINA</h5><p>The principle is similar to after 2001</p><p>But the data is just the transpose: (1435<em>41)</em>1435</p><p><strong>The seventh block are all countries to CHINA</strong></p><p><strong>Run extract1995.py</strong><br><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">datt = dat[<span class="number">1435</span>*<span class="number">6</span>:<span class="number">1435</span>*<span class="number">7</span>,:]   <span class="number">#143</span>5*<span class="number">1435</span>datt[<span class="number">35</span>*<span class="selector-tag">i</span>:<span class="number">35</span>*(i+<span class="number">1</span>),<span class="number">35</span>*<span class="selector-tag">i</span>:<span class="number">35</span>*(i+<span class="number">1</span>)]  #loop</span><br></pre></td></tr></table></figure></p><p>Use for loop, <strong>use a array:35<em>（35</em>41）</strong>to store<br><strong>save to chnto.csv</strong></p><h5 id="CHINA-to-1"><a href="#CHINA-to-1" class="headerlink" title="CHINA to"></a>CHINA to</h5><p><strong>each 1435*1435 block’ seventh mini block is China to another country</strong><br><figure class="highlight makefile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">dattt =dat[1435*i:1435*(i+1),:]  <span class="comment">#the ith country 1435*1435</span></span><br><span class="line"><span class="section">dattt[35*6:35*(6+1),35*6:35*(6+1)]</span></span><br></pre></td></tr></table></figure></p><p>Save to <strong>chnto.csv</strong></p><h2 id="To-do"><a href="#To-do" class="headerlink" title="To do"></a>To do</h2><h3 id="Exception-dealing"><a href="#Exception-dealing" class="headerlink" title="Exception dealing"></a>Exception dealing</h3><ul><li>[ ] before 2001 there are some files not in standard form, needs more examine to extract. Maybe case by case<h3 id="analyze-data"><a href="#analyze-data" class="headerlink" title="analyze data"></a>analyze data</h3><h4 id="plot-the-change"><a href="#plot-the-change" class="headerlink" title="plot the change,"></a>plot the change,</h4>for example, heat map, line chart, animation …</li></ul><p><img src="http://i1.bvimg.com/640680/0ce616088c435eae.gif" alt="Markdown"></p><h4 id="do-simple-statistics"><a href="#do-simple-statistics" class="headerlink" title="do simple statistics"></a>do simple statistics</h4><h4 id="model-the-change"><a href="#model-the-change" class="headerlink" title="model the change"></a>model the change</h4>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;It is a brief pipeline to extract data from datasets in &lt;a href=&quot;http://139.129.209.66:8000/d/daedafb854/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;here&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;The work is from my cute girl friend, who know nothing about code but brag to her mentor she can do it.&lt;/p&gt;
&lt;p&gt;In this work, I use R, Bash and Python to extract different countries different indicators in different years. The data have some property: big, not unified(.RData or .csv), some have mistakes. It is very sparse so it waste many storage. And the conversion of Rdata to csv leads some mistakes, so it needs very careful examination and check work. At first I want to store all of them in HDF5 for better IO, but people in my girl friend’s working team are’t familiar with codes, so I store them in csv. I also think about later work(for example, basic statistical work, model the interaction and time series, maybe a hierarchical time series machine learning model), but I am too busy to help my little bragger to do all kinds of things.&lt;br&gt;
    
    </summary>
    
      <category term="interestings" scheme="http://james20141606.github.io/categories/interestings/"/>
    
    
      <category term="codes" scheme="http://james20141606.github.io/tags/codes/"/>
    
      <category term="statistics" scheme="http://james20141606.github.io/tags/statistics/"/>
    
      <category term="economics" scheme="http://james20141606.github.io/tags/economics/"/>
    
      <category term="girl friend" scheme="http://james20141606.github.io/tags/girl-friend/"/>
    
  </entry>
  
  <entry>
    <title>Setup and Linux</title>
    <link href="http://james20141606.github.io/2018/04/12/setup/"/>
    <id>http://james20141606.github.io/2018/04/12/setup/</id>
    <published>2018-04-12T15:39:17.000Z</published>
    <updated>2018-04-14T17:20:41.471Z</updated>
    
    <content type="html"><![CDATA[<p>分享一点setup和linux的东西，包括使用git，使用支持markdown的笔记软件Bear，Anaconda的一些使用技巧以及jupyter在服务器上的设置。也可以在<a href="https://legacy.gitbook.com/book/lulab/bioinfo-training-2018/details" target="_blank" rel="noopener">这里</a>找到更多分享<br><a id="more"></a></p><h1 id="Setup"><a href="#Setup" class="headerlink" title="Setup"></a>Setup</h1><h2 id="版本控制与GitHub管理"><a href="#版本控制与GitHub管理" class="headerlink" title="版本控制与GitHub管理"></a>版本控制与GitHub管理</h2><h3 id="Git简介"><a href="#Git简介" class="headerlink" title="Git简介"></a>Git简介</h3><h4 id="Git是目前世界上最先进的分布式版本控制系统。"><a href="#Git是目前世界上最先进的分布式版本控制系统。" class="headerlink" title="Git是目前世界上最先进的分布式版本控制系统。"></a>Git是目前世界上最先进的分布式版本控制系统。</h4><h5 id="没有版本控制系统会遇到什么困难："><a href="#没有版本控制系统会遇到什么困难：" class="headerlink" title="没有版本控制系统会遇到什么困难："></a>没有版本控制系统会遇到什么困难：</h5><ul><li>版本更新的困难：如果你用Microsoft Word写过长篇大论，那你一定有这样的经历：想删除一个段落，又怕将来想恢复找不回来怎么办？于是只好先把当前文件“另存为”一个新的Word文件，再接着改，改到一定程度，再“另存为”一个新文件，这样一直改下去，最后你的Word文档可能会有几十个不同版本的备份。过了一周，你想找回被删除的文字，但是已经记不清删除前保存在哪个文件里了，只好一个一个文件回去找，非常麻烦。如果是代码的话，来回的更改就更频繁了，如果想找到之前某个版本的代码，很有可能已经被删除了，对于稍微大一点的工程来说可能麻烦就大了。</li><li>合作时的困难：有些部分需要你的合作者帮助写，于是你把文件Copy到U盘里给她（也可能通过Email发送一份给她），然后，你继续修改文件。一段时间后你的合作者把改动后的文件给你，此时，文件的合并就是一件麻烦事了，你要不然得问她一个一个指出她的改动，或者你就要记录自己的改动，和她的文件合并。<br><br></li></ul><p>如果有一个软件，不但能自动帮我记录每次文件的改动，还可以让同事协作编辑，这样就不用自己管理一堆类似的文件了，也不需要把文件传来传去。如果想查看某次改动，只需要在软件里看一眼就可以看到改动的日期和内容，岂不是很方便？</p><h5 id="这就是21世纪的版本控制系统，Git。"><a href="#这就是21世纪的版本控制系统，Git。" class="headerlink" title="这就是21世纪的版本控制系统，Git。"></a>这就是21世纪的版本控制系统，Git。</h5><h4 id="Git诞生"><a href="#Git诞生" class="headerlink" title="Git诞生"></a>Git诞生</h4><p>Git是Linus (Linux之父)花了两周时间用C写的，在2002年以前，世界各地的志愿者把源代码文件通过diff的方式发给Linus，然后由Linus本人通过手工方式合并代码，Linux反对集中式的，需要联网的版本控制系统，也反对商业版的版本控制系统，于是创造了Git，一个月之内，Linux系统的源码已经由Git管理了。<br>Git迅速成为最流行的分布式版本控制系统，尤其是2008年，GitHub网站上线了，它为开源项目免费提供Git存储，无数开源项目开始迁移至GitHub，这就是程序员最爱的Git和Github的诞生史。</p><h3 id="安装与使用Git"><a href="#安装与使用Git" class="headerlink" title="安装与使用Git"></a>安装与使用Git</h3><h4 id="安装git"><a href="#安装git" class="headerlink" title="安装git"></a>安装git</h4><p>只介绍Mac OS系统安装方法</p><ul><li><p>方法一：先安装homebrew，然后通过homebrew安装Git。安装homebrew可查看<a href="http://brew.sh/" target="_blank" rel="noopener">http://brew.sh/</a></p><figure class="highlight mipsasm"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">brew </span><span class="keyword">install </span>git</span><br></pre></td></tr></table></figure></li><li><p>方法二：<br>第二种方法更简单，也是推荐的方法，就是用Xcode，Xcode集成了Git，不过默认没有安装，在终端输入命令安装command line tools，即可安装git。</p><figure class="highlight ada"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">xcode-<span class="keyword">select</span> <span class="comment">--install</span></span><br></pre></td></tr></table></figure></li></ul><h4 id="使用git"><a href="#使用git" class="headerlink" title="使用git"></a>使用git</h4><h5 id="创建或使用文件夹作为需要管理的仓库"><a href="#创建或使用文件夹作为需要管理的仓库" class="headerlink" title="创建或使用文件夹作为需要管理的仓库"></a>创建或使用文件夹作为需要管理的仓库</h5><p>在本地建立项目文件夹，或者使用已存在的项目文件夹，如helloworld<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> helloworld</span><br><span class="line">git init <span class="comment">#通过git init命令把这个目录变成Git可以管理的仓库</span></span><br></pre></td></tr></table></figure></p><h5 id="添加或更改文件"><a href="#添加或更改文件" class="headerlink" title="添加或更改文件"></a>添加或更改文件</h5><figure class="highlight dockerfile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">vi README.md <span class="comment">#创建一个新文件README.md，添加内容并保存</span></span><br><span class="line">git <span class="keyword">add</span><span class="bash"> README.md</span></span><br><span class="line"><span class="bash"><span class="comment">#用命令git add告诉Git，把文件README.md添加到仓库</span></span></span><br><span class="line"><span class="bash"><span class="comment">#如果一次性添加了多个文件，可以使用git add . git会自己判别哪些是新文件。</span></span></span><br></pre></td></tr></table></figure><p>所有的版本控制系统只能跟踪文本文件的改动，比如TXT文件，网页，所有的程序代码等等，Git可以告诉你每次的改动，比如在第5行加了一个单词“Linux”，在第8行删了一个单词“Windows”。而图片、视频这些二进制文件，只知道大小的改动，但更改的内容版本控制系统无法知道。</p><h5 id="添加更改信息"><a href="#添加更改信息" class="headerlink" title="添加更改信息"></a>添加更改信息</h5><p>下面可以告诉git你本次更改的内容，如果一次add了多个文件，则所有的文件都会被标注同样的更改信息。比如：<br><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">git <span class="keyword">commit</span> -m <span class="string">"first commit"</span></span><br><span class="line">git <span class="keyword">commit</span> -m <span class="string">"add README.md"</span></span><br></pre></td></tr></table></figure></p><h5 id="上传至GitHub"><a href="#上传至GitHub" class="headerlink" title="上传至GitHub"></a>上传至GitHub</h5><p>首先在github上新建一个repository，如helloworld，你将会看到跳转页面上提示你需要推送到的HTTPS地址<a href="https://github.com/accountname/repositoryname.git" target="_blank" rel="noopener">https://github.com/accountname/repositoryname.git</a><br>接下来使用<br><figure class="highlight dockerfile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">git remote <span class="keyword">add</span><span class="bash"> origin https://github.com/accountname/repositoryname.git</span></span><br><span class="line"><span class="bash">git push -u origin master</span></span><br></pre></td></tr></table></figure></p><p>即可把自己的本地仓库推送到github上，速度很快。<br>注意如果第一次把远程地址输入错误，可以用以下命令更正地址<br><figure class="highlight dsconfig"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">git </span><span class="string">remote </span><span class="built_in">set-url</span> <span class="string">origin </span><span class="string">https:</span>//<span class="string">github.</span><span class="string">com/</span><span class="string">accountname/</span><span class="string">repositoryname.</span><span class="string">git</span></span><br></pre></td></tr></table></figure></p><h4 id="使用ssh-key-免账户与密码推送方法："><a href="#使用ssh-key-免账户与密码推送方法：" class="headerlink" title="使用ssh key 免账户与密码推送方法："></a>使用ssh key 免账户与密码推送方法：</h4><h5 id="在终端输入"><a href="#在终端输入" class="headerlink" title="在终端输入"></a>在终端输入</h5><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">git<span class="built_in"> config </span>--global user.name <span class="string">"yourgithubname"</span></span><br><span class="line">git<span class="built_in"> config </span>--global user.email <span class="string">"yourgithubaccountmail"</span></span><br></pre></td></tr></table></figure><h5 id="生成ssh-key"><a href="#生成ssh-key" class="headerlink" title="生成ssh key"></a>生成ssh key</h5><figure class="highlight ebnf"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="attribute">ssh-keygen</span></span><br></pre></td></tr></table></figure><p>生成的密钥在~/.ssh/id_rsa.pub位置。</p><h5 id="配置git-的ssh-key"><a href="#配置git-的ssh-key" class="headerlink" title="配置git 的ssh key"></a>配置git 的ssh key</h5><ul><li>登录github 点击头像选择settings</li><li>选择左侧菜单SSH and GPG keys ；点击右上角的NEW SSH key</li><li>新建ssh 链接。</li><li>title 可随意填写</li><li>Key 将上一步生成的 id_rsa.pub文件 的内容全部复制到此处</li></ul><p>参考链接：<br><a href="https://www.liaoxuefeng.com/wiki/0013739516305929606dd18361248578c67b8067c8c017b000" target="_blank" rel="noopener">Git教程</a><br><a href="http://blog.csdn.net/u012373815/article/details/53575362" target="_blank" rel="noopener">SSH连接GitHub、GitHub配置ssh key</a><br><a href="https://peerj.com/preprints/3159/" target="_blank" rel="noopener">version control</a></p><h2 id="支持markdown的轻量笔记软件-Bear"><a href="#支持markdown的轻量笔记软件-Bear" class="headerlink" title="支持markdown的轻量笔记软件 Bear"></a>支持markdown的轻量笔记软件 Bear</h2><p>推荐一款Mac下的非常好用额轻量级笔记软件Bear</p><h5 id="它的优点包括："><a href="#它的优点包括：" class="headerlink" title="它的优点包括："></a>它的优点包括：</h5><ul><li>轻量级，非常顺滑，无任何延迟</li><li>快捷键/markdown支持，符合程序员思维</li><li>加粗，下划线，项目列举，待办方块，代码块，多级标题，均有键盘快捷键以及markdown格式下的快捷键</li><li>网页链接、文件可拖拽至笔记，并显示内容概要。</li><li>内容可无缝衔接至gitbook等支持markdown格式的场合。（比如这些tips都可以直接在Bear编辑好，复制粘贴来就可以。）</li><li>可以快速通过# 加入标签，对笔记进行分类</li></ul><h1 id="Linux"><a href="#Linux" class="headerlink" title="Linux"></a>Linux</h1><h5 id="Edited-by-19’-Under-Xupeng-Chen"><a href="#Edited-by-19’-Under-Xupeng-Chen" class="headerlink" title="Edited by 19’ Under Xupeng Chen"></a>Edited by 19’ Under Xupeng Chen</h5><h2 id="Conda-amp-Bioconda"><a href="#Conda-amp-Bioconda" class="headerlink" title="Conda &amp; Bioconda"></a>Conda &amp; Bioconda</h2><p>Conda是一个包管理软件，可以帮助方便地下载各种软件而不需要编译。尤其是Bioconda可以用来管理linux系统上的生信相关的软件，是解决安装权限不够的问题的好工具。</p><h3 id="Conda"><a href="#Conda" class="headerlink" title="Conda"></a>Conda</h3><p>conda是一个包，依赖和环境管理工具，适用于多种语言，如: Python, R, Scala, Java, Javascript, C/ C++, FORTRAN</p><h4 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h4><p>Anaconda安装可以去官方下载，但是强烈推荐使用tuna镜像，免流量，而且速度极快。<br><a href="https://mirrors.tuna.tsinghua.edu.cn/anaconda/archive/" target="_blank" rel="noopener">下载地址</a>，下载.sh文件后运行，按照提示一步一步往下运行即可。<br>下载Anaconda后，很多python的常用库都会被自动安装好，另外建议运行以下命令</p><figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">conda config --<span class="built_in">add</span> channels http<span class="variable">s:</span>//mirrors.tuna.tsinghua.edu.<span class="keyword">cn</span>/anaconda/pkgs/free/</span><br><span class="line">conda config --<span class="built_in">add</span> channels http<span class="variable">s:</span>//mirrors.tuna.tsinghua.edu.<span class="keyword">cn</span>/anaconda/pkgs/main/</span><br><span class="line">conda config --<span class="keyword">set</span> show_channel_urls yes</span><br></pre></td></tr></table></figure><p>这样以后使用conda install packages命令下载需要的包的时候，会自动从tuna镜像下载，速度会非常快。</p><h3 id="Bioconda"><a href="#Bioconda" class="headerlink" title="Bioconda"></a>Bioconda</h3><p>Bioconda是conda上一个分发生物信息软件的频道，使用它的最大好处是，你不用自己编译软件了。<br>Conda tuna 安装 conda设置 从tuna下载免流量，快<br>目前Bioconda有超过130个添加、更新和维护生物信息软件的贡献者，他们为这个频道发布了1500多个软件包。总结起来，bioconda有以下几个特点：</p><ul><li>软件是编译好的，无需自己编译</li><li>跨平台，支持Linux和Mac OS（本身conda还支持Windows）</li><li>支持多种语言，Python/Perl/R/Java/Go等</li><li>兼容多种语言的包管理器，如pip，CRAN，CPAN，Bioconductor，apt-get以及 homebrew<br>针对Python来说，使用conda相比pip的很大优势，就是不用自己编译。安装软件最头疼的问题，就是解决编译报错，很多时候忙活一天就为了把一个软件装好。</li></ul><h4 id="使用方法"><a href="#使用方法" class="headerlink" title="使用方法"></a>使用方法</h4><p>先添加Bioconda频道</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">conda<span class="built_in"> config </span>--<span class="builtin-name">add</span> channels defaults</span><br><span class="line">conda<span class="built_in"> config </span>--<span class="builtin-name">add</span> channels conda-forge</span><br><span class="line">conda<span class="built_in"> config </span>--<span class="builtin-name">add</span> channels bioconda</span><br></pre></td></tr></table></figure><p>然后即可用conda安装各种需要的软件，可以先去bioconda channel看看自己需要的软件在不在列表内。</p><figure class="highlight mipsasm"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">conda <span class="keyword">install </span><span class="keyword">bowtie</span></span><br><span class="line"><span class="keyword">conda </span>create -n myenv <span class="keyword">bwa </span><span class="keyword">bowtie </span>hisat star <span class="comment">#a new environment can be created</span></span><br><span class="line">source activate myenv <span class="comment">#activate the environment</span></span><br></pre></td></tr></table></figure><p>参考资料：<br><a href="https://bioconda.github.io/" target="_blank" rel="noopener">Using Bioconda — Bioconda documentation</a><br><a href="https://bioconda.github.io/recipes.html" target="_blank" rel="noopener">packages list</a></p><h2 id="在服务器上运行jupyter-notebook并在本地浏览器使用"><a href="#在服务器上运行jupyter-notebook并在本地浏览器使用" class="headerlink" title="在服务器上运行jupyter notebook并在本地浏览器使用"></a>在服务器上运行jupyter notebook并在本地浏览器使用</h2><p>Jupyter Notebook是基于网页的用于交互计算的应用程序。其可被应用于全过程计算：开发、文档编写（markdown）、运行代码和展示结果。</p><ul><li><p>jupyter适合课题的早期尝试、绘图等非常便利，代码重复运行和复制粘贴方便，方便反复调试，尤其适合尚未工程化，需要大量尝试的阶段。</p></li><li><p>jupyter非常适合教学，交互效果非常好，github上有大量的教学项目是用jupyter notebook展示的，方便查看结果，查看相关说明、公式，方便学习者进行反复实验。</p></li></ul><h4 id="本地设置服务器信息"><a href="#本地设置服务器信息" class="headerlink" title="本地设置服务器信息"></a>本地设置服务器信息</h4><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">vi .ssh/config</span><br><span class="line">Host ibme</span><br><span class="line">HostName 166.111.152.116 #ibme的ip</span><br><span class="line">ControlPersist <span class="literal">yes</span></span><br><span class="line">ControlMaster auto</span><br><span class="line">User chenxupeng</span><br><span class="line">DynamicForward 127.0.0.1:32987 #最后的port（如32987）要自己设置，不能与他人冲突</span><br></pre></td></tr></table></figure><h4 id="使用SwitchOmega在本地浏览器设置代理"><a href="#使用SwitchOmega在本地浏览器设置代理" class="headerlink" title="使用SwitchOmega在本地浏览器设置代理"></a>使用SwitchOmega在本地浏览器设置代理</h4><h5 id="添加情景模式，如ibme"><a href="#添加情景模式，如ibme" class="headerlink" title="添加情景模式，如ibme"></a>添加情景模式，如ibme</h5><p>代理协议SOCKS5，代理服务器127.0.0.1，代理端口填写自己设置的port。</p><h5 id="在auto-switch页面添加规则"><a href="#在auto-switch页面添加规则" class="headerlink" title="在auto switch页面添加规则"></a>在auto switch页面添加规则</h5><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">172<span class="selector-class">.235</span><span class="selector-class">.0</span>.*，192<span class="selector-class">.235</span><span class="selector-class">.0</span>.*，<span class="selector-tag">node50</span>*等，情景模式选择<span class="selector-tag">ibme</span></span><br></pre></td></tr></table></figure><p>点击应用选项</p><h5 id="在服务器上设置start-jupyter文件"><a href="#在服务器上设置start-jupyter文件" class="headerlink" title="在服务器上设置start-jupyter文件"></a>在服务器上设置start-jupyter文件</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">vi ~/bin/start-jupyter</span><br><span class="line">填写：</span><br><span class="line"><span class="meta">#! /bin/bash</span></span><br><span class="line">bsub &lt;&lt;EOF</span><br><span class="line"><span class="comment">#BSUB -J jupyter</span></span><br><span class="line"><span class="comment">#BSUB -R span[hosts=1]</span></span><br><span class="line"><span class="comment">#BSUB -q Z-LU</span></span><br><span class="line"><span class="built_in">cd</span></span><br><span class="line">jupyter notebook --no-browser --ip=0.0.0.0 --port=10087 <span class="comment">#port自己设置一个，不要冲突</span></span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><p>(也可以使用#BSUB -q Z-BNODE)</p><h4 id="start-jupyter"><a href="#start-jupyter" class="headerlink" title="start jupyter"></a>start jupyter</h4><p>首先start-jupyter启动，会自动提交一个任务到某个节点</p><h5 id="使用节点名称连接"><a href="#使用节点名称连接" class="headerlink" title="使用节点名称连接"></a>使用节点名称连接</h5><p>接下来可以用bjobs看到jupyter被提交到了哪个节点。接下来打开本地浏览器，输入</p><figure class="highlight elixir"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">node5<span class="number">0</span>*<span class="symbol">:port</span> <span class="comment">#如node504/10087</span></span><br></pre></td></tr></table></figure><p>若使用Z-BNODE，可在浏览器填写</p><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="selector-tag">zbnode01</span><span class="selector-class">.cluster</span><span class="selector-class">.com</span><span class="selector-pseudo">:port</span></span><br></pre></td></tr></table></figure><h5 id="使用ip连接"><a href="#使用ip连接" class="headerlink" title="使用ip连接"></a>使用ip连接</h5><p>用nslookup获得节点的ip，在本地浏览器输入：</p><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="selector-tag">ip</span><span class="selector-pseudo">:port</span> #如192<span class="selector-class">.235</span><span class="selector-class">.5</span><span class="selector-class">.48</span><span class="selector-pseudo">:10087</span></span><br><span class="line">#获得<span class="selector-tag">ip</span>方法</span><br><span class="line"><span class="selector-tag">nslookup</span> <span class="selector-tag">node504</span><span class="selector-class">.cluster</span><span class="selector-class">.com</span></span><br><span class="line"><span class="selector-tag">nslookup</span> <span class="selector-tag">zbnode01</span><span class="selector-class">.cluster</span><span class="selector-class">.com</span></span><br></pre></td></tr></table></figure><p>第一次登陆需要密码，用bpeek查看任务输出，即可看到token，复制至浏览器即可使用jupyter notebook进行编程了。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;分享一点setup和linux的东西，包括使用git，使用支持markdown的笔记软件Bear，Anaconda的一些使用技巧以及jupyter在服务器上的设置。也可以在&lt;a href=&quot;https://legacy.gitbook.com/book/lulab/bioinfo-training-2018/details&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;这里&lt;/a&gt;找到更多分享&lt;br&gt;
    
    </summary>
    
      <category term="techniques" scheme="http://james20141606.github.io/categories/techniques/"/>
    
      <category term="linux" scheme="http://james20141606.github.io/categories/techniques/linux/"/>
    
    
      <category term="techniques" scheme="http://james20141606.github.io/tags/techniques/"/>
    
      <category term="bioinformatics" scheme="http://james20141606.github.io/tags/bioinformatics/"/>
    
  </entry>
  
  <entry>
    <title>eMaize_Tutorial</title>
    <link href="http://james20141606.github.io/2018/04/12/emaize-tutorial/"/>
    <id>http://james20141606.github.io/2018/04/12/emaize-tutorial/</id>
    <published>2018-04-12T15:30:17.000Z</published>
    <updated>2018-04-16T12:30:23.114Z</updated>
    
    <content type="html"><![CDATA[<p>这是为实验室写的，借由eMaize问题帮助大家简单了解机器学习基本方法和基础代码的教程。也可以在<a href="https://lulab.gitbooks.io/bioinfo/content/5%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%95%B4%E5%90%88----%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%92%8C%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/51.html" target="_blank" rel="noopener">这里</a>看到</p><p>由于jupyter notebook的强大的展示功能，本教程还用jupyter notebook组织且运行，可以获得更好的学习效果，代码在<a href="https://github.com/james20141606/somethingmore/blob/master/bioinfo.ipynb" target="_blank" rel="noopener">这里</a>,欢迎取用。<a href="http://localhost:4000/2018/04/12/setup/" target="_blank" rel="noopener">在这里</a>我简单介绍了如何配置jupyter，在<a href="https://james20141606.github.io/2018/04/10/Deep-Learning-Practice/">Deep Learning tutorial</a>中我也强烈推荐了jupyter，并且介绍了很多基于jupyter的资源，强烈建议尝试一下。<br><a id="more"></a></p><h2 id="0-背景简介"><a href="#0-背景简介" class="headerlink" title="0.背景简介"></a>0.背景简介</h2><p>该通过基因型预测表型的实例来自<a href="http://emaize.imaze.org" target="_blank" rel="noopener">eMaize challenge</a>:<br>eMaize问题要求我们以SNP作为特征，通过训练一个模型，对玉米的三个性状进行预测。<br>接下来的教程会展示从原始数据开始，如何对数据进行转换，存取，特征选择以及回归和后续分析的整个过程。本问题最基本的目标是使用6210个样本中的前4754个样本作为训练集，预测其他样本的性状<br></p><h2 id="I-上机指南"><a href="#I-上机指南" class="headerlink" title="I.上机指南"></a>I.上机指南</h2><p>本任务依赖于python语言及jupyter notebook，所需工具已安装到虚拟机。以下指南的所有代码均可在4.Emaize/jupyter_notebook/basic_tutorial.ipynb 中找到。</p><p>使用方法：</p><ul><li><p>打开终端，进入Bioinfo_Lab/4.Emaize/ 文件夹</p></li><li><p>输入jupyter notebook，等待弹出窗口，或者手动复制粘贴终端显示的网址到浏览器。</p></li><li><p>点击jupyter_notebook,再点击basic_tutorial.ipynb,即可看到本部分的教程。按照相关指南一步一步运行即可。本部分接下来的内容与basic_tutorial.ipynb中的内容一致。</p></li></ul><h4 id="jupyter-notebook基本使用指南："><a href="#jupyter-notebook基本使用指南：" class="headerlink" title="jupyter notebook基本使用指南："></a>jupyter notebook基本使用指南：</h4><p>本教程使用jupyter notebook，可以让使用者获得更好的体验，方便对代码进行修改，以及对结果进行查看和分析</p><ul><li>一段相关的代码在同一个代码框中书写 <br></li><li>同时按住shift与enter即可运行选中的代码框的代码<br></li><li>仅仅按enter键具有回车的效果</li><li><h5 id="使用上方的编辑栏："><a href="#使用上方的编辑栏：" class="headerlink" title="使用上方的编辑栏："></a>使用上方的编辑栏：</h5>点击加号在两个代码框中间插入新的代码框，删除代码框点击剪刀，中止程序点击方框</li></ul><figure class="highlight xl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">#导入必需的库</span><br><span class="line"><span class="keyword">import</span> warnings</span><br><span class="line">warnings.filterwarnings(<span class="string">'ignore'</span>)</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">from sklearn.random_projection <span class="keyword">import</span> SparseRandomProjection</span><br><span class="line">from scipy.sparse <span class="keyword">import</span> load_npz, save_npz</span><br><span class="line"><span class="keyword">import</span> scipy.stats</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> sklearn</span><br><span class="line"><span class="keyword">import</span> h5py</span><br><span class="line">from sklearn.metrics <span class="keyword">import</span> r2_score</span><br><span class="line">from scipy.stats.stats <span class="keyword">import</span> pearsonr</span><br><span class="line"><span class="keyword">import</span> pickle</span><br><span class="line">from sklearn.linear_model <span class="keyword">import</span> LinearRegression</span><br><span class="line">#<span class="keyword">import</span> xgboost</span><br><span class="line">#from xgboost.sklearn <span class="keyword">import</span> XGBRegressor</span><br><span class="line">from sklearn.linear_model <span class="keyword">import</span> Ridge</span><br><span class="line">from sklearn.kernel_ridge <span class="keyword">import</span> KernelRidge</span><br><span class="line">from sklearn <span class="keyword">import</span> neighbors</span><br><span class="line">from sklearn.ensemble <span class="keyword">import</span> RandomForestRegressor</span><br><span class="line">from sklearn.gaussian_process <span class="keyword">import</span> GaussianProcessRegressor</span><br><span class="line">from sklearn.gaussian_process.kernels <span class="keyword">import</span> DotProduct</span><br><span class="line">from tqdm <span class="keyword">import</span> tqdm_notebook <span class="keyword">as</span> tqdm</span><br><span class="line">from IPython.display <span class="keyword">import</span> display, Image</span><br><span class="line">%pylab inline</span><br></pre></td></tr></table></figure><h3 id="1-查看原始数据"><a href="#1-查看原始数据" class="headerlink" title="1.查看原始数据"></a>1.查看原始数据</h3><h4 id="1-1-数据种类"><a href="#1-1-数据种类" class="headerlink" title="1.1 数据种类"></a>1.1 数据种类</h4><ul><li>genotype：SNP数据，每个位点可能有三种情况，如AA，AT，TT <br></li><li>trait：共三种，trait1开花期，trait2株高，trait3产量，为连续值 <br></li><li>原始数据中有6210个样本，每个样本SNP位点约为190万个,<br>因为计算资源的原因，这里仅仅选取其中的5000个SNP作为示例,因为数据量的原因，结果肯定不够理想</li></ul><h4 id="1-2-数据格式"><a href="#1-2-数据格式" class="headerlink" title="1.2 数据格式"></a>1.2 数据格式</h4><p>txt存储格式不适合大数据读取的问题，对内存的占用过多。对于结构化的、能够存储为矩阵的数据，可以使用HDF5格式存取，内存占用小，读取速度快</p><h5 id="读取SNP数据"><a href="#读取SNP数据" class="headerlink" title="读取SNP数据"></a>读取SNP数据</h5><p>数据格式为HDF5</p><h5 id="在命令行查看数据shape"><a href="#在命令行查看数据shape" class="headerlink" title="在命令行查看数据shape"></a>在命令行查看数据shape</h5><p>方法为：</p><ul><li>cd至文件路径下，输入：h5ls snp_5000 <br></li><li>若使用了新版h5py，可能出现无法打开的情况，此时输入HDF5_USE_FILE_LOCKING=FALSE h5ls snp_5000</li></ul><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="section">#使用h5py读取5000个SNP：</span></span><br><span class="line">with h5py.File('data/snp_5000') as f:</span><br><span class="line">snps = f[<span class="string">'snp'</span>][<span class="symbol">:</span>]</span><br><span class="line"><span class="section">#查看数据shape,h5py读取出的snps是一个矩阵，可以用.shape查看其shape</span></span><br><span class="line">snps.shape</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">查看数据内容</span></span><br><span class="line">snps</span><br></pre></td></tr></table></figure><h5 id="读取性状数据"><a href="#读取性状数据" class="headerlink" title="读取性状数据"></a>读取性状数据</h5><p>使用numpy/pandas均可读取性状数据并显示，这里用pandas展示，真正计算时一般用numpy</p><figure class="highlight nginx"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="attribute">traits</span> = pd.read_csv(<span class="string">'data/pheno_emaize.txt'</span>,delimiter=<span class="string">'\t'</span>)</span><br><span class="line"><span class="comment">#仅显示前五个,4754之后的样本的性状是未知的</span></span><br><span class="line">traits.head()</span><br></pre></td></tr></table></figure><figure class="highlight fortran"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">#pandas dataframe也可查看<span class="built_in">shape</span></span><br><span class="line"><span class="built_in">print</span> traits.<span class="built_in">shape</span></span><br></pre></td></tr></table></figure><figure class="highlight prolog"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">#查看性状的分布情况</span><br><span class="line">trait1 = np.array(traits[<span class="string">'trait1'</span>])[:<span class="number">4754</span>]</span><br><span class="line">trait2 = np.array(traits[<span class="string">'trait2'</span>])[:<span class="number">4754</span>]</span><br><span class="line">trait3 = np.array(traits[<span class="string">'trait3'</span>])[:<span class="number">4754</span>]</span><br><span class="line">fig, ax = plt.subplots(<span class="number">1</span>,<span class="number">3</span>, figsize=(<span class="number">15</span>,<span class="number">3</span>))</span><br><span class="line">ax[<span class="number">0</span>].hist(trait1,bins = <span class="number">50</span>)</span><br><span class="line">ax[<span class="number">0</span>].set_title(<span class="string">'normalized trait1 value distribution'</span>)</span><br><span class="line">ax[<span class="number">1</span>].hist(trait2,bins = <span class="number">50</span>)</span><br><span class="line">ax[<span class="number">1</span>].set_title(<span class="string">'normalized trait2 value distribution'</span>)</span><br><span class="line">ax[<span class="number">2</span>].hist(trait3,bins = <span class="number">50</span>)</span><br><span class="line">ax[<span class="number">2</span>].set_title(<span class="string">'normalized trait3 value distribution'</span>)</span><br></pre></td></tr></table></figure><p><img src="http://i1.bvimg.com/640680/30af897795c31338.png" alt="Markdown"></p><h5 id="查看训练集与测试集的划分"><a href="#查看训练集与测试集的划分" class="headerlink" title="查看训练集与测试集的划分"></a>查看训练集与测试集的划分</h5><p>下图中彩色部分为训练集性状，白色部分为待预测性状 <br><br>可以发现其划分方式并不随机，这会导致常规的机器学习方法出现一些问题，由于是基础介绍，这里不讨论如何解决这个问题。</p><figure class="highlight prolog"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">def generate_parent_table(phenotype_file):</span><br><span class="line">phenotypes = pd.read_table(phenotype_file)</span><br><span class="line">pedigree = phenotypes[<span class="string">'pedigree'</span>].str.split(<span class="string">'_'</span>, expand=<span class="symbol">True</span>)</span><br><span class="line">pedigree.columns = [<span class="string">'f'</span>, <span class="string">'X'</span>, <span class="string">'m'</span>]</span><br><span class="line">phenotypes = pd.concat([phenotypes, pedigree], axis=<span class="number">1</span>)</span><br><span class="line">phenotypes[<span class="string">'number'</span>] = np.arange(phenotypes.shape[<span class="number">0</span>])</span><br><span class="line">parent_table = phenotypes.pivot_table(values=<span class="string">'number'</span>, index=[<span class="string">'m'</span>], columns=[<span class="string">'f'</span>], dropna=<span class="symbol">False</span>)</span><br><span class="line">male_ids = [<span class="string">'m%d'</span> <span class="comment">% i for i in range(1, parent_table.shape[0] + 1)]</span></span><br><span class="line">female_ids = [<span class="string">'f%d'</span> <span class="comment">% i for i in range(1, parent_table.shape[1] + 1)]</span></span><br><span class="line">parent_table = parent_table.loc[male_ids, female_ids]</span><br><span class="line">return parent_table</span><br><span class="line">phenotype_file = <span class="string">'data/pheno_emaize.txt'</span></span><br><span class="line">parent_table = generate_parent_table(phenotype_file)</span><br><span class="line">phenotypes = pd.read_table(<span class="string">'data/pheno_emaize.txt'</span>)</span><br><span class="line">fig, ax = subplots(<span class="number">3</span>,<span class="number">1</span>, figsize=(<span class="number">20</span>, <span class="number">10</span>))</span><br><span class="line">for i in range(<span class="number">3</span>):</span><br><span class="line">trait = [<span class="string">'trait1'</span>,<span class="string">'trait2'</span>,<span class="string">'trait3'</span>][i]</span><br><span class="line">ax[i].matshow(np.take(np.ravel(phenotypes[trait].values), parent_table), cmap=cm.<span class="symbol">RdBu</span>)</span><br><span class="line">ax[i].set_title(<span class="string">'Phenotypes of training data (%s)'</span><span class="comment">%trait)</span></span><br></pre></td></tr></table></figure><h3 id="2-将SNP数据编码为向量"><a href="#2-将SNP数据编码为向量" class="headerlink" title="2. 将SNP数据编码为向量"></a>2. 将SNP数据编码为向量</h3><p>每个位点的碱基只有三种情况，不会出现更多碱基组合的可能，比如某位点仅有AA，AT，TT三种可能的情况<br><br>我们可以采取三种方式对其编码：</p><ul><li>转化为0、1、2。找到minor allele frequency（MAF），即两种碱基（如A、T）中出现频率低的那个，以A作为MAF为例，则TT为0，AT为1，AA为2，这样可以突出MAF</li><li>转化为3-bit one hot vector,$[1,0,0]^T,[0,1,0]^T,[0,0,1]^T$这样可以保持三种向量在空间距离的一致</li><li>转化为2-bit vector,则AA，AT，TT分别编为$[1,0]^T,[1,1]^T,[0,1]^T$,不需要考虑MAF<br>我们采取第三种方式<figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">def convert_2bit(seq):</span><br><span class="line">genotypes = np.zeros([6210,2])</span><br><span class="line">a = seq[1].split('/')</span><br><span class="line">for i in range(6210):</span><br><span class="line">if seq[<span class="string">4:</span>][<span class="symbol">i</span>] == a[0] + a[0]:</span><br><span class="line">genotypes[i] = np.array([0,1])</span><br><span class="line">if seq[<span class="string">4:</span>][<span class="symbol">i</span>] == a[0] + a[1]:</span><br><span class="line">genotypes[i] = np.array([1,0])</span><br><span class="line">if seq[<span class="string">4:</span>][<span class="symbol">i</span>] == a[1] + a[1]:</span><br><span class="line">genotypes[i] = np.array([1,1])</span><br><span class="line">genotypes = genotypes.astype('int').T</span><br><span class="line">return genotypes</span><br></pre></td></tr></table></figure></li></ul><p><strong>注意，接下来的步骤耗时14min</strong><br>真实计算时此步骤使用C加速计算，这里为了连贯性仅仅展示python的方法 <br><br>可以跳过接下来的代码框步骤，直接使用处理好的结果，结果放在 /data/2bit_geno<br><figure class="highlight makefile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#该代码框可跳过以节约时间，直接运行下一个代码框</span></span><br><span class="line">geno_conv = convert_2bit(snps[1])</span><br><span class="line">for i in tqdm(range(4999)):</span><br><span class="line">geno_conv = np.concatenate((geno_conv,convert_2bit(snps[i+2])),axis =0)</span><br></pre></td></tr></table></figure></p><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="section">#读取处理成2bit格式的SNP</span></span><br><span class="line">with h5py.File('data/2bit_geno') as f:</span><br><span class="line">geno_conv = f[<span class="string">'data'</span>][<span class="symbol">:</span>]</span><br></pre></td></tr></table></figure><figure class="highlight nix"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#查看SNP的大致情况</span></span><br><span class="line">fig, <span class="attr">ax</span> = plt.subplots(<span class="attr">figsize=(5,10))</span></span><br><span class="line">ax.matshow(geno_conv[:<span class="number">300</span>,:<span class="number">200</span>],<span class="attr">cmap</span> = cm.binary_r)</span><br></pre></td></tr></table></figure><h3 id="3-特征提取与降维"><a href="#3-特征提取与降维" class="headerlink" title="3. 特征提取与降维"></a>3. 特征提取与降维</h3><ul><li>原数据每个样本有190万个SNP，转化为2bit coding后有大约380万个feature，大多数的feature可能是冗余的 <br></li><li>过多的feature使得机器学习模型无法承受，一个考虑时间开销及效果的feature数量应该在几千至几万量级 <br></li></ul><h4 id="特征选择："><a href="#特征选择：" class="headerlink" title="特征选择："></a>特征选择：</h4><p>特征选择的方法包括filter，wrapper和embedding三大类 <br><br>我们使用过如下方法： <br></p><ul><li>Mutual information:劣势在于需要将连续的性状值离散化，损失信息<br></li><li>ANOVA:通过p-value筛选feature，速度较慢，我们设计了加速ANOVA计算的算法。<br></li><li>基于模型的方法:基于广义线性模型或其他带有feature权重的机器学习模型，根据权重挑选feature<br></li></ul><h4 id="降维："><a href="#降维：" class="headerlink" title="降维："></a>降维：</h4><ul><li>PCA、SVD：劣势在于降维后的feature数量不能超过样本数量，一次性损失的feature过多<br></li><li>Random projection:基于LSH的降维方式，速度较快<br></li></ul><p>通过对问题的后续分析，我们发现对于预测绝大多数样本，基本的降维方法就已经够用<br><br>但是对于部分很难预测的样本，简单的特征选择方法也无法取得好的效果<br><br>我们根据后续开发的针对性的模型，设计了基于模型的特征选择方法，因为内容限制，不在这里使用。</p><p><strong>接下来分别使用ANOVA和Random projection演示特征选择和降维，对于后续的计算来说，选择其中一种就可以，也可以把不同的方法拼起来使用</strong></p><p><strong>我们会提供ANOVA、Random projection处理上面5000个snps后的数据，以及在完整数据集上用Random projection降维至10000个feature的数据。用于送入下一部分的回归模型。下面的三种方法的处理后的数据可以在feature_selection文件夹下找到，存储格式为HDF5，可用h5py打开</strong></p><h5 id="ANOVA数据："><a href="#ANOVA数据：" class="headerlink" title="ANOVA数据："></a>ANOVA数据：</h5><p>feature_selection/anova 包含三个性状各自的feature，大小为4000*6210</p><h5 id="Random-projection-5000-数据："><a href="#Random-projection-5000-数据：" class="headerlink" title="Random projection(5000)数据："></a>Random projection(5000)数据：</h5><p>feature_selection/randomproj_5000 从5000个SNPs降维得到，三个性状使用同一组feature,大小为1000*6210</p><h5 id="Random-projection-whole-SNPs-数据："><a href="#Random-projection-whole-SNPs-数据：" class="headerlink" title="Random projection(whole SNPs)数据："></a>Random projection(whole SNPs)数据：</h5><p>feature_selection/randomproj_whole 从所有SNPs降维得到，三个性状使用同一组feature，大小为10000*6210</p><h4 id="3-1-ANOVA"><a href="#3-1-ANOVA" class="headerlink" title="3.1 ANOVA"></a>3.1 ANOVA</h4><p>方差分析方法可以利用p值挑选feature <br><br>调用scipy.stats.f_oneway,利用SNPs和性状可以很容易地计算出p-value，但是对于大量数据来说速度较慢 <br><br>这里我们使用一种加速ANOVA计算的方法完成计算，相比于scipy.stats的方法可以提升计算速度数百倍。<br><figure class="highlight makefile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#加速ANOVA算法</span></span><br><span class="line">def fast_anova_2bit(X, y):</span><br><span class="line">y = y - y.mean()</span><br><span class="line">y2 = y*y</span><br><span class="line">N = X.shape[0]</span><br><span class="line">SS_tot = np.sum(y2)</span><br><span class="line"><span class="comment"># 10, 01, 11</span></span><br><span class="line">masks = [np.logical_and(X[:, 0::2], np.logical_not(X[:, 1::2])),</span><br><span class="line"><span class="section">np.logical_and(np.logical_not(X[:, 0::2]), X[:, 1::2]),</span></span><br><span class="line"><span class="section">np.logical_and(X[:, 0::2], X[:, 1::2])]</span></span><br><span class="line">Ni = np.concatenate([np.sum(mask, axis=0) for mask in masks]).reshape((3, -1))</span><br><span class="line">at_least_one = Ni &gt; 0</span><br><span class="line">SS_bn = [np.sum(y.reshape((-1, 1))*mask, axis=0) for mask in masks]</span><br><span class="line">SS_bn = np.concatenate(SS_bn).reshape((3, -1))</span><br><span class="line">SS_bn **= 2</span><br><span class="line">SS_bn = np.where(at_least_one, SS_bn/Ni, 0)</span><br><span class="line">SS_bn = np.sum(SS_bn, axis=0)</span><br><span class="line">SS_wn = SS_tot - SS_bn</span><br><span class="line">M = np.sum(at_least_one, axis=0)</span><br><span class="line">DF_bn = M - 1</span><br><span class="line">DF_wn = N - M</span><br><span class="line">SS_bn /= DF_bn</span><br><span class="line">SS_wn /= DF_wn</span><br><span class="line">F = SS_bn/SS_wn</span><br><span class="line">p_vals = np.ones(F.shape[0])</span><br><span class="line">ind = np.nonzero(M == 2)[0]</span><br><span class="line">if ind.shape[0] &gt; 0:</span><br><span class="line">p_vals[ind] = scipy.stats.f.sf(F[ind], 1, N - 2)</span><br><span class="line">ind = np.nonzero(M == 3)[0]</span><br><span class="line">if ind.shape[0] &gt; 0:</span><br><span class="line">p_vals[ind] = scipy.stats.f.sf(F[ind], 2, N - 3)</span><br><span class="line">return F, p_vals</span><br></pre></td></tr></table></figure></p><p><strong>注意X和y分别是什么</strong><br>我们需要输入进 fast_anova_2bit(X, y)的X是处理过的SNPs中前4754个样本的，y是trait1、trait2、trait3<br>因为需要分别预测三个性状，我们需要针对三个性状分别挑选特征</p><figure class="highlight fortran"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">geno_conv_train = geno_conv[:,:<span class="number">4754</span>]</span><br><span class="line">geno_conv_test = geno_conv[:,<span class="number">4754</span>:]</span><br><span class="line"><span class="built_in">print</span> geno_conv_train.<span class="built_in">shape</span></span><br><span class="line"><span class="built_in">print</span> geno_conv_test.<span class="built_in">shape</span></span><br></pre></td></tr></table></figure><figure class="highlight autohotkey"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">#分别计算三种性状下<span class="number">5000</span>个SNPs做完ANOVA的p-value</span><br><span class="line"><span class="built_in">F,</span>pval_1 = fast_anov<span class="built_in">a_2bit</span>(geno_conv_train.T,trait1)</span><br><span class="line"><span class="built_in">F,</span>pval_2 = fast_anov<span class="built_in">a_2bit</span>(geno_conv_train.T,trait2)</span><br><span class="line"><span class="built_in">F,</span>pval_3 = fast_anov<span class="built_in">a_2bit</span>(geno_conv_train.T,trait3)</span><br><span class="line">pval_1.shape</span><br></pre></td></tr></table></figure><figure class="highlight lsl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">fig, ax = plt.subplots(<span class="number">1</span>,<span class="number">3</span>, figsize=(<span class="number">15</span>,<span class="number">3</span>))</span><br><span class="line">ax[<span class="number">0</span>].hist(pval_1,bins = <span class="number">50</span>)</span><br><span class="line">ax[<span class="number">0</span>].set_title('<span class="number">5000</span> SNPs p-value for trait1 distribution')</span><br><span class="line">ax[<span class="number">1</span>].hist(pval_2,bins = <span class="number">50</span>)</span><br><span class="line">ax[<span class="number">1</span>].set_title('<span class="number">5000</span> SNPs p-value for trait2 distribution')</span><br><span class="line">ax[<span class="number">2</span>].hist(pval_3,bins = <span class="number">50</span>)</span><br><span class="line">ax[<span class="number">2</span>].set_title('<span class="number">5000</span> SNPs p-value for trait3 distribution')</span><br></pre></td></tr></table></figure><p><img src="http://i1.bvimg.com/640680/b8b10e4aac94c22f.png" alt="Markdown"><br>可以设定一个阈值，比如留下p-value前百分之四十的SNPs,根据index选取留下的SNPs<br><figure class="highlight cos"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">threshold1 = np.percentile(pval_1,<span class="number">40</span>)</span><br><span class="line">threshold2 = np.percentile(pval_2,<span class="number">40</span>)</span><br><span class="line">threshold3 = np.percentile(pval_3,<span class="number">40</span>)</span><br><span class="line"><span class="keyword">print</span> 'threshold1: <span class="built_in">%f</span>' <span class="built_in">%threshold</span>1</span><br><span class="line"><span class="keyword">print</span> 'threshold2: <span class="built_in">%f</span>' <span class="built_in">%threshold</span>2</span><br><span class="line"><span class="keyword">print</span> 'threshold3: <span class="built_in">%f</span>' <span class="built_in">%threshold</span>3</span><br></pre></td></tr></table></figure></p><figure class="highlight ini"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#返回符合条件的p-value的坐标，即可找到需要留下的SNPs的位置，注意每个SNPs占据两行</span></span><br><span class="line"><span class="attr">anova_index_1</span> = np.where(pval_1&lt;threshold1)[<span class="number">0</span>]</span><br><span class="line"><span class="attr">anova_index_1</span> = np.sort(np.concatenate((anova_index_1,anova_index_1 +<span class="number">1</span>)))</span><br><span class="line"><span class="attr">anova_index_2</span> = np.where(pval_2&lt;threshold2)[<span class="number">0</span>]</span><br><span class="line"><span class="attr">anova_index_2</span> = np.sort(np.concatenate((anova_index_2,anova_index_2 +<span class="number">1</span>)))</span><br><span class="line"><span class="attr">anova_index_3</span> = np.where(pval_3&lt;threshold3)[<span class="number">0</span>]</span><br><span class="line"><span class="attr">anova_index_3</span> = np.sort(np.concatenate((anova_index_3,anova_index_3 +<span class="number">1</span>)))</span><br></pre></td></tr></table></figure><figure class="highlight ini"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#根据p-value选取保留的SNPs，注意这一步要在所有的6210个样本上做</span></span><br><span class="line"><span class="attr">feature1_anova</span> = np.take(geno_conv,anova_index_1,axis=<span class="number">0</span>)</span><br><span class="line"><span class="attr">feature2_anova</span> = np.take(geno_conv,anova_index_2,axis=<span class="number">0</span>)</span><br><span class="line"><span class="attr">feature3_anova</span> = np.take(geno_conv,anova_index_3,axis=<span class="number">0</span>)</span><br></pre></td></tr></table></figure><h4 id="3-2-Random-projection"><a href="#3-2-Random-projection" class="headerlink" title="3.2 Random projection"></a>3.2 Random projection</h4><p>Random projection 不依赖于性状，仅仅在原SNPs数据进行降维 <br><br>Random projection可以使用scikit-learn下的sklearn.random_projection模块计算 <br><br>包括generate，transform和normalize等步骤 <br><br>这里演示从5000个feature（10000行）降维</p><h5 id="3-2-1-generate"><a href="#3-2-1-generate" class="headerlink" title="3.2.1 generate"></a>3.2.1 generate</h5><p>产生一个稀疏矩阵</p><figure class="highlight lsl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">#<span class="number">10000</span>为操作前的feature个数：<span class="number">2</span>*<span class="number">5000</span></span><br><span class="line">X = np.zeros((<span class="number">2</span>, <span class="number">10000</span>))</span><br><span class="line">#确定降维后的个数，这里定为<span class="number">1000</span>，使用sklearn random_projection 模块下的 SparseRandomProjection 函数</span><br><span class="line">proj = SparseRandomProjection(<span class="number">1000</span>)</span><br><span class="line">proj.fit(X)</span><br><span class="line">print proj.components_.shape</span><br></pre></td></tr></table></figure><h5 id="3-2-2-transform"><a href="#3-2-2-transform" class="headerlink" title="3.2.2 transform"></a>3.2.2 transform</h5><figure class="highlight ini"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">X</span>= geno_conv.T</span><br><span class="line"><span class="attr">X_</span> = proj.transform(X)</span><br></pre></td></tr></table></figure><h5 id="3-2-3-normalize"><a href="#3-2-3-normalize" class="headerlink" title="3.2.3 normalize"></a>3.2.3 normalize</h5><p>对每个feature进行normalize，避免出现过大的值<br><figure class="highlight capnproto"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler</span><br><span class="line">normalized_feeature = StandardScaler().fit_transform(X_).T</span><br></pre></td></tr></table></figure></p><p>最终大小为1000*6210，结果在feature_selection/randomproj_5000</p><h3 id="4-回归模型"><a href="#4-回归模型" class="headerlink" title="4. 回归模型"></a>4. 回归模型</h3><p>这部分通过几个常用的机器学习模型对上一部处理过的feature进行拟合和预测<br>这里使用sklearn和xgboost提供的模块，这些模块具有很好的封装，使用风格统一，使用时可以查看其官方文档<br><br>这里不介绍具体的机器学习模型的算法原理，可以参考周志华老师的《机器学习》等书进行学习。</p><h4 id="4-1-机器学习模型"><a href="#4-1-机器学习模型" class="headerlink" title="4.1 机器学习模型"></a>4.1 机器学习模型</h4><p>接下来会使用一些常用的可以用于回归的机器学习模型，可以选择其中的一种或几种对feature_selection/文件夹下的三种数据进行回归和预测。下面我们将几个模型列出，并且选择其中的一种作为示例，其他的模型可同理调用。 <br></p><h4 id="4-2-评价指标"><a href="#4-2-评价指标" class="headerlink" title="4.2 评价指标"></a>4.2 评价指标</h4><p>我们使用<script type="math/tex">r^2,pcc</script>作为衡量预测结果的指标</p><p>$r^2 = 1-\frac{SS<em>{res}}{SS</em>{tot}}$</p><p>$pcc = \frac{cov(X,Y)}{\sigma_X \sigma_Y} = \frac{E[(X-\mu_X)(Y-\mu_Y)]}{\sigma_X \sigma_Y} $</p><p>我们可以绘制结果的heatmap图，散点图等进行可视化。</p><h4 id="4-3-交叉验证"><a href="#4-3-交叉验证" class="headerlink" title="4.3 交叉验证"></a>4.3 交叉验证</h4><p>交叉验证(Cross validation)可以帮助调参，寻找机器学习模型中的超参数 <br><br>一般可以使用10折或者5折交叉验证，注意在最终预测时，使用调参后的模型在整个训练集上训练，这时不再交叉验证 <br><br>因为交叉验证需要额外增加计算时间，因此这里只在整个训练集上训练一次，不再展示交叉验证的过程。</p><p><strong>如果深究的话，本问题还有其特殊性，可以设计特殊的交叉验证方式</strong><br>不同的样本具有关联性CV，有的样本可能来自同一亲本，而且训练集和测试集的划分并不是随机的<br>因此在真正解决这个问题的时候，需要考虑不同的抽样方式下的调参与训练，我们可以使用下图所示的几种抽样方式<br><figure class="highlight oxygene"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> <span class="function"><span class="keyword">method</span> <span class="title">in</span> <span class="params">(<span class="string">'random'</span>, <span class="string">'by_female'</span>, <span class="string">'by_male'</span>, <span class="string">'cross'</span>)</span>:</span></span><br><span class="line"><span class="keyword">with</span> h5py.File(<span class="string">'data/cv_index.%s'</span>%<span class="function"><span class="keyword">method</span>, '<span class="title">r</span>') <span class="title">as</span> <span class="title">f</span>:</span></span><br><span class="line">index_train = f[<span class="string">'0/train'</span>][:]</span><br><span class="line">index_test = f[<span class="string">'0/test'</span>][:]</span><br><span class="line">fig, ax = subplots(<span class="number">2</span>, <span class="number">1</span>, figsize=(<span class="number">16</span>, <span class="number">6</span>))</span><br><span class="line">sampling_table = np.zeros(np.prod(parent_table.shape))</span><br><span class="line">sampling_table[index_train] = <span class="number">1</span></span><br><span class="line">sampling_table = np.take(sampling_table, parent_table)</span><br><span class="line">ax[<span class="number">0</span>].matshow(sampling_table, cmap=cm.Greys)</span><br><span class="line">ax[<span class="number">0</span>].set_title(<span class="string">'Training samples (%s)'</span>%<span class="function"><span class="keyword">method</span>)</span></span><br><span class="line"><span class="function"></span></span><br><span class="line"><span class="function"><span class="title">sampling_table</span> = <span class="title">np</span>.<span class="title">zeros</span><span class="params">(np.prod(parent_table.shape)</span>)</span></span><br><span class="line"><span class="function"><span class="title">sampling_table</span>[<span class="title">index_test</span>] = 1</span></span><br><span class="line"><span class="function"><span class="title">sampling_table</span> = <span class="title">np</span>.<span class="title">take</span><span class="params">(sampling_table, parent_table)</span></span></span><br><span class="line"><span class="function"><span class="title">ax</span>[1].<span class="title">matshow</span><span class="params">(sampling_table, cmap=cm.Greys)</span></span></span><br><span class="line"><span class="function"><span class="title">ax</span>[1].<span class="title">set_title</span><span class="params">(<span class="string">'Test samples (%s)'</span>%<span class="keyword">method</span>)</span></span></span><br><span class="line"><span class="function"><span class="title">plt</span>.<span class="title">tight_layout</span><span class="params">()</span></span></span><br></pre></td></tr></table></figure></p><p><img src="http://i1.bvimg.com/640680/53ed95bc3fc879c9.png" alt="Markdown"><br><img src="http://i1.bvimg.com/640680/3a9fec1a694dc533.png" alt="Markdown"><br><img src="http://i1.bvimg.com/640680/f9198785ddd4e809.png" alt="Markdown"><br><img src="http://i1.bvimg.com/640680/18e028db8839992e.png" alt="Markdown"></p><h4 id="4-4-准备数据"><a href="#4-4-准备数据" class="headerlink" title="4.4 准备数据"></a>4.4 准备数据</h4><p>sklearn的机器学习模型一般需要提供X和y以供模型训练，然后提供新的X，模型就可以预测新的y <br><br>通过前面的工作，我们获得了三种不同的X(ANOVA、random projection(5000/whole))，我们还需要将X和y划分为训练集和测试集 <br><br>注意我们需要分别对三个性状进行预测，因此ANOVA的X是三种 <br><br><strong>评价模型的时候要注意，y_true的部分值缺失</strong></p><h5 id="4-4-1-准备y"><a href="#4-4-1-准备y" class="headerlink" title="4.4.1 准备y"></a>4.4.1 准备y</h5><p>先处理y，y 的train和test是统一的，不受方法影响<br><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="section">#y 的train和test的统一的，不受方法影响</span></span><br><span class="line">pheno<span class="emphasis">_whole = pd.read_</span>csv('data/emaize<span class="emphasis">_pheno_</span>whole',delimiter=',')</span><br><span class="line">wholepheno = &#123;&#125;</span><br><span class="line">for trait in ['trait1','trait2','trait3']:</span><br><span class="line">wholepheno[trait] = np.array(pheno_whole[trait])</span><br><span class="line">y_train = &#123;&#125;</span><br><span class="line">y_test = &#123;&#125;</span><br><span class="line">for trait in ['trait1','trait2','trait3']:</span><br><span class="line">y_train[<span class="string">trait</span>] = wholepheno[<span class="string">trait</span>][<span class="symbol">:4754</span>]</span><br><span class="line">y_test[<span class="string">trait</span>] = wholepheno[<span class="string">trait</span>][<span class="symbol">4754:</span>]</span><br></pre></td></tr></table></figure></p><h5 id="4-4-2-准备X"><a href="#4-4-2-准备X" class="headerlink" title="4.4.2 准备X"></a>4.4.2 准备X</h5><p>再处理X，使用ANOVA时X要区分不同性状，Random projection由于是与性状无关的降维方法，三种性状下的feature都一样 <br><br>因为机器学习模型要求一般数据形式为sample*feature，因此需要对结果转置<br><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">def prepare_data(method):</span><br><span class="line">if method == 'randomproj_5000':</span><br><span class="line">with h5py.File('feature<span class="emphasis">_selection/randomproj_</span>5000') as f:</span><br><span class="line">X_train = f[<span class="string">'data'</span>][<span class="symbol">:</span>][<span class="string">:,:4754</span>].T</span><br><span class="line">X_test = f[<span class="string">'data'</span>][<span class="symbol">:</span>][<span class="string">:,4754:</span>].T</span><br><span class="line">if method == 'randomproj_whole':</span><br><span class="line">with h5py.File('feature<span class="emphasis">_selection/randomproj_</span>whole') as f:</span><br><span class="line">X_train = f[<span class="string">'X'</span>][<span class="symbol">:</span>][<span class="string">:4754,:</span>]</span><br><span class="line">X_test = f[<span class="string">'X'</span>][<span class="symbol">:</span>][<span class="string">4754:,:</span>]</span><br><span class="line">if method == 'anova':</span><br><span class="line">X_train = &#123;&#125;</span><br><span class="line">X_test = &#123;&#125;</span><br><span class="line">with h5py.File('feature_selection/anova') as f:</span><br><span class="line">X_train[<span class="string">'trait1'</span>] = f[<span class="string">'feature1'</span>][<span class="symbol">:</span>][<span class="string">:,:4754</span>].T</span><br><span class="line">X_test[<span class="string">'trait1'</span>] = f[<span class="string">'feature1'</span>][<span class="symbol">:</span>][<span class="string">:,4754:</span>].T</span><br><span class="line">X_train[<span class="string">'trait2'</span>] = f[<span class="string">'feature2'</span>][<span class="symbol">:</span>][<span class="string">:,:4754</span>].T</span><br><span class="line">X_test[<span class="string">'trait2'</span>] = f[<span class="string">'feature2'</span>][<span class="symbol">:</span>][<span class="string">:,4754:</span>].T</span><br><span class="line">X_train[<span class="string">'trait3'</span>] = f[<span class="string">'feature3'</span>][<span class="symbol">:</span>][<span class="string">:,:4754</span>].T</span><br><span class="line">X_test[<span class="string">'trait3'</span>] = f[<span class="string">'feature3'</span>][<span class="symbol">:</span>][<span class="string">:,4754:</span>].T</span><br><span class="line">return X<span class="emphasis">_train,X_</span>test</span><br></pre></td></tr></table></figure></p><p>选择三种方法之一作为X,注意ANOVA方法返回的X需要指明性状</p><figure class="highlight mel"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">#查看anova方法的X_train X_test</span><br><span class="line">X_train, X_test = prepare_data(<span class="string">'anova'</span>)</span><br><span class="line"><span class="keyword">print</span> <span class="string">'anova method X_train shape: %s'</span> %(X_train[<span class="string">'trait1'</span>].shape,)</span><br><span class="line"><span class="keyword">print</span> <span class="string">'anova method X_test shape: %s'</span> %(X_test[<span class="string">'trait1'</span>].shape,)</span><br></pre></td></tr></table></figure><figure class="highlight maxima"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">#查看<span class="built_in">random</span> projection方法的X_train X_test</span><br><span class="line">X_train, X_test = prepare_data('randomproj_5000')</span><br><span class="line"><span class="built_in">print</span> '<span class="built_in">random</span> projection <span class="built_in">method</span> X_train shape: <span class="built_in">%s</span>' <span class="symbol">%</span>(X_train.shape,)</span><br><span class="line"><span class="built_in">print</span> '<span class="built_in">random</span> projection <span class="built_in">method</span> X_test shape: <span class="built_in">%s</span>' <span class="symbol">%</span>(X_test.shape,)</span><br></pre></td></tr></table></figure><h4 id="4-5-选择需要的机器学习模型"><a href="#4-5-选择需要的机器学习模型" class="headerlink" title="4.5 选择需要的机器学习模型"></a>4.5 选择需要的机器学习模型</h4><p>接下来会提供多种机器学习模型，并且讲解其使用方法，可以选择自己喜欢的模型进行回归，也可以用sklearn或其他package提供的模型<br>模型包括：</p><ul><li>lr: Linear regression</li><li>ridge: Ridge regression</li><li>kr: Kernel Ridge regression</li><li>rfr: Random Forest regression</li><li>xgbr: XGBoost regression</li><li>knr: K-nearest neigbour regression</li><li>gpr: Gaussian Process regression<figure class="highlight nix"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">def Model(model):</span><br><span class="line"><span class="keyword">if</span> <span class="attr">model=='lr':</span></span><br><span class="line"><span class="attr">reg</span> = LinearRegression()</span><br><span class="line"><span class="comment">#elif model=='xgbr':</span></span><br><span class="line"><span class="comment"># reg = XGBRegressor()</span></span><br><span class="line">elif <span class="attr">model=='ridge':</span></span><br><span class="line"><span class="attr">reg</span> = Ridge()</span><br><span class="line">elif <span class="attr">model=='kr':</span></span><br><span class="line"><span class="attr">reg</span> = KernelRidge(<span class="attr">alpha</span> = <span class="number">10000</span>, <span class="attr">kernel</span> = 'polynomial',<span class="attr">degree</span> = <span class="number">3</span>)</span><br><span class="line">elif <span class="attr">model=='knr':</span></span><br><span class="line"><span class="attr">reg</span> = neighbors.KNeighborsRegressor(<span class="attr">n_neighbors=4,</span> <span class="attr">algorithm='brute')</span></span><br><span class="line">elif <span class="attr">model=='rfr':</span></span><br><span class="line"><span class="attr">reg</span> = RandomForestRegressor(<span class="attr">n_estimators=10,</span> <span class="attr">criterion='mse',</span> <span class="attr">max_depth=12,</span> <span class="attr">n_jobs=5)</span></span><br><span class="line">elif <span class="attr">model=='gpr':</span></span><br><span class="line"><span class="attr">kernel</span> = <span class="number">1.0</span> * DotProduct(<span class="attr">sigma_0=1.0)**4</span></span><br><span class="line"><span class="attr">reg</span> = GaussianProcessRegressor(<span class="attr">kernel</span> = kernel, <span class="attr">optimizer=None)</span></span><br><span class="line">return reg</span><br></pre></td></tr></table></figure></li></ul><p>接下来可以使用三种特征（X）中的一种以及七种方法中的一种进行训练和预测 <br><br>这里我们以randomproj_5000作为特征，用Ridge作为回归模型演示 <br><br>如果用anova得到的feature，需要注意不同性状的feature不一样 <br></p><h5 id="某个机器学习模型的使用方法如下：reg-fit-X-y-用于拟合，reg-predict-X-用于预测。更多用法可以参考sklearn官方文档"><a href="#某个机器学习模型的使用方法如下：reg-fit-X-y-用于拟合，reg-predict-X-用于预测。更多用法可以参考sklearn官方文档" class="headerlink" title="某个机器学习模型的使用方法如下：reg.fit(X,y)用于拟合，reg.predict(X)用于预测。更多用法可以参考sklearn官方文档"></a>某个机器学习模型的使用方法如下：reg.fit(X,y)用于拟合，reg.predict(X)用于预测。更多用法可以参考sklearn官方文档</h5><figure class="highlight kotlin"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">X_train, X_test = prepare_data(<span class="string">'randomproj_whole'</span>)</span><br><span class="line">reg = Model(<span class="string">'gpr'</span>)</span><br><span class="line">y_predict = &#123;&#125;</span><br><span class="line">y_predict_train = &#123;&#125;</span><br><span class="line"><span class="keyword">for</span> <span class="class"><span class="keyword">trait</span> <span class="title">in</span> <span class="title">tqdm</span></span>([<span class="string">'trait1'</span>,<span class="string">'trait2'</span>,<span class="string">'trait3'</span>]):</span><br><span class="line">reg.fit(X_train,y_train[<span class="class"><span class="keyword">trait</span>])</span></span><br><span class="line">y_predict[<span class="class"><span class="keyword">trait</span>] = <span class="title">reg</span>.<span class="title">predict</span></span>(X_test)</span><br><span class="line">y_predict_train[<span class="class"><span class="keyword">trait</span>] = <span class="title">reg</span>.<span class="title">predict</span></span>(X_train)</span><br><span class="line"></span><br><span class="line">X_test.shape</span><br></pre></td></tr></table></figure><p>计算预测结果与真实值的<script type="math/tex">r^2,pcc</script><br><figure class="highlight kotlin"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">test_nonan = np.where(np.isnan(np.array(pheno_whole[<span class="string">'trait1'</span>])[<span class="number">4754</span>:]) ==<span class="number">0</span>)</span><br><span class="line">pcc_train = &#123;&#125;</span><br><span class="line">pcc_test = &#123;&#125;</span><br><span class="line">r2_train = &#123;&#125;</span><br><span class="line">r2_test = &#123;&#125;</span><br><span class="line"><span class="keyword">for</span> <span class="class"><span class="keyword">trait</span> <span class="title">in</span> ['<span class="title">trait1</span>',<span class="type">'trait2'</span>,<span class="type">'trait3']:</span></span></span><br><span class="line">pcc_test[<span class="class"><span class="keyword">trait</span>] = <span class="title">pearsonr</span></span>(y_predict[<span class="class"><span class="keyword">trait</span>][<span class="title">test_nonan</span>],<span class="type">np.array</span></span>(pheno_whole[<span class="class"><span class="keyword">trait</span>])[4754:<span class="type">][test_nonan])</span></span></span><br><span class="line">pcc_train[<span class="class"><span class="keyword">trait</span>] = <span class="title">pearsonr</span></span>(y_predict_train[<span class="class"><span class="keyword">trait</span>],<span class="type">y_train[trait])</span></span></span><br><span class="line">r2_test[<span class="class"><span class="keyword">trait</span>] = <span class="title">r2_score</span></span>(y_predict[<span class="class"><span class="keyword">trait</span>][<span class="title">test_nonan</span>],<span class="type">np.array</span></span>(pheno_whole[<span class="class"><span class="keyword">trait</span>])[4754:<span class="type">][test_nonan])</span></span></span><br><span class="line">r2_train[<span class="class"><span class="keyword">trait</span>] = <span class="title">r2_score</span></span>(y_predict_train[<span class="class"><span class="keyword">trait</span>],<span class="type">y_train[trait])</span></span></span><br></pre></td></tr></table></figure></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pcc_test</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pcc_train</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">r2_test</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">r2_train</span><br></pre></td></tr></table></figure><p>可以看到预测结果并不是很好，在测试集上的pcc只有0.5左右。后续的分析可以发现，这是因为样本之间具有相关性导致的<br><br>具体的原因分析比较复杂，简单来说，因为这组测试集与训练集的样本的亲本之间亲缘关系较远，模型难以从SNPs得到的feature推断出亲本信息，导致预测结果较差。</p><p>绘制heatmap图观察预测结果 <br><br>GPR具有很强的拟合能力，总可以在训练集上得到接近1的PCC<br><figure class="highlight prolog"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">fig, ax = plt.subplots(<span class="number">2</span>,<span class="number">3</span>, figsize=(<span class="number">15</span>,<span class="number">10</span>))</span><br><span class="line">for i in range(<span class="number">3</span>):</span><br><span class="line">traits = [<span class="string">'trait1'</span>,<span class="string">'trait2'</span>,<span class="string">'trait3'</span>]</span><br><span class="line">ax[<span class="number">0</span>,i].scatter(y_predict[traits[i]][test_nonan],np.array(pheno_whole[traits[i]])[<span class="number">4754</span>:][test_nonan])</span><br><span class="line">ax[<span class="number">0</span>,i].set_title(<span class="string">'%s test set predict &amp; true value plot'</span> <span class="comment">%traits[i])</span></span><br><span class="line">line1 = [(<span class="number">-4</span>, <span class="number">-4</span>), (<span class="number">4</span>, <span class="number">4</span>)]</span><br><span class="line">(line1_xs, line1_ys) = zip(*line1)</span><br><span class="line">ax[<span class="number">0</span>,i].add_line(<span class="symbol">Line2D</span>(line1_xs, line1_ys, linewidth=<span class="number">1</span>, color=<span class="string">'red'</span>))</span><br><span class="line">ax[<span class="number">0</span>,i].set_xlim(left=<span class="number">-4</span>, right=<span class="number">4</span>)</span><br><span class="line">ax[<span class="number">0</span>,i].set_ylim(bottom=<span class="number">-4</span>, top=<span class="number">4</span>)</span><br><span class="line">ax[<span class="number">1</span>,i].scatter(y_predict_train[traits[i]],y_train[traits[i]])</span><br><span class="line">ax[<span class="number">1</span>,i].set_title(<span class="string">'%s train set predict &amp; true value plot'</span> <span class="comment">%traits[i])</span></span><br><span class="line">ax[<span class="number">1</span>,i].add_line(<span class="symbol">Line2D</span>(line1_xs, line1_ys, linewidth=<span class="number">1</span>, color=<span class="string">'red'</span>))</span><br><span class="line">ax[<span class="number">1</span>,i].set_xlim(left=<span class="number">-4</span>, right=<span class="number">4</span>)</span><br><span class="line">ax[<span class="number">1</span>,i].set_ylim(bottom=<span class="number">-4</span>, top=<span class="number">4</span>)</span><br></pre></td></tr></table></figure></p><p><img src="http://i1.bvimg.com/640680/dc071479d62b84c7.png" alt="Markdown"></p><p>绘制完整真实值与预测值的heatmap图 <br><br>从图中我们可以清晰地看出一个基本的模型的问题： <br><br>模型强烈地依赖已有信息进行预测，当未知样本的父本与已知训练集的亲缘关系较远时，模型只能依赖母本（横坐标）进行预测 <br><br>导致预测的heatmap图有明显的与母本相关的特征，而实际上子代的性状更容易被父本主导 <br></p><figure class="highlight kotlin"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">wholepre = np.concatenate((y_predict[<span class="string">'trait1'</span>],y_predict[<span class="string">'trait2'</span>],y_predict[<span class="string">'trait3'</span>])).reshape(<span class="number">3</span>,<span class="number">-1</span>)</span><br><span class="line">predictions = pd.DataFrame(wholepre.T)</span><br><span class="line">predictions.columns = [<span class="string">'trait1'</span>, <span class="string">'trait2'</span>, <span class="string">'trait3'</span>]</span><br><span class="line">predictions = predictions.set_index(np.arange(<span class="number">4754</span>,<span class="number">6210</span>))</span><br><span class="line">def normalize_phenotype(x, range_pheno=<span class="number">4.0</span>):</span><br><span class="line"><span class="keyword">return</span> (np.clip(x, -range_pheno, range_pheno) + range_pheno)/<span class="number">2.0</span>/range_pheno</span><br><span class="line"><span class="keyword">for</span> <span class="class"><span class="keyword">trait</span> <span class="title">in</span> <span class="title">traits</span>:<span class="type"></span></span></span><br><span class="line">fig, ax = subplots(<span class="number">2</span>, <span class="number">1</span>, figsize=(<span class="number">16</span>, <span class="number">6</span>))</span><br><span class="line">ax[<span class="number">0</span>].matshow(np.take(np.ravel(normalize_phenotype(pheno_whole[<span class="class"><span class="keyword">trait</span>].<span class="title">values</span>)), <span class="type">parent_table)</span>, <span class="type">cmap=cm.RdBu_r)</span></span></span><br><span class="line">ax[<span class="number">0</span>].set_title(<span class="string">'Phenotypes of whole true data (%s)'</span>%<span class="class"><span class="keyword">trait</span>)</span></span><br><span class="line"></span><br><span class="line">trait_pred = np.full(phenotypes.shape[<span class="number">0</span>], np.nan)</span><br><span class="line">trait_pred[predictions.index.tolist()] = normalize_phenotype(predictions[<span class="class"><span class="keyword">trait</span>].<span class="title">values</span>)</span></span><br><span class="line">ax[<span class="number">1</span>].matshow(np.take(trait_pred, parent_table), cmap=cm.RdBu)</span><br><span class="line">ax[<span class="number">1</span>].set_title(<span class="string">'Prediction on test data (%s 1)'</span>%traits)</span><br></pre></td></tr></table></figure><p><img src="http://i1.bvimg.com/640680/b514a9c8b7cc7943.png" alt="Markdown"><br><img src="http://i1.bvimg.com/640680/bbe789a8598188da.png" alt="Markdown"><br><img src="http://i1.bvimg.com/640680/19a5441ffb4534eb.png" alt="Markdown"></p><h3 id="后续分析"><a href="#后续分析" class="headerlink" title="后续分析"></a>后续分析</h3><h4 id="不同样本具有不同的预测难度"><a href="#不同样本具有不同的预测难度" class="headerlink" title="不同样本具有不同的预测难度"></a>不同样本具有不同的预测难度</h4><p>普通的机器学习模型在测试集上表现结果不好，但是通过多次的十字交叉抽样模拟，可以发现不同样本的预测难度不同，在大多数样本上，不需要专门设计的机器学习模型就足够表现很好</p><h4 id="样本之间具有关联性"><a href="#样本之间具有关联性" class="headerlink" title="样本之间具有关联性"></a>样本之间具有关联性</h4><p>不服从一些基本的假设，比如线性模型下，残差并不是独立的，需要考虑问题的特殊性进行额外的设计。<br><br>由于存储空间和计算时间的限制，无法展示其他有效的方法，有兴趣的同学可以查找育种领域的其他模型进行尝试。</p><h4 id="复杂的机器学习模型并不一定有效"><a href="#复杂的机器学习模型并不一定有效" class="headerlink" title="复杂的机器学习模型并不一定有效"></a>复杂的机器学习模型并不一定有效</h4><p>育种领域目前最好的模型依然是线性模型，通过特殊的设计，可以考虑到亲缘关系、显著相关的SNP(causal)以及随机效应部分<br>而寻找合适的feature是预测结果好坏的决定性因素，至今没有非常好的方法。</p><p>我们通过模拟特殊的十字交叉抽样方式发现，虽然测试集的样本不好预测，但是大多数的样本使用简单的机器学习方法就可以在大多数样本上取得较好的结果 <br><br>由于计算资源限制，下面直接展示模拟结果</p><p>我们使用一种特殊的十字交叉抽样，在训练集上抽样1000次，用来测试基本的机器学习模型结果 <br><br>我们使用了2bit coding编码的SNPs,通过random projection降维至80000 <br><br>然后使用Gaussian Process Regression作为回归模型 <br><br><img src="http://i1.bvimg.com/640680/839f4f607631b772.png" alt="Markdown"><br><img src="http://i1.bvimg.com/640680/4c1eb79af4bbb5fb.png" alt="Markdown"><br>可以看到一千次抽样的测试结果，大多数测试的PCC都比较高</p><p><img src="http://i1.bvimg.com/640680/bea286d0f130d044.png" alt="Markdown"><br>按照样本查看每个样本多次抽样的平均PCC，注意这里是有bias没有消除的<br><img src="http://i1.bvimg.com/640680/17df0a3dcaa71ada.png" alt="Markdown"></p><p>这里绘制了每个样本的平均PCC heatmap图像,可以发现大多数的样本是很好预测的，样本性状基本由父本性状主导(纵坐标为父本),但是少数亲缘关系较远的父本（图中蓝色线）就很难预测。<br><img src="http://i1.bvimg.com/640680/50adb2cb87536a3d.png" alt="Markdown"><br><img src="http://i1.bvimg.com/640680/48ee63cf0c78d296.png" alt="Markdown"><br><img src="http://i1.bvimg.com/640680/d3bb77a76c162d53.png" alt="Markdown"></p><p>不同父本的性状有显著差别，而子代的性状由于设计原因，主要由父本控制。<br><br>我们可以通过绘图查看不同父本的性状的变化<br><figure class="highlight prolog"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">male_index = np.ndarray([<span class="number">6210</span>,]).astype(<span class="string">'int'</span>)</span><br><span class="line">for i in range(<span class="number">6210</span>):</span><br><span class="line">male_index[i] = int(np.array(phenotypes[<span class="string">'pedigree'</span>])[i].split(<span class="string">'_'</span>)[<span class="number">2</span>][<span class="number">1</span>:])</span><br><span class="line">male_trait1 = np.concatenate((male_index.reshape(<span class="number">1</span>,<span class="number">-1</span>),np.array(pheno_whole[<span class="string">'trait1'</span>]).reshape(<span class="number">1</span>,<span class="number">-1</span>))).<span class="symbol">T</span></span><br><span class="line">male_trait1_bysort = male_trait1[male_trait1[:,<span class="number">0</span>].argsort()]</span><br><span class="line">fig, ax = plt.subplots(figsize=(<span class="number">16</span>,<span class="number">8</span>))</span><br><span class="line">ax.plot(male_trait1_bysort[:,<span class="number">1</span>])</span><br><span class="line">ax.set_title(<span class="string">'different males have varied values'</span>)</span><br></pre></td></tr></table></figure></p><p><img src="http://i1.bvimg.com/640680/818f1d13de885240.png" alt="Markdown"><br>以上内容简要地介绍了eMaize问题使用的一些基本的常用的机器学习方法，包括数据预处理、特征选择、降维、回归以及分析。本教程还顺便展示了一些python常用的工具包的使用，读者有时间可以慢慢体会其中的具体操作，因为jupyter notebook的可视化与交互性很强，读者可以方便地查看中间步骤的数据情况，更好地理解代码所进行的操作。<br><br>由于实际工作的步骤、数据量、变量等问题，还需要慎重考虑计算时间、任务管理等工作 <br><br>想要预测较难预测的样本仅仅靠常规的机器学习方法并不够用，将机器学习应用于生物学问题时，不能简单套用模型，还需要根据问题进行针对性的设计，才有可能取得更好的结果。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;这是为实验室写的，借由eMaize问题帮助大家简单了解机器学习基本方法和基础代码的教程。也可以在&lt;a href=&quot;https://lulab.gitbooks.io/bioinfo/content/5%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%95%B4%E5%90%88----%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%92%8C%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/51.html&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;这里&lt;/a&gt;看到&lt;/p&gt;
&lt;p&gt;由于jupyter notebook的强大的展示功能，本教程还用jupyter notebook组织且运行，可以获得更好的学习效果，代码在&lt;a href=&quot;https://github.com/james20141606/somethingmore/blob/master/bioinfo.ipynb&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;这里&lt;/a&gt;,欢迎取用。&lt;a href=&quot;http://localhost:4000/2018/04/12/setup/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;在这里&lt;/a&gt;我简单介绍了如何配置jupyter，在&lt;a href=&quot;https://james20141606.github.io/2018/04/10/Deep-Learning-Practice/&quot;&gt;Deep Learning tutorial&lt;/a&gt;中我也强烈推荐了jupyter，并且介绍了很多基于jupyter的资源，强烈建议尝试一下。&lt;br&gt;
    
    </summary>
    
      <category term="techniques" scheme="http://james20141606.github.io/categories/techniques/"/>
    
      <category term="machine learning" scheme="http://james20141606.github.io/categories/techniques/machine-learning/"/>
    
    
      <category term="codes" scheme="http://james20141606.github.io/tags/codes/"/>
    
      <category term="statistics" scheme="http://james20141606.github.io/tags/statistics/"/>
    
      <category term="techniques" scheme="http://james20141606.github.io/tags/techniques/"/>
    
      <category term="machine learning" scheme="http://james20141606.github.io/tags/machine-learning/"/>
    
      <category term="bioinformatics" scheme="http://james20141606.github.io/tags/bioinformatics/"/>
    
  </entry>
  
  <entry>
    <title>陈炳林回忆录 序言</title>
    <link href="http://james20141606.github.io/2018/04/11/auto0/"/>
    <id>http://james20141606.github.io/2018/04/11/auto0/</id>
    <published>2018-04-11T02:10:01.000Z</published>
    <updated>2018-04-12T15:22:53.537Z</updated>
    
    <content type="html"><![CDATA[<p>本文章同时被整理成一本小书，放置在我的gitbook账户下，点击<a href="https://legacy.gitbook.com/book/james20141606/grandpa-autobiography/details" target="_blank" rel="noopener">这里</a>可以阅读。</p><h1 id="Preface-前言"><a href="#Preface-前言" class="headerlink" title="Preface 前言"></a>Preface 前言</h1><p>这份不长不短的回忆录源自于清华大学的毛中特课程的一份作业，在一年以前就打算选择冯务中老师的课，原因就是打听了各个老师的任务，对这项任务非常感兴趣，无奈没有抢到课，但是却开始了帮助爷爷奶奶整理这份回忆录的过程。平时每个周末都会和爷爷奶奶视频聊天一两个小时，自从有了这个想法，就会专门和爷爷奶奶聊过去的故事，顺带鼓励他们动笔写一些。爷爷年轻时是县委组织部的笔杆子，虽然已经七十多岁了，听到我的鼓励也有些心动，多年未提笔，写起来却是收不住，听奶奶说爷爷经常凌晨四五点起来就开始写，边写边流泪，回忆幼年时的艰辛与不易。这份回忆录，讲到了爷爷中年时期即止，爷爷说，年轻的生活更加刻骨铭心，令人难忘，后来生活好转，一切顺利如意，倒也没什么可写了。<br><a id="more"></a></p><p>这份回忆录以爷爷为主要视角叙述，补充了很多和奶奶讨论之后获得的细节，家里过去非常的穷，留下的资料几乎为零，曾经爷爷的哥哥大爷试图整理一份家谱出来，也被爷爷的爸爸在大爷去世后烧毁，因此我们也觉得能够再整理出一些过去的故事非常有意义。这里面的一些故事朴实又动人，在我整理的过程中充满了感慨和感动，让我体会到祖辈们的艰苦和不屈的精神。有的故事还带来了意想不到的惊喜，比如爷爷专门回忆了他年轻时结识的一个好朋友周聚照，已经几十年联系不上了，我整理完爷爷的回忆录，对这位朋友印象深刻，因此自告奋勇帮爷爷联系，在几个可能的地点的百度贴吧发布帖子，真的找到了这位老人的家人。爷爷和奶奶非常激动，第二天就坐车前去看望，周聚照老人已经不在了，但是他的后代生活的很好，孙辈们都获得了很好的教育，考入了很好的大学，改变了自己的命运，真的很令人感慨。另一个故事，爷爷没有亲手写下来，但是还是忍不住告诉了我，就是回忆录的最后一个故事，关于正义的故事，这个发生于爷爷的父亲身上的真实的故事深深的震撼了我，让我对那个混乱的年代有了更深的体会。<br>最后还整理了一个简单的按照年份的时间表，还用一个专门的软件macfamilytree制作了一个这三四代人的家谱树，希望未来的家谱树可以越来越大，开枝散叶，生生不息。</p><h1 id="目录-Table-of-Contents"><a href="#目录-Table-of-Contents" class="headerlink" title="目录   Table of Contents"></a>目录   Table of Contents</h1><h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a><a href="https://james20141606.github.io/2018/04/11/auto0/">前言</a></h2><h2 id="Chapter-Ⅰ-我的童年"><a href="#Chapter-Ⅰ-我的童年" class="headerlink" title="Chapter Ⅰ 我的童年"></a><a href="https://james20141606.github.io/2018/04/11/auto1/">Chapter Ⅰ 我的童年</a></h2><h2 id="Chapter-Ⅱ-初中生活"><a href="#Chapter-Ⅱ-初中生活" class="headerlink" title="Chapter Ⅱ 初中生活"></a><a href="https://james20141606.github.io/2018/04/11/auto2/">Chapter Ⅱ 初中生活</a></h2><h2 id="Chapter-Ⅲ-难忘的一九五八"><a href="#Chapter-Ⅲ-难忘的一九五八" class="headerlink" title="Chapter Ⅲ 难忘的一九五八"></a><a href="https://james20141606.github.io/2018/04/11/auto3/">Chapter Ⅲ 难忘的一九五八</a></h2><h2 id="Chapter-Ⅳ-新的篇章"><a href="#Chapter-Ⅳ-新的篇章" class="headerlink" title="Chapter Ⅳ 新的篇章"></a><a href="https://james20141606.github.io/2018/04/11/auto4/">Chapter Ⅳ 新的篇章</a></h2><h2 id="Chapter-Ⅴ-休学的日子"><a href="#Chapter-Ⅴ-休学的日子" class="headerlink" title="Chapter Ⅴ 休学的日子"></a><a href="https://james20141606.github.io/2018/04/11/auto5/">Chapter Ⅴ 休学的日子</a></h2><h2 id="Chapter-Ⅵ-婚后的生活"><a href="#Chapter-Ⅵ-婚后的生活" class="headerlink" title="Chapter Ⅵ 婚后的生活"></a><a href="https://james20141606.github.io/2018/04/11/auto6/">Chapter Ⅵ 婚后的生活</a></h2><h2 id="Chapter-Ⅶ-喜上加喜-喜中有忧"><a href="#Chapter-Ⅶ-喜上加喜-喜中有忧" class="headerlink" title="Chapter Ⅶ 喜上加喜 喜中有忧"></a><a href="https://james20141606.github.io/2018/04/11/auto7/">Chapter Ⅶ 喜上加喜 喜中有忧</a></h2><h2 id="Chapter-Ⅷ-工作-崭新的篇章"><a href="#Chapter-Ⅷ-工作-崭新的篇章" class="headerlink" title="Chapter Ⅷ 工作 崭新的篇章"></a><a href="https://james20141606.github.io/2018/04/11/auto8/">Chapter Ⅷ 工作 崭新的篇章</a></h2><h2 id="Essays-短文数篇"><a href="#Essays-短文数篇" class="headerlink" title="Essays    短文数篇"></a><a href="https://james20141606.github.io/2018/04/11/auto9/">Essays    短文数篇</a></h2><h3 id="忆祖母"><a href="#忆祖母" class="headerlink" title="忆祖母"></a>忆祖母</h3><h3 id="忆母亲"><a href="#忆母亲" class="headerlink" title="忆母亲"></a>忆母亲</h3><h3 id="四伯家生活写照"><a href="#四伯家生活写照" class="headerlink" title="四伯家生活写照"></a>四伯家生活写照</h3><h3 id="八岁孩子学走路"><a href="#八岁孩子学走路" class="headerlink" title="八岁孩子学走路"></a>八岁孩子学走路</h3><h3 id="我的第一双棉鞋"><a href="#我的第一双棉鞋" class="headerlink" title="我的第一双棉鞋"></a>我的第一双棉鞋</h3><h3 id="大伯和父亲给祖母惹祸"><a href="#大伯和父亲给祖母惹祸" class="headerlink" title="大伯和父亲给祖母惹祸"></a>大伯和父亲给祖母惹祸</h3><h3 id="真实故事三则"><a href="#真实故事三则" class="headerlink" title="真实故事三则"></a>真实故事三则</h3><h3 id="我的朋友周聚照"><a href="#我的朋友周聚照" class="headerlink" title="我的朋友周聚照"></a>我的朋友周聚照</h3><h3 id="人生机遇只有一次"><a href="#人生机遇只有一次" class="headerlink" title="人生机遇只有一次"></a>人生机遇只有一次</h3><h3 id="过个革命化春节"><a href="#过个革命化春节" class="headerlink" title="过个革命化春节"></a>过个革命化春节</h3><h3 id="石人传说二则"><a href="#石人传说二则" class="headerlink" title="石人传说二则"></a>石人传说二则</h3><h3 id="Poems-诗歌九则"><a href="#Poems-诗歌九则" class="headerlink" title="Poems 诗歌九则"></a><a href="https://james20141606.github.io/2018/04/11/auto10/">Poems 诗歌九则</a></h3><h4 id="玩秋千"><a href="#玩秋千" class="headerlink" title="玩秋千"></a>玩秋千</h4><h4 id="石人山景"><a href="#石人山景" class="headerlink" title="石人山景"></a>石人山景</h4><h4 id="恋家"><a href="#恋家" class="headerlink" title="恋家"></a>恋家</h4><h4 id="四伯"><a href="#四伯" class="headerlink" title="四伯"></a>四伯</h4><h4 id="无题"><a href="#无题" class="headerlink" title="无题"></a>无题</h4><h4 id="思往昔看今朝"><a href="#思往昔看今朝" class="headerlink" title="思往昔看今朝"></a>思往昔看今朝</h4><h4 id="石人山下荡秋千"><a href="#石人山下荡秋千" class="headerlink" title="石人山下荡秋千"></a>石人山下荡秋千</h4><h4 id="咏春"><a href="#咏春" class="headerlink" title="咏春"></a>咏春</h4><h4 id="大竹园变堰潭"><a href="#大竹园变堰潭" class="headerlink" title="大竹园变堰潭"></a>大竹园变堰潭</h4><h2 id="Story-of-Justice-一个关于正义的故事"><a href="#Story-of-Justice-一个关于正义的故事" class="headerlink" title="Story of Justice    一个关于正义的故事"></a><a href="https://james20141606.github.io/2018/04/11/auto11/">Story of Justice    一个关于正义的故事</a></h2><h2 id="Chronology年表"><a href="#Chronology年表" class="headerlink" title="Chronology年表"></a><a href="https://james20141606.github.io/2018/04/11/auto12/">Chronology年表</a></h2>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文章同时被整理成一本小书，放置在我的gitbook账户下，点击&lt;a href=&quot;https://legacy.gitbook.com/book/james20141606/grandpa-autobiography/details&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;这里&lt;/a&gt;可以阅读。&lt;/p&gt;
&lt;h1 id=&quot;Preface-前言&quot;&gt;&lt;a href=&quot;#Preface-前言&quot; class=&quot;headerlink&quot; title=&quot;Preface 前言&quot;&gt;&lt;/a&gt;Preface 前言&lt;/h1&gt;&lt;p&gt;这份不长不短的回忆录源自于清华大学的毛中特课程的一份作业，在一年以前就打算选择冯务中老师的课，原因就是打听了各个老师的任务，对这项任务非常感兴趣，无奈没有抢到课，但是却开始了帮助爷爷奶奶整理这份回忆录的过程。平时每个周末都会和爷爷奶奶视频聊天一两个小时，自从有了这个想法，就会专门和爷爷奶奶聊过去的故事，顺带鼓励他们动笔写一些。爷爷年轻时是县委组织部的笔杆子，虽然已经七十多岁了，听到我的鼓励也有些心动，多年未提笔，写起来却是收不住，听奶奶说爷爷经常凌晨四五点起来就开始写，边写边流泪，回忆幼年时的艰辛与不易。这份回忆录，讲到了爷爷中年时期即止，爷爷说，年轻的生活更加刻骨铭心，令人难忘，后来生活好转，一切顺利如意，倒也没什么可写了。&lt;br&gt;
    
    </summary>
    
      <category term="爷爷回忆录" scheme="http://james20141606.github.io/categories/%E7%88%B7%E7%88%B7%E5%9B%9E%E5%BF%86%E5%BD%95/"/>
    
    
      <category term="life" scheme="http://james20141606.github.io/tags/life/"/>
    
      <category term="autobiograpy" scheme="http://james20141606.github.io/tags/autobiograpy/"/>
    
  </entry>
  
  <entry>
    <title>回忆录 CHAPTER Ⅰ  我的童年</title>
    <link href="http://james20141606.github.io/2018/04/11/auto1/"/>
    <id>http://james20141606.github.io/2018/04/11/auto1/</id>
    <published>2018-04-11T02:10:00.000Z</published>
    <updated>2018-04-12T07:56:57.045Z</updated>
    
    <content type="html"><![CDATA[<p>我出生在石桥街西夹后布袋街外祖母家那个巷子里，是一九四零年(民国二十九年)生，赶上三十年年成那年。我出生后家里有四口人，大哥已经两岁。在集镇上住，家里没地没房，不做生意，生存十分困难。后来经人介绍，父亲用卖菜的筐一头一个孩子，挑着我们去白河东沙山给地主彭山种地。地主给了草房两间，几亩薄地，生活勉强过得去。日本侵华后战乱频起，又逢灾年（指1942年七月到1943年春天的那场大灾荒，河南受灾总人数达1200万人，约三百万人死亡），祖母不愿骨肉分离，我们一家四口只好又两手空空搬到祖母借住地薛庄去（魏庄西边西边的那个庄）。<br><a id="more"></a><br>1941年春天母亲得了一场大病（奶花疮），那时我才不足一周岁，正值三十年年成，没饭吃也没奶喝，眼看着就要饿死。父亲只得将家里的一床被子和一条床单带上，徒步到老河口换点吃的。当他第三天凌晨回到沙山家里时，我和母亲两人已经两天没有吃东西了。父亲忙生火做面麸汤面水救命，我竟一口气喝了三大碗，肚子撑得得鼓鼓的，父亲说当时我肚子上的青筋都能看到，我还想再喝一些，母亲坚决不要我再喝，说否则会把人撑死，还是母亲心细。这两升麸面可是救了我的命啊。</p><p>再一次搬回薛庄后，祖母，大伯，爹妈，大哥和我六口人没吃没喝，据说那时候能够食用的榆树皮都被剥光了，树枝、豆科的角皮都吃，人吃了以后拉不出来就用竹签剜，母亲说我当时就用的这个办法，吃饭已艰难至此，总不能全家一起饿死吧，为了减轻压力，祖母带着自己平时少言寡语，木讷死板的大儿子，也就是我的大伯远走他乡要饭去。为了大家的生存，大伯也只能跟着祖母要饭去了。母亲说，为了不让我饿死，她只好把我的大哥也送到外公家，留下我自己一个孩子。好心的黄奶（她有一个终身未娶的儿子留在身边）每顿煮菜汤的时候剩下的饭跟都会让我喝，两家人的饭跟救活了我，黄奶也是我的救命恩人！</p><p>奶奶领大伯远走他乡本身就够难了，大伯一个大男人实在是委屈，进村后他就站在树下或者墙根处，不愿进院子里。可是这样怎么能讨得来饭呢。于是每顿都是祖母先讨来饭让大伯吃，吃的差不多了再去要几口饭自己吃，如果要不到，两个人就只能饿肚子。她要饭不是为了大儿子，不是为了她自己，她带走大儿子，留下我们，是为了留下家族的火种。饿死大伯只饿死一个，饿死我们一家人，不知道陈家还能不能延续下去。祖母说：“人留子孙，草留根”，当时的我们，真可谓是离离原上草，一岁一枯荣；野火烧不尽，春风吹又生了。</p><p>灾荒终于过去，祖母和大伯回家了，大伯没有饿死，祖母没有饿死，我也没有饿死，大家都没有饿死，陈氏家族总算有一线希望了。祖母很伟大，大伯的牺牲值得铭记，我还是大伯的过继儿，大伯的恩情我不会忘记。</p><p>解放前，薛庄有个大地主叫做郭老八，大名并不记得了。他每日都搬一个大圈椅，坐在槐树下纳凉，经常自言自语道：我儿强似我，要钱做什么；我儿不如我，要钱干什么？此人精明算计，在土改前他竟然大肆低价变卖土地、房屋，挥霍家产，到了土改时家产变卖一空，竟然成了贫农，我们才知道原来是人家外边有人，提前知道了形势，想躲过一劫。我的祖母、长辈因长期地无一分、椽无一根，困苦惯了，深受压榨剥削，为了有两亩地，竟然在土改的前一年全家人节衣缩食，纺花织布，买下了两亩薄地。结果第二年就土改了，你说傻不傻，没有知识和文化，真是命苦啊！因我们家里太穷，土改时定为雇农，因此将地主李氏南家最好的三间大瓦房分给我们，后来因为此处没有庭院，所以将瓦房推倒，在现在的老家旧居所用这些砖瓦盖了新房。当时还分得一张核桃木雕花木床，两把圈椅，木床不幸遗失，两把圈椅送去了寺庙中。</p><p>一九四八年，八周岁的我的得了天花，发高烧，没有吃的也没有药医，后来竟然下不了床，不会走路了，于是大伯经常用长腰带绑着我带我重学走路，这次大难不死，没有落下什么大毛病，只是让我的体质变得特别差，这也是我一生体质不好的原因。</p><p>我八岁到十岁的几年主要跟随奶奶去石人沟四伯家生活，他那里吃饱饭没有什么大问题，那几年我终身难忘，有苦有乐，有悲有喜。一九五零年的春天，我开始在尹店小学读书，三年后转学到皇路店完小上学。那时候的学校不布置作业，家里又没有一个识字的人，学的怎么样谁又知道呢？</p><p>抽空拾柴捡粪是我那时候最喜欢干的主业，当时土改给我家分了几亩地，我做梦都想买来一头牛耕地，攒粪让庄稼长得好，能够有吃有喝站到人前，这就是我当时的梦想了！父亲答应了我，买了一头全身黑色的小母牛娃，条件是我不能因此耽误了上学，这头小牛完全由我负责，我当然无比爽快地答应。每日上学和放学路上，我都不走大路，一定要从田间地埂走，割青草喂我心爱的小牛，我爱我的小牛就像现代人爱自己的宝马车一样，这牛是我的希望呀。夏季牛拴在外边，我就把床铺到它的附近，生怕有人晚上偷走它，我上学、养牛两不耽误，把小牛照顾得很好。两年之后，它产下了一头小牛犊，这可把我高兴坏了，生牛肚那晚，我一夜未眠，我想：梦想要实现了，我家现在有两头牛了啊！</p><p>既上学又养牛已经够忙活了，后来小我四岁的弟弟炳义也开始在皇路店上小学了，刚吃过饭去上学还好，他有劲就自己走，等放学回家肚子饿了，他就坐在地上耍赖，非要我背他回去，作为哥哥我就只能背着他走一段路，这可耽误了我割草了啊。</p><p>那时候家里穷，没有任何防寒、避雨的工具，夏季还好办，到了冬天，一下雪可就惨了，我只能把鞋子脱下夹在腋下（只有一双鞋，不能弄湿），脚总是会冻得钻心地痛。</p><p>小学离家有两公里地，下雪的时候就干脆不回家吃午饭了，等晚上路上结冰时再穿鞋回家，当然也是因为中午回家也没什么好吃的，不回去还能少一趟冻脚之苦。我从小喜爱咸饭，若是做了咸面条，妈绝对会给留一大碗。有时候母亲也会托人中午捎来一点红薯面馍，啃一口一个白印。吃红薯太多伤胃，我也因此落下了终身的胃病。</p><p>一九五六年小学毕业后我成功考入石桥镇上的初级中学，当时小升初率不高，所以我真是很幸运。我小学时的班主任张文彬老师亲自步行来家里送录取通知书。全家都高兴极了，家里有中学生啦，就像中了状元似的。父亲一定要留张老师吃午饭，老师答应了，那顿饭也就算是谢师宴了吧！后来我也和张老师成了好朋友，忘年之交了。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;我出生在石桥街西夹后布袋街外祖母家那个巷子里，是一九四零年(民国二十九年)生，赶上三十年年成那年。我出生后家里有四口人，大哥已经两岁。在集镇上住，家里没地没房，不做生意，生存十分困难。后来经人介绍，父亲用卖菜的筐一头一个孩子，挑着我们去白河东沙山给地主彭山种地。地主给了草房两间，几亩薄地，生活勉强过得去。日本侵华后战乱频起，又逢灾年（指1942年七月到1943年春天的那场大灾荒，河南受灾总人数达1200万人，约三百万人死亡），祖母不愿骨肉分离，我们一家四口只好又两手空空搬到祖母借住地薛庄去（魏庄西边西边的那个庄）。&lt;br&gt;
    
    </summary>
    
      <category term="爷爷回忆录" scheme="http://james20141606.github.io/categories/%E7%88%B7%E7%88%B7%E5%9B%9E%E5%BF%86%E5%BD%95/"/>
    
    
      <category term="life" scheme="http://james20141606.github.io/tags/life/"/>
    
      <category term="autobiograpy" scheme="http://james20141606.github.io/tags/autobiograpy/"/>
    
  </entry>
  
</feed>
