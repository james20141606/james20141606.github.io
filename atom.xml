<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>James Chen&#39;s Blogs</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://james20141606.github.io/"/>
  <updated>2018-04-15T04:07:55.412Z</updated>
  <id>http://james20141606.github.io/</id>
  
  <author>
    <name>James Chen</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Data Mining of Deng Era</title>
    <link href="http://james20141606.github.io/2018/04/15/datamining/"/>
    <id>http://james20141606.github.io/2018/04/15/datamining/</id>
    <published>2018-04-14T16:55:43.000Z</published>
    <updated>2018-04-15T04:07:55.412Z</updated>
    
    <content type="html"><![CDATA[<p>期中作业之一是写一篇邓小平时代的读后感，说实话这种书是实在没空读了，虽然粗略地翻了几章，十分吸引人，但是这两年真的越来越讨厌写文科式的论文，瞎胡诌凑字数曾经也是我作为理科生的优势，但是这一两年对这种风格的文章：东拼西凑，无病呻吟，迷茫又自负的写作非常地厌恶。因为就想玩点花样，做点简单的文本数据挖掘凑凑字数，虽然多花了很多时间，但是毕竟很有意思，有意思的事情就不算浪费时间对吧，没有意思的事情，哪怕一分钟也是对生命的浪费呢。<br><a id="more"></a><br>中文分词是个很好玩的事情，但是jieba和THULAC之类的工具已经把中文分词和词性标注之类的变得很简单，最折腾的，花了我很久时间的是这本中文材料。。。matplotlib本身不支持英文绘图，python的encoding方式也让我折腾了很久，竟然做了很久装卸各种包的工作，，，直到最后奇葩的matplotlib就是找不到字体，不管在本地还是在几个服务器上竟然都不行，于是只好测试好代码让斌斌帮忙在他的账户跑一下。下面简单记一下过程和代码，最后再把自己胡乱拼凑的论文也扔上。</p><p>代码也放到<a href="https://github.com/james20141606/somethingmore/datamining_dxp" target="_blank" rel="noopener">GitHub</a>上了,里面附有jupyter版本的代码和可以直接运行产生各种类型图片的代码。</p><h1 id="对邓小平时代的分词与词频统计"><a href="#对邓小平时代的分词与词频统计" class="headerlink" title="对邓小平时代的分词与词频统计"></a>对邓小平时代的分词与词频统计</h1><h2 id="数据准备"><a href="#数据准备" class="headerlink" title="数据准备"></a>数据准备</h2><p><strong>convert to UTF-8 format</strong><br><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">file ip.txt</span><br><span class="line"><span class="comment">#use vim to change encoding format</span></span><br><span class="line">:<span class="builtin-name">set</span> <span class="attribute">fileencoding</span>=utf-8</span><br></pre></td></tr></table></figure></p><p>首先是找到txt版本的邓小平时代资源，用utf-8编码，方便后续处理。</p><h2 id="分词与词频统计"><a href="#分词与词频统计" class="headerlink" title="分词与词频统计"></a>分词与词频统计</h2><p>想做词频统计分析，就得对文本进行分词，中文和英文不同，英文单词是孤立的，而中文单词需要人工分开，这里我找了一个比较经典的分词方法，<strong>Jieba分词</strong>，对整本书进行了分词处理，把每句话都给分开，存储了各个名词，并且顺便统计了一下出现频次前10000的所有词语。因此我通过代码可以获取以下<strong>两个文件</strong>：</p><p>被分词分开的全书“词汇”，按顺序一个个存储起来，以及对各个词汇出现频次的统计文件。接下来就可以对数据进行进一步的分析。</p><p><strong>Use Jieba for Chinese words partition</strong><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os, codecs  </span><br><span class="line"><span class="keyword">import</span> jieba  </span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> Counter </span><br><span class="line"><span class="keyword">import</span> matplotlib.pylab <span class="keyword">as</span> plt</span><br><span class="line">plt.style.use(<span class="string">'ggplot'</span>)</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> matplotlib</span><br><span class="line">plt.rcParams[<span class="string">'font.style'</span>] = <span class="string">u'normal'</span></span><br><span class="line">plt.rcParams[<span class="string">'font.family'</span>] = <span class="string">u'Microsoft YaHei'</span></span><br><span class="line"><span class="keyword">with</span> codecs.open(<span class="string">'output.txt'</span>, <span class="string">'r'</span>, <span class="string">'utf8'</span>) <span class="keyword">as</span> f:  </span><br><span class="line">    txt = f.read() </span><br><span class="line">seg_list = jieba.cut(txt) </span><br><span class="line">c = Counter()  </span><br><span class="line"><span class="keyword">for</span> x <span class="keyword">in</span> seg_list:  </span><br><span class="line">    <span class="keyword">if</span> len(x)&gt;<span class="number">1</span> <span class="keyword">and</span> x != <span class="string">'\r\n'</span>:  </span><br><span class="line">        c[x] += <span class="number">1</span></span><br><span class="line">np.savetxt(<span class="string">'count10000.txt'</span>,np.array(c.most_common(<span class="number">10000</span>)),fmt=<span class="string">'%s'</span>)</span><br><span class="line">data = np.loadtxt(<span class="string">'count10000.txt'</span>,dtype=<span class="string">'str'</span>)</span><br><span class="line"><span class="keyword">with</span> codecs.open(<span class="string">'output.txt'</span>, <span class="string">'r'</span>, <span class="string">'utf8'</span>) <span class="keyword">as</span> f:  </span><br><span class="line">    txt = f.read() </span><br><span class="line">wordlist = np.array(txt.split(<span class="string">' '</span>))</span><br><span class="line"><span class="comment">#wordlist.shape</span></span><br><span class="line">countlist = []</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">10000</span>):</span><br><span class="line">    countlist.append(data[i,<span class="number">0</span>]+<span class="string">': '</span>+str(data[i,<span class="number">1</span>]))</span><br><span class="line">pd.DataFrame(np.array(countlist)[:<span class="number">200</span>].reshape(<span class="number">20</span>,<span class="number">10</span>)).head()</span><br></pre></td></tr></table></figure></p><p>然后选取最靠前的200个词语制出来一张表格，从这个表格里还是可以看出一些信息量的，还是很有趣的。比如毛泽东作为中国近现代史的第一人物，是本书除了邓小平之外绕不开的第二号人物。干部一词也反复出现，在中国这是个非常重要的词语，很多东西都取决于干部之间的博弈和关系。北京作为政治中心和中国的代名词，自然也反复出现，而国家和地区层面，美国，苏联、日本和中国台湾也榜上有名，广东作为非常重要的试验地点，被提及的频率也相当的高。人物上，胡耀邦、陈云、赵紫阳、周恩来也都出现了多次。军队、学生等关键词也出现次数不少。</p><p>除此之外还有年份也引人关注，比如1975、1977、1979、1980、1989等关键节点也都帮上有名。</p><p><img src="http://i4.bvimg.com/640680/f405c02d19042f6b.png" alt="Markdown"></p><h2 id="结果可视化"><a href="#结果可视化" class="headerlink" title="结果可视化"></a>结果可视化</h2><p><strong>除此之外，我还对一些非常重要的关键词画了一些可视化的图，这里选取一些放上来。</strong></p><h3 id="bar-plot"><a href="#bar-plot" class="headerlink" title="bar plot"></a>bar plot</h3><figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">namelist = [<span class="string">u'邓小平'</span>,<span class="string">u'中国'</span>,<span class="string">u'毛泽东'</span>,<span class="string">u'工作'</span>,<span class="string">u'干部'</span>,<span class="string">u'问题'</span>,<span class="string">u'北京'</span>,<span class="string">u'美国'</span>,<span class="string">u'领导人'</span>,<span class="string">u'会议'</span>,<span class="string">u'经济'</span>,<span class="string">u'关系'</span>,<span class="string">u'香港'</span>,<span class="string">u'1975'</span>,<span class="string">u'领导'</span>,<span class="string">u'胡耀邦'</span>,<span class="string">u'苏联'</span>,<span class="string">u'政治'</span>,<span class="string">u'支持'</span>,</span><br><span class="line"><span class="string">u'军队'</span>,<span class="string">u'陈云'</span>,<span class="string">u'政策'</span>,<span class="string">u'赵紫阳'</span>,<span class="string">u'周恩'</span>,<span class="string">u'讲话'</span>,<span class="string">u'学生'</span>,<span class="string">u'华国锋'</span>,<span class="string">u'改革'</span>,<span class="string">u'日本'</span>]</span><br><span class="line">index_25 = [<span class="number">0</span>,<span class="number">1</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">6</span>,<span class="number">7</span>,<span class="number">8</span>,<span class="number">11</span>,<span class="number">12</span>,<span class="number">14</span>,<span class="number">16</span>,<span class="number">20</span>,<span class="number">21</span>,<span class="number">23</span>,<span class="number">26</span>,<span class="number">27</span>,<span class="number">28</span>,<span class="number">31</span>,<span class="number">33</span>,<span class="number">37</span>,<span class="number">39</span>,<span class="number">40</span>,<span class="number">42</span>,<span class="number">46</span>,<span class="number">47</span>,<span class="number">48</span>,<span class="number">49</span>]</span><br><span class="line">count = <span class="number">27</span></span><br><span class="line">fig,ax=plt.subplots(<span class="number">1</span>,figsize=(<span class="number">20</span>,<span class="number">10</span>))</span><br><span class="line">ax.bar(range(count),data[index_25,<span class="number">1</span>].astype(<span class="string">'int'</span>),color = <span class="string">'b'</span>)</span><br><span class="line"><span class="comment">#ax.bar(range(count),data[:count,1].astype('int'))</span></span><br><span class="line">ax.set_xticks(range(count))</span><br><span class="line">ax.set_xticklabels(namelist)</span><br><span class="line"><span class="comment">#plt.savefig('tst.png')</span></span><br><span class="line">ax.set_title(str(count)+<span class="string">' key words frequency in book'</span>)</span><br></pre></td></tr></table></figure><p>比如这个显示前二十个关键词的bar plot，可以发现相当有趣的现象，在一本书中的关键词分布竟然也挺像幂率分布，某两三个关键词频次非常高，然后是一堆比较重要的关键词，这个也很有趣。<br><img src="http://i4.bvimg.com/640680/9190c33461d494bd.png" alt="Markdown"></p><h3 id="fluctuation"><a href="#fluctuation" class="headerlink" title="fluctuation"></a>fluctuation</h3><p>接下来我还画了重要词汇再不同章节的变化图。这个的难点是要先获取每一章的起始和结束的位置（不是书本的页码，而是自己分割出来的“单词表”上的位置）<br><figure class="highlight maxima"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">chapterind = <span class="built_in">np</span>.<span class="built_in">array</span>([<span class="number">16590</span>,  <span class="number">31267</span>,  <span class="number">54053</span>,<span class="number">69769</span>,  <span class="number">90171</span>, <span class="number">104745</span>,<span class="number">121010</span>, <span class="number">138136</span>,  <span class="number">147048</span>,  <span class="number">161724</span>,<span class="number">170963</span>,  <span class="number">193593</span>, <span class="number">206502</span>, <span class="number">214129</span>,<span class="number">230193</span>, </span><br><span class="line">                <span class="number">245828</span>,  <span class="number">260400</span>,<span class="number">285768</span>, <span class="number">303284</span>, <span class="number">324922</span>, <span class="number">337101</span>, <span class="number">349241</span>, <span class="number">362426</span>, <span class="number">377184</span>])-<span class="number">1</span></span><br><span class="line">def count_frequent(chap):</span><br><span class="line">    freqlist =[]</span><br><span class="line">    <span class="keyword">if</span> chap &lt;<span class="number">23</span>:</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">27</span>):</span><br><span class="line">            freqlist.<span class="built_in">append</span>(<span class="built_in">np</span>.where(wordlist[chapterind[chap]:chapterind[chap+<span class="number">1</span>]] ==namelist[i])[<span class="number">0</span>].shape[<span class="number">0</span>])</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">27</span>):</span><br><span class="line">            freqlist.<span class="built_in">append</span>(<span class="built_in">np</span>.where(wordlist[chapterind[chap]:] ==namelist[i])[<span class="number">0</span>].shape[<span class="number">0</span>])</span><br><span class="line">    <span class="built_in">return</span> <span class="built_in">np</span>.<span class="built_in">array</span>(freqlist)</span><br><span class="line"></span><br><span class="line">freq_var = <span class="built_in">np</span>.ndarray([<span class="number">24</span>,<span class="number">27</span>])</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">24</span>):</span><br><span class="line">    freq_var[i] = count_frequent(i)</span><br><span class="line"></span><br><span class="line">from sklearn.preprocessing import MinMaxScaler</span><br><span class="line">scaler = MinMaxScaler()</span><br><span class="line">transformed = scaler.fit_transform(freq_var)</span><br><span class="line"></span><br><span class="line">fig,ax=plt.subplots(<span class="number">1</span>,figsize =(<span class="number">10</span>,<span class="number">10</span>))</span><br><span class="line">ax.matshow(transformed.T ,cmap ='jet')</span><br><span class="line">ax.set_title('<span class="number">27</span> <span class="built_in">key</span> words fluctuation <span class="keyword">in</span> <span class="number">24</span> chapters')</span><br><span class="line">ax.set_xticks(<span class="built_in">range</span>(<span class="number">24</span>))</span><br><span class="line">ax.set_yticks(<span class="built_in">range</span>(<span class="number">27</span>))</span><br><span class="line">ax.set_yticklabels(namelist)</span><br></pre></td></tr></table></figure></p><h4 id="heatmap"><a href="#heatmap" class="headerlink" title="heatmap"></a>heatmap</h4><p>经过一番折腾就可以统计出来27个关键词在24章的词频的变化，然后先画了一个<strong>heatmap热力图</strong>，这里为了避免某些关键词，比如邓小平出现频次太多影响到其他关键词的颜色，对每行做了归一化的处理（Minmaxscale）。<br><img src="http://i4.bvimg.com/640680/231ecc794e04d4d7.png" alt="Markdown"></p><p>这个图每一行是一个关键词，每一列是一章。信息量也是蛮大的，比如毛泽东在前面几章出现频次极其的高，后面由于趋势的原因，提的渐渐少了很多，变化相当明显。再比如支持一词，在后面的章节出现很多，可以推理强调邓小平受到他人支持以及支持他人推进改革的次数不少。学生这个关键词在19-21章出现非常多，闭着眼睛也知道这几张在讲什么（政治的潮起潮落、北京之春和天安门事件）。总之用heatmap图的方法也可以粗略地对关键词，尤其是关键词在每章中的变化做一些分析，更加细致的分析可以通过索引回一开始产生的全书词汇找到前后文再仔细看。</p><h4 id="折线图"><a href="#折线图" class="headerlink" title="折线图"></a>折线图</h4><p>接下来又绘制了一个更加直观的折线图，展示不同关键词在不同章节的变化情况，但是由于混杂在一起，可能不如热力图易读。<br><figure class="highlight stata"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">fig,ax=plt.subplots(1,figsize =(20,10))</span><br><span class="line">#ax.<span class="keyword">plot</span>(freq_var[:,:10])</span><br><span class="line"><span class="keyword">count</span> =10</span><br><span class="line"><span class="keyword">for</span> x,y <span class="keyword">in</span> <span class="keyword">zip</span>(freq_var[:,:<span class="keyword">count</span>].T,namelist[:<span class="keyword">count</span>]):</span><br><span class="line">    plt.<span class="keyword">plot</span>(x,<span class="keyword">label</span> =y)</span><br><span class="line">plt.title(str(<span class="keyword">count</span>)+' key words fluctuation <span class="keyword">in</span> 24 chapters')</span><br><span class="line">plt.legend()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure></p><p><img src="http://i4.bvimg.com/640680/c79d691ce875618e.png" alt="Markdown"></p><h2 id="进一步分析"><a href="#进一步分析" class="headerlink" title="进一步分析"></a>进一步分析</h2><h3 id="定义关键词之间的关系"><a href="#定义关键词之间的关系" class="headerlink" title="定义关键词之间的关系"></a>定义关键词之间的关系</h3><p>之前做的是一些基本的分析，我又思考了一下，能不能怎样表示一下两个关键词之间的关系呢？因为时间仓促，我也没有查找资料，就自己定义了某种衡量方法：</p><p>想衡量两个关键词的关系，以邓小平和毛泽东为例，他们分别出现了四千多次和两千多次，分布在全书中的各个位置，我想看他们的关系，就是看他们是否会出现的比较近，或者很多时候没有什么关系。于是我考虑去计算两个关键词的“<strong>最近邻距离</strong>”。接下来就是如何定义这个最近邻距离。因为两个关键词的数量不一致，以个数少的作为基准，已经可以知道这个词语在我生成的词汇表的具体位置，因此我分别找到毛泽东出现的两千多个位置，然后搜索每个位置最近的邓小平这个词汇出现的位置，然后获得他们的距离。这样就可以衡量出两个关键词在每个位置的最近距离了。</p><p>虽然听起来这个过程十分的繁琐，需要大量的搜索，但是通过把循环和搜索问题变成矩阵的运算（反正位置都是数字），就可以非常快地计算出任意两个关键词的距离分布了，我给定义成了<strong>calculate_distance</strong>函数。</p><figure class="highlight nix"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">def calculate_distance(ind1,ind2):</span><br><span class="line">    <span class="attr">pos1</span> = np.where(<span class="attr">wordlist==namelist[ind1])[0]</span></span><br><span class="line">    <span class="attr">pos2</span> = np.where(<span class="attr">wordlist==namelist[ind2])[0]</span></span><br><span class="line">    num1 ,<span class="attr">num2</span> = pos1.shape[<span class="number">0</span>],pos2.shape[<span class="number">0</span>]</span><br><span class="line">    <span class="keyword">if</span> num1&gt;num2:</span><br><span class="line">        <span class="attr">small</span> = num2</span><br><span class="line">        <span class="attr">large</span> = num1</span><br><span class="line">        <span class="attr">lararr</span> = pos1</span><br><span class="line">        <span class="attr">smarr</span> = pos2</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="attr">small</span> = num1</span><br><span class="line">        <span class="attr">large</span> = num2</span><br><span class="line">        <span class="attr">lararr</span> = pos2</span><br><span class="line">        <span class="attr">smarr</span> = pos1</span><br><span class="line">    <span class="attr">disarr</span> = np.ndarray([small,large])  <span class="comment">#each line calculate the small set's ith word's and large set's every words distance</span></span><br><span class="line">    <span class="attr">arr1=</span> np.repeat(smarr,large).reshape(-<span class="number">1</span>,large)</span><br><span class="line">    <span class="attr">arr2=</span> np.repeat(lararr,small).reshape(-<span class="number">1</span>,small).T</span><br><span class="line">    <span class="attr">mindis</span> = np.min(np.abs(arr2-arr1),<span class="attr">axis=1)</span></span><br><span class="line">    return mindis</span><br><span class="line"></span><br><span class="line">def draw_dist_count(ind1,ind2):</span><br><span class="line">    fig,<span class="attr">ax=plt.subplots(1,figsize=(20,10))</span></span><br><span class="line">    ax.bar(range(calculate_distance(<span class="number">0</span>,<span class="number">1</span>).shape[<span class="number">0</span>]),calculate_distance(<span class="number">0</span>,<span class="number">1</span>),<span class="attr">color='g')</span></span><br><span class="line">    ax.set_title('Minimum Distance of '+namelist[ind1]+<span class="string">" and "</span>+namelist[ind2])</span><br><span class="line">draw_dist_count(<span class="number">0</span>,<span class="number">1</span>)</span><br></pre></td></tr></table></figure><h4 id="绘图"><a href="#绘图" class="headerlink" title="绘图"></a>绘图</h4><p>这里就拿邓小平和毛泽东两个关键词举例，我按照顺序画了出来，毛泽东出现的两千多次里，每个毛泽东与最近的一个邓小平的位置距离。值越小说明这两个关键词越靠近，要是值为1的话就说明他们挨着（不过对于名词来说一般中间至少隔着一个介词）。<strong>这样就可以看到任意两个关键词的关系随书的文字的紧张的变化情况。</strong></p><p>可以看到700到1400左右，两个词的距离明显较近，说明在这部分文字中，两人发生了更为密切的联系，而500左右的距离有的非常远，说明这部分是各讲各的故事，两个人还没有交集。</p><p><img src="http://i4.bvimg.com/640680/a421383dc7da2619.png" alt="Markdown"></p><p>同样的调用计算距离和绘图的函数，可以查看任意两个关键词的距离并按顺序绘制其值。</p><p>下面一次性展示了同一个关键词和其他好几个关键词的距离图。</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">fig,ax=plt.subplots(<span class="number">4</span>,<span class="number">2</span>,figsize=(<span class="number">20</span>,<span class="number">20</span>))</span><br><span class="line"><span class="keyword">for</span> <span class="selector-tag">i</span> <span class="keyword">in</span> range(<span class="number">4</span>):</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">2</span>):</span><br><span class="line">        ax[<span class="selector-tag">i</span>,j].bar(range(calculate_distance(<span class="number">0</span>,<span class="number">1</span>+<span class="number">2</span>*i+j)<span class="selector-class">.shape</span>[<span class="number">0</span>]),calculate_distance(<span class="number">0</span>,<span class="number">1</span>+<span class="number">2</span>*i+j))</span><br><span class="line">        ax[<span class="selector-tag">i</span>,j].set_title(<span class="string">'Minimum Distance of '</span>+namelist[<span class="number">0</span>]+<span class="string">" and "</span>+namelist[<span class="number">1</span>+<span class="number">2</span>*i+j])</span><br></pre></td></tr></table></figure><p><img src="http://i4.bvimg.com/640680/4414d03a5b228e77.png" alt="Markdown"></p><h4 id="hist-plot"><a href="#hist-plot" class="headerlink" title="hist plot"></a>hist plot</h4><p>接下来还画了一下距离的<strong>分布图</strong>，就是把上面的图中的距离统计一下他们的分布。</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">fig,ax=plt.subplots(<span class="number">4</span>,<span class="number">2</span>,figsize=(<span class="number">20</span>,<span class="number">20</span>))</span><br><span class="line"><span class="keyword">for</span> <span class="selector-tag">i</span> <span class="keyword">in</span> range(<span class="number">4</span>):</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">2</span>):</span><br><span class="line">        ax[<span class="selector-tag">i</span>,j].hist(calculate_distance(<span class="number">0</span>,<span class="number">1</span>+<span class="number">2</span>*i+j),bins =<span class="number">50</span>,<span class="attribute">color</span>=<span class="string">'b'</span>,alpha=<span class="number">0.4</span>)</span><br><span class="line">        ax[<span class="selector-tag">i</span>,j].set_title(<span class="string">'Minimum Distance of '</span>+namelist[<span class="number">0</span>]+<span class="string">" and "</span>+namelist[<span class="number">1</span>+<span class="number">2</span>*i+j])</span><br></pre></td></tr></table></figure><p><img src="http://i4.bvimg.com/640680/3c6048942654bc2c.png" alt="Markdown"></p><p>这种图感觉就丢失很多信息了，看不出来随着书籍的发展，两个名词的关系的变化。当然如果做得更细致，可以用某些指标刻画一下这种距离图，更好地衡量两个指标的关系，用可视化的方法当然是更直观的。</p><h4 id="boxplot"><a href="#boxplot" class="headerlink" title="boxplot"></a>boxplot</h4><p>最后是<strong>Boxplot</strong>，这是另一种直观显示距离分布的图。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">dist_data = &#123;&#125;</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> np.arange(<span class="number">1</span>,<span class="number">20</span>):</span><br><span class="line">    dist_data[i] = calculate_distance(<span class="number">0</span>,i)</span><br><span class="line">dataframe_dxp = pd.concat((pd.DataFrame(&#123;namelist[i]:dist_data[i]&#125;) <span class="keyword">for</span> i <span class="keyword">in</span> np.arange(<span class="number">1</span>,<span class="number">20</span>)),axis=<span class="number">1</span>)</span><br><span class="line"><span class="keyword">import</span> seaborn <span class="keyword">as</span> sns</span><br><span class="line">fig, ax = plt.subplots(figsize=(<span class="number">100</span>,<span class="number">20</span>))</span><br><span class="line">sns.boxplot(data =dataframe_dxp,ax=ax,boxprops=dict(alpha=<span class="number">.5</span>),color=<span class="string">'g'</span>)</span><br><span class="line">ax.set_title(<span class="string">u'Dengxiaoping and others'</span>,fontsize=<span class="number">80</span>)</span><br><span class="line">ax.set_xticks(range(<span class="number">19</span>))</span><br><span class="line">ax.set_xticklabels(namelist[<span class="number">1</span>:<span class="number">20</span>],fontsize=<span class="number">80</span>)</span><br><span class="line">fig.savefig(<span class="string">'boxplot.png'</span>)</span><br></pre></td></tr></table></figure></p><p><img src="http://i4.bvimg.com/640680/5ef0af735a848eca.png" alt="Markdown"></p><p>可以看到邓小平和好几个关键词的距离的分布，每一个box就是一个分布的统计，当然也和分布图一样，这样一画就<strong>拉平了</strong>不同关键词之间的差异了。</p><p>其实对文本挖掘还有<strong>词性标注、情感分析</strong>等更多方法，包括归纳段落或篇章的主题等等，目前都有很多统计模型和机器学习方法可以做。不过在尝试的过程中，我还是感觉到这只是很基本的辅助方法，更重要的还在于人文历史政治学科的专家们对书籍做仔细的解读，<strong>挖掘历史细节中的关键信息是人最擅长的</strong>，比机器强大的多的地方，不过在卷帙浩繁的历史典籍中，面对成千上万的书籍时，快速挖掘书籍的要点，分析出来一些有趣的东西，也许机器能够帮上大忙。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;期中作业之一是写一篇邓小平时代的读后感，说实话这种书是实在没空读了，虽然粗略地翻了几章，十分吸引人，但是这两年真的越来越讨厌写文科式的论文，瞎胡诌凑字数曾经也是我作为理科生的优势，但是这一两年对这种风格的文章：东拼西凑，无病呻吟，迷茫又自负的写作非常地厌恶。因为就想玩点花样，做点简单的文本数据挖掘凑凑字数，虽然多花了很多时间，但是毕竟很有意思，有意思的事情就不算浪费时间对吧，没有意思的事情，哪怕一分钟也是对生命的浪费呢。&lt;br&gt;
    
    </summary>
    
      <category term="techniques" scheme="http://james20141606.github.io/categories/techniques/"/>
    
      <category term="data science" scheme="http://james20141606.github.io/categories/techniques/data-science/"/>
    
    
      <category term="codes" scheme="http://james20141606.github.io/tags/codes/"/>
    
      <category term="statistics" scheme="http://james20141606.github.io/tags/statistics/"/>
    
      <category term="data mining" scheme="http://james20141606.github.io/tags/data-mining/"/>
    
      <category term="matplotlib" scheme="http://james20141606.github.io/tags/matplotlib/"/>
    
  </entry>
  
  <entry>
    <title>Extract Countries&#39; Commercial Data</title>
    <link href="http://james20141606.github.io/2018/04/12/economics/"/>
    <id>http://james20141606.github.io/2018/04/12/economics/</id>
    <published>2018-04-12T15:49:57.000Z</published>
    <updated>2018-04-12T16:10:43.156Z</updated>
    
    <content type="html"><![CDATA[<p>It is a brief pipeline to extract data from datasets in <a href="http://139.129.209.66:8000/d/daedafb854/" target="_blank" rel="noopener">here</a></p><p>The work is from my cute girl friend, who know nothing about code but brag to her mentor she can do it.</p><p>In this work, I use R, Bash and Python to extract different countries different indicators in different years. The data have some property: big, not unified(.RData or .csv), some have mistakes. It is very sparse so it waste many storage. And the conversion of Rdata to csv leads some mistakes, so it needs very careful examination and check work. At first I want to store all of them in HDF5 for better IO, but people in my girl friend’s working team are’t familiar with codes, so I store them in csv. I also think about later work(for example, basic statistical work, model the interaction and time series, maybe a hierarchical time series machine learning model), but I am too busy to help my little bragger to do all kinds of things.<br><a id="more"></a><br>Here are codes I wrote to extract and organize data: <a href="https://github.com/james20141606/economics" target="_blank" rel="noopener">https://github.com/james20141606/economics</a></p><h4 id="wget-to-extract-data"><a href="#wget-to-extract-data" class="headerlink" title="wget to extract data"></a>wget to extract data</h4><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="selector-tag">getdata</span><span class="selector-class">.sh</span></span><br></pre></td></tr></table></figure><h4 id="check-row-and-col-names-for-further-extraction"><a href="#check-row-and-col-names-for-further-extraction" class="headerlink" title="check row and col names for further extraction"></a>check row and col names for further extraction</h4><h5 id="before-2001"><a href="#before-2001" class="headerlink" title="before 2001"></a>before 2001</h5><p>use awk to read row and columns in csv, then compare them with std names<br><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">#first</span><br><span class="line">extractnames.sh</span><br><span class="line">#second</span><br><span class="line">refer to codes <span class="keyword">in</span> analyze_dim_name<span class="selector-class">.ipynb</span>:检查<span class="number">1995</span>-<span class="number">2000</span>年的行和列名</span><br><span class="line"><span class="selector-id">#run</span> and check</span><br></pre></td></tr></table></figure></p><p><strong>There are some wrong files, need to examine them later</strong></p><h5 id="after-2001"><a href="#after-2001" class="headerlink" title="after 2001"></a>after 2001</h5><p>Use R to extract row and columns and compare<br><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="selector-id">#first</span> run Rscript to get row and <span class="attribute">columns</span></span><br><span class="line">run extractrowandcol.R</span><br><span class="line">#second</span><br><span class="line">refer to codes <span class="keyword">in</span> analyze_dim_name<span class="selector-class">.ipynb</span>:<span class="number">2001</span>以后的，用R脚本转出来csv，同样的方式读取并判断</span><br><span class="line"><span class="selector-id">#run</span> and check</span><br></pre></td></tr></table></figure></p><p><strong>All the files have exactly the same structure</strong></p><h2 id="extract-data-concerning-CHINA"><a href="#extract-data-concerning-CHINA" class="headerlink" title="extract data concerning CHINA"></a>extract data concerning CHINA</h2><h3 id="analysis"><a href="#analysis" class="headerlink" title="analysis"></a>analysis</h3><p><strong>The data dimension is: </strong> 1435<em>1435</em>41<br>If use RData to extract some matrix to analyze its row and column names,  the  automatically saved names have mistakes. So we use the previous plot to inspire us and find the true data structure:</p><p><img src="http://i1.bvimg.com/640680/0ba5f17f200bc207.png" alt="Markdown"></p><p>The first big block is <strong>to</strong> the first country(AUS)</p><p>So the column names in first big block are:   X&gt;AUS</p><p>When row and columns names match there are values, so there are only values in diagonal. </p><p><strong>(That’s the main reason the R data file is big: the minimum number is :41<em>41</em>35<em>35, but RData have 1435</em>1435*41, 41 fold redundancy)</strong></p><h3 id="How-to-find-a-country"><a href="#How-to-find-a-country" class="headerlink" title="How to find a country"></a>How to find a country</h3><h4 id="after-2001-1"><a href="#after-2001-1" class="headerlink" title="after 2001"></a>after 2001</h4><p>The data we use is RData, use Rscript to extract data</p><h5 id="to-CHINA"><a href="#to-CHINA" class="headerlink" title="to CHINA"></a>to CHINA</h5><p>Locate the country，<strong>CHN is 7th</strong></p><p><strong>The seventh block are all countries to CHINA</strong><br><strong>dat[,,7]</strong>  the matrix dimension is 1435*1435<br><figure class="highlight subunit"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">datt&lt;-dat[,,7]</span><br><span class="line">datt[1<span class="string">+35</span>*(i<span class="string">-1</span>):35*i,1<span class="string">+35</span>*(i<span class="string">-1</span>):35*i]</span><br></pre></td></tr></table></figure></p><p>Use for loop, <strong>use a array:35<em>（35</em>41）</strong> to store<br>save to <strong>tochn.csv</strong><br>Rows:c1-c35<br>Columns: every 35 columns are a same country<br><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Run extract2001.R</span><br><span class="line"><span class="selector-id">#output</span> <span class="keyword">in</span> out directory</span><br></pre></td></tr></table></figure></p><h5 id="CHINA-to"><a href="#CHINA-to" class="headerlink" title="CHINA to"></a>CHINA to</h5><p><strong>each 1435*1435 block’s seventh mini block is China to another country</strong><br><strong>dat[211:245,211:245,i]</strong><br><figure class="highlight dockerfile"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">Run</span><span class="bash"> extract2001.R</span></span><br></pre></td></tr></table></figure></p><p>Use for loop, <strong>use a array:35<em>（35</em>41）</strong>to store<br><strong>save to chnto.csv</strong></p><p>Rows:c1-c35</p><p>Columns: every 35 columns are a same country</p><h4 id="before-2001-1"><a href="#before-2001-1" class="headerlink" title="before 2001"></a>before 2001</h4><p>There are some exceptions we do not deal with at first</p><p>The data we use only has csv, so use python to extract</p><h5 id="to-CHINA-1"><a href="#to-CHINA-1" class="headerlink" title="to CHINA"></a>to CHINA</h5><p>The principle is similar to after 2001</p><p>But the data is just the transpose: (1435<em>41)</em>1435</p><p><strong>The seventh block are all countries to CHINA</strong></p><p><strong>Run extract1995.py</strong><br><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">datt = dat[<span class="number">1435</span>*<span class="number">6</span>:<span class="number">1435</span>*<span class="number">7</span>,:]   <span class="number">#143</span>5*<span class="number">1435</span>datt[<span class="number">35</span>*<span class="selector-tag">i</span>:<span class="number">35</span>*(i+<span class="number">1</span>),<span class="number">35</span>*<span class="selector-tag">i</span>:<span class="number">35</span>*(i+<span class="number">1</span>)]  #loop</span><br></pre></td></tr></table></figure></p><p>Use for loop, <strong>use a array:35<em>（35</em>41）</strong>to store<br><strong>save to chnto.csv</strong></p><h5 id="CHINA-to-1"><a href="#CHINA-to-1" class="headerlink" title="CHINA to"></a>CHINA to</h5><p><strong>each 1435*1435 block’ seventh mini block is China to another country</strong><br><figure class="highlight makefile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">dattt =dat[1435*i:1435*(i+1),:]  <span class="comment">#the ith country 1435*1435</span></span><br><span class="line"><span class="section">dattt[35*6:35*(6+1),35*6:35*(6+1)]</span></span><br></pre></td></tr></table></figure></p><p>Save to <strong>chnto.csv</strong></p><h2 id="To-do"><a href="#To-do" class="headerlink" title="To do"></a>To do</h2><h3 id="Exception-dealing"><a href="#Exception-dealing" class="headerlink" title="Exception dealing"></a>Exception dealing</h3><ul><li>[ ] before 2001 there are some files not in standard form, needs more examine to extract. Maybe case by case<h3 id="analyze-data"><a href="#analyze-data" class="headerlink" title="analyze data"></a>analyze data</h3><h4 id="plot-the-change"><a href="#plot-the-change" class="headerlink" title="plot the change,"></a>plot the change,</h4>for example, heat map, line chart, animation …</li></ul><p><img src="http://i1.bvimg.com/640680/0ce616088c435eae.gif" alt="Markdown"></p><h4 id="do-simple-statistics"><a href="#do-simple-statistics" class="headerlink" title="do simple statistics"></a>do simple statistics</h4><h4 id="model-the-change"><a href="#model-the-change" class="headerlink" title="model the change"></a>model the change</h4>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;It is a brief pipeline to extract data from datasets in &lt;a href=&quot;http://139.129.209.66:8000/d/daedafb854/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;here&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;The work is from my cute girl friend, who know nothing about code but brag to her mentor she can do it.&lt;/p&gt;
&lt;p&gt;In this work, I use R, Bash and Python to extract different countries different indicators in different years. The data have some property: big, not unified(.RData or .csv), some have mistakes. It is very sparse so it waste many storage. And the conversion of Rdata to csv leads some mistakes, so it needs very careful examination and check work. At first I want to store all of them in HDF5 for better IO, but people in my girl friend’s working team are’t familiar with codes, so I store them in csv. I also think about later work(for example, basic statistical work, model the interaction and time series, maybe a hierarchical time series machine learning model), but I am too busy to help my little bragger to do all kinds of things.&lt;br&gt;
    
    </summary>
    
      <category term="interestings" scheme="http://james20141606.github.io/categories/interestings/"/>
    
    
      <category term="codes" scheme="http://james20141606.github.io/tags/codes/"/>
    
      <category term="statistics" scheme="http://james20141606.github.io/tags/statistics/"/>
    
      <category term="economics" scheme="http://james20141606.github.io/tags/economics/"/>
    
      <category term="girl friend" scheme="http://james20141606.github.io/tags/girl-friend/"/>
    
  </entry>
  
  <entry>
    <title>Setup and Linux</title>
    <link href="http://james20141606.github.io/2018/04/12/setup/"/>
    <id>http://james20141606.github.io/2018/04/12/setup/</id>
    <published>2018-04-12T15:39:17.000Z</published>
    <updated>2018-04-14T17:20:41.471Z</updated>
    
    <content type="html"><![CDATA[<p>分享一点setup和linux的东西，包括使用git，使用支持markdown的笔记软件Bear，Anaconda的一些使用技巧以及jupyter在服务器上的设置。也可以在<a href="https://legacy.gitbook.com/book/lulab/bioinfo-training-2018/details" target="_blank" rel="noopener">这里</a>找到更多分享<br><a id="more"></a></p><h1 id="Setup"><a href="#Setup" class="headerlink" title="Setup"></a>Setup</h1><h2 id="版本控制与GitHub管理"><a href="#版本控制与GitHub管理" class="headerlink" title="版本控制与GitHub管理"></a>版本控制与GitHub管理</h2><h3 id="Git简介"><a href="#Git简介" class="headerlink" title="Git简介"></a>Git简介</h3><h4 id="Git是目前世界上最先进的分布式版本控制系统。"><a href="#Git是目前世界上最先进的分布式版本控制系统。" class="headerlink" title="Git是目前世界上最先进的分布式版本控制系统。"></a>Git是目前世界上最先进的分布式版本控制系统。</h4><h5 id="没有版本控制系统会遇到什么困难："><a href="#没有版本控制系统会遇到什么困难：" class="headerlink" title="没有版本控制系统会遇到什么困难："></a>没有版本控制系统会遇到什么困难：</h5><ul><li>版本更新的困难：如果你用Microsoft Word写过长篇大论，那你一定有这样的经历：想删除一个段落，又怕将来想恢复找不回来怎么办？于是只好先把当前文件“另存为”一个新的Word文件，再接着改，改到一定程度，再“另存为”一个新文件，这样一直改下去，最后你的Word文档可能会有几十个不同版本的备份。过了一周，你想找回被删除的文字，但是已经记不清删除前保存在哪个文件里了，只好一个一个文件回去找，非常麻烦。如果是代码的话，来回的更改就更频繁了，如果想找到之前某个版本的代码，很有可能已经被删除了，对于稍微大一点的工程来说可能麻烦就大了。</li><li>合作时的困难：有些部分需要你的合作者帮助写，于是你把文件Copy到U盘里给她（也可能通过Email发送一份给她），然后，你继续修改文件。一段时间后你的合作者把改动后的文件给你，此时，文件的合并就是一件麻烦事了，你要不然得问她一个一个指出她的改动，或者你就要记录自己的改动，和她的文件合并。<br><br></li></ul><p>如果有一个软件，不但能自动帮我记录每次文件的改动，还可以让同事协作编辑，这样就不用自己管理一堆类似的文件了，也不需要把文件传来传去。如果想查看某次改动，只需要在软件里看一眼就可以看到改动的日期和内容，岂不是很方便？</p><h5 id="这就是21世纪的版本控制系统，Git。"><a href="#这就是21世纪的版本控制系统，Git。" class="headerlink" title="这就是21世纪的版本控制系统，Git。"></a>这就是21世纪的版本控制系统，Git。</h5><h4 id="Git诞生"><a href="#Git诞生" class="headerlink" title="Git诞生"></a>Git诞生</h4><p>Git是Linus (Linux之父)花了两周时间用C写的，在2002年以前，世界各地的志愿者把源代码文件通过diff的方式发给Linus，然后由Linus本人通过手工方式合并代码，Linux反对集中式的，需要联网的版本控制系统，也反对商业版的版本控制系统，于是创造了Git，一个月之内，Linux系统的源码已经由Git管理了。<br>Git迅速成为最流行的分布式版本控制系统，尤其是2008年，GitHub网站上线了，它为开源项目免费提供Git存储，无数开源项目开始迁移至GitHub，这就是程序员最爱的Git和Github的诞生史。</p><h3 id="安装与使用Git"><a href="#安装与使用Git" class="headerlink" title="安装与使用Git"></a>安装与使用Git</h3><h4 id="安装git"><a href="#安装git" class="headerlink" title="安装git"></a>安装git</h4><p>只介绍Mac OS系统安装方法</p><ul><li><p>方法一：先安装homebrew，然后通过homebrew安装Git。安装homebrew可查看<a href="http://brew.sh/" target="_blank" rel="noopener">http://brew.sh/</a></p><figure class="highlight mipsasm"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">brew </span><span class="keyword">install </span>git</span><br></pre></td></tr></table></figure></li><li><p>方法二：<br>第二种方法更简单，也是推荐的方法，就是用Xcode，Xcode集成了Git，不过默认没有安装，在终端输入命令安装command line tools，即可安装git。</p><figure class="highlight ada"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">xcode-<span class="keyword">select</span> <span class="comment">--install</span></span><br></pre></td></tr></table></figure></li></ul><h4 id="使用git"><a href="#使用git" class="headerlink" title="使用git"></a>使用git</h4><h5 id="创建或使用文件夹作为需要管理的仓库"><a href="#创建或使用文件夹作为需要管理的仓库" class="headerlink" title="创建或使用文件夹作为需要管理的仓库"></a>创建或使用文件夹作为需要管理的仓库</h5><p>在本地建立项目文件夹，或者使用已存在的项目文件夹，如helloworld<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> helloworld</span><br><span class="line">git init <span class="comment">#通过git init命令把这个目录变成Git可以管理的仓库</span></span><br></pre></td></tr></table></figure></p><h5 id="添加或更改文件"><a href="#添加或更改文件" class="headerlink" title="添加或更改文件"></a>添加或更改文件</h5><figure class="highlight dockerfile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">vi README.md <span class="comment">#创建一个新文件README.md，添加内容并保存</span></span><br><span class="line">git <span class="keyword">add</span><span class="bash"> README.md</span></span><br><span class="line"><span class="bash"><span class="comment">#用命令git add告诉Git，把文件README.md添加到仓库</span></span></span><br><span class="line"><span class="bash"><span class="comment">#如果一次性添加了多个文件，可以使用git add . git会自己判别哪些是新文件。</span></span></span><br></pre></td></tr></table></figure><p>所有的版本控制系统只能跟踪文本文件的改动，比如TXT文件，网页，所有的程序代码等等，Git可以告诉你每次的改动，比如在第5行加了一个单词“Linux”，在第8行删了一个单词“Windows”。而图片、视频这些二进制文件，只知道大小的改动，但更改的内容版本控制系统无法知道。</p><h5 id="添加更改信息"><a href="#添加更改信息" class="headerlink" title="添加更改信息"></a>添加更改信息</h5><p>下面可以告诉git你本次更改的内容，如果一次add了多个文件，则所有的文件都会被标注同样的更改信息。比如：<br><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">git <span class="keyword">commit</span> -m <span class="string">"first commit"</span></span><br><span class="line">git <span class="keyword">commit</span> -m <span class="string">"add README.md"</span></span><br></pre></td></tr></table></figure></p><h5 id="上传至GitHub"><a href="#上传至GitHub" class="headerlink" title="上传至GitHub"></a>上传至GitHub</h5><p>首先在github上新建一个repository，如helloworld，你将会看到跳转页面上提示你需要推送到的HTTPS地址<a href="https://github.com/accountname/repositoryname.git" target="_blank" rel="noopener">https://github.com/accountname/repositoryname.git</a><br>接下来使用<br><figure class="highlight dockerfile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">git remote <span class="keyword">add</span><span class="bash"> origin https://github.com/accountname/repositoryname.git</span></span><br><span class="line"><span class="bash">git push -u origin master</span></span><br></pre></td></tr></table></figure></p><p>即可把自己的本地仓库推送到github上，速度很快。<br>注意如果第一次把远程地址输入错误，可以用以下命令更正地址<br><figure class="highlight dsconfig"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">git </span><span class="string">remote </span><span class="built_in">set-url</span> <span class="string">origin </span><span class="string">https:</span>//<span class="string">github.</span><span class="string">com/</span><span class="string">accountname/</span><span class="string">repositoryname.</span><span class="string">git</span></span><br></pre></td></tr></table></figure></p><h4 id="使用ssh-key-免账户与密码推送方法："><a href="#使用ssh-key-免账户与密码推送方法：" class="headerlink" title="使用ssh key 免账户与密码推送方法："></a>使用ssh key 免账户与密码推送方法：</h4><h5 id="在终端输入"><a href="#在终端输入" class="headerlink" title="在终端输入"></a>在终端输入</h5><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">git<span class="built_in"> config </span>--global user.name <span class="string">"yourgithubname"</span></span><br><span class="line">git<span class="built_in"> config </span>--global user.email <span class="string">"yourgithubaccountmail"</span></span><br></pre></td></tr></table></figure><h5 id="生成ssh-key"><a href="#生成ssh-key" class="headerlink" title="生成ssh key"></a>生成ssh key</h5><figure class="highlight ebnf"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="attribute">ssh-keygen</span></span><br></pre></td></tr></table></figure><p>生成的密钥在~/.ssh/id_rsa.pub位置。</p><h5 id="配置git-的ssh-key"><a href="#配置git-的ssh-key" class="headerlink" title="配置git 的ssh key"></a>配置git 的ssh key</h5><ul><li>登录github 点击头像选择settings</li><li>选择左侧菜单SSH and GPG keys ；点击右上角的NEW SSH key</li><li>新建ssh 链接。</li><li>title 可随意填写</li><li>Key 将上一步生成的 id_rsa.pub文件 的内容全部复制到此处</li></ul><p>参考链接：<br><a href="https://www.liaoxuefeng.com/wiki/0013739516305929606dd18361248578c67b8067c8c017b000" target="_blank" rel="noopener">Git教程</a><br><a href="http://blog.csdn.net/u012373815/article/details/53575362" target="_blank" rel="noopener">SSH连接GitHub、GitHub配置ssh key</a><br><a href="https://peerj.com/preprints/3159/" target="_blank" rel="noopener">version control</a></p><h2 id="支持markdown的轻量笔记软件-Bear"><a href="#支持markdown的轻量笔记软件-Bear" class="headerlink" title="支持markdown的轻量笔记软件 Bear"></a>支持markdown的轻量笔记软件 Bear</h2><p>推荐一款Mac下的非常好用额轻量级笔记软件Bear</p><h5 id="它的优点包括："><a href="#它的优点包括：" class="headerlink" title="它的优点包括："></a>它的优点包括：</h5><ul><li>轻量级，非常顺滑，无任何延迟</li><li>快捷键/markdown支持，符合程序员思维</li><li>加粗，下划线，项目列举，待办方块，代码块，多级标题，均有键盘快捷键以及markdown格式下的快捷键</li><li>网页链接、文件可拖拽至笔记，并显示内容概要。</li><li>内容可无缝衔接至gitbook等支持markdown格式的场合。（比如这些tips都可以直接在Bear编辑好，复制粘贴来就可以。）</li><li>可以快速通过# 加入标签，对笔记进行分类</li></ul><h1 id="Linux"><a href="#Linux" class="headerlink" title="Linux"></a>Linux</h1><h5 id="Edited-by-19’-Under-Xupeng-Chen"><a href="#Edited-by-19’-Under-Xupeng-Chen" class="headerlink" title="Edited by 19’ Under Xupeng Chen"></a>Edited by 19’ Under Xupeng Chen</h5><h2 id="Conda-amp-Bioconda"><a href="#Conda-amp-Bioconda" class="headerlink" title="Conda &amp; Bioconda"></a>Conda &amp; Bioconda</h2><p>Conda是一个包管理软件，可以帮助方便地下载各种软件而不需要编译。尤其是Bioconda可以用来管理linux系统上的生信相关的软件，是解决安装权限不够的问题的好工具。</p><h3 id="Conda"><a href="#Conda" class="headerlink" title="Conda"></a>Conda</h3><p>conda是一个包，依赖和环境管理工具，适用于多种语言，如: Python, R, Scala, Java, Javascript, C/ C++, FORTRAN</p><h4 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h4><p>Anaconda安装可以去官方下载，但是强烈推荐使用tuna镜像，免流量，而且速度极快。<br><a href="https://mirrors.tuna.tsinghua.edu.cn/anaconda/archive/" target="_blank" rel="noopener">下载地址</a>，下载.sh文件后运行，按照提示一步一步往下运行即可。<br>下载Anaconda后，很多python的常用库都会被自动安装好，另外建议运行以下命令</p><figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">conda config --<span class="built_in">add</span> channels http<span class="variable">s:</span>//mirrors.tuna.tsinghua.edu.<span class="keyword">cn</span>/anaconda/pkgs/free/</span><br><span class="line">conda config --<span class="built_in">add</span> channels http<span class="variable">s:</span>//mirrors.tuna.tsinghua.edu.<span class="keyword">cn</span>/anaconda/pkgs/main/</span><br><span class="line">conda config --<span class="keyword">set</span> show_channel_urls yes</span><br></pre></td></tr></table></figure><p>这样以后使用conda install packages命令下载需要的包的时候，会自动从tuna镜像下载，速度会非常快。</p><h3 id="Bioconda"><a href="#Bioconda" class="headerlink" title="Bioconda"></a>Bioconda</h3><p>Bioconda是conda上一个分发生物信息软件的频道，使用它的最大好处是，你不用自己编译软件了。<br>Conda tuna 安装 conda设置 从tuna下载免流量，快<br>目前Bioconda有超过130个添加、更新和维护生物信息软件的贡献者，他们为这个频道发布了1500多个软件包。总结起来，bioconda有以下几个特点：</p><ul><li>软件是编译好的，无需自己编译</li><li>跨平台，支持Linux和Mac OS（本身conda还支持Windows）</li><li>支持多种语言，Python/Perl/R/Java/Go等</li><li>兼容多种语言的包管理器，如pip，CRAN，CPAN，Bioconductor，apt-get以及 homebrew<br>针对Python来说，使用conda相比pip的很大优势，就是不用自己编译。安装软件最头疼的问题，就是解决编译报错，很多时候忙活一天就为了把一个软件装好。</li></ul><h4 id="使用方法"><a href="#使用方法" class="headerlink" title="使用方法"></a>使用方法</h4><p>先添加Bioconda频道</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">conda<span class="built_in"> config </span>--<span class="builtin-name">add</span> channels defaults</span><br><span class="line">conda<span class="built_in"> config </span>--<span class="builtin-name">add</span> channels conda-forge</span><br><span class="line">conda<span class="built_in"> config </span>--<span class="builtin-name">add</span> channels bioconda</span><br></pre></td></tr></table></figure><p>然后即可用conda安装各种需要的软件，可以先去bioconda channel看看自己需要的软件在不在列表内。</p><figure class="highlight mipsasm"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">conda <span class="keyword">install </span><span class="keyword">bowtie</span></span><br><span class="line"><span class="keyword">conda </span>create -n myenv <span class="keyword">bwa </span><span class="keyword">bowtie </span>hisat star <span class="comment">#a new environment can be created</span></span><br><span class="line">source activate myenv <span class="comment">#activate the environment</span></span><br></pre></td></tr></table></figure><p>参考资料：<br><a href="https://bioconda.github.io/" target="_blank" rel="noopener">Using Bioconda — Bioconda documentation</a><br><a href="https://bioconda.github.io/recipes.html" target="_blank" rel="noopener">packages list</a></p><h2 id="在服务器上运行jupyter-notebook并在本地浏览器使用"><a href="#在服务器上运行jupyter-notebook并在本地浏览器使用" class="headerlink" title="在服务器上运行jupyter notebook并在本地浏览器使用"></a>在服务器上运行jupyter notebook并在本地浏览器使用</h2><p>Jupyter Notebook是基于网页的用于交互计算的应用程序。其可被应用于全过程计算：开发、文档编写（markdown）、运行代码和展示结果。</p><ul><li><p>jupyter适合课题的早期尝试、绘图等非常便利，代码重复运行和复制粘贴方便，方便反复调试，尤其适合尚未工程化，需要大量尝试的阶段。</p></li><li><p>jupyter非常适合教学，交互效果非常好，github上有大量的教学项目是用jupyter notebook展示的，方便查看结果，查看相关说明、公式，方便学习者进行反复实验。</p></li></ul><h4 id="本地设置服务器信息"><a href="#本地设置服务器信息" class="headerlink" title="本地设置服务器信息"></a>本地设置服务器信息</h4><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">vi .ssh/config</span><br><span class="line">Host ibme</span><br><span class="line">HostName 166.111.152.116 #ibme的ip</span><br><span class="line">ControlPersist <span class="literal">yes</span></span><br><span class="line">ControlMaster auto</span><br><span class="line">User chenxupeng</span><br><span class="line">DynamicForward 127.0.0.1:32987 #最后的port（如32987）要自己设置，不能与他人冲突</span><br></pre></td></tr></table></figure><h4 id="使用SwitchOmega在本地浏览器设置代理"><a href="#使用SwitchOmega在本地浏览器设置代理" class="headerlink" title="使用SwitchOmega在本地浏览器设置代理"></a>使用SwitchOmega在本地浏览器设置代理</h4><h5 id="添加情景模式，如ibme"><a href="#添加情景模式，如ibme" class="headerlink" title="添加情景模式，如ibme"></a>添加情景模式，如ibme</h5><p>代理协议SOCKS5，代理服务器127.0.0.1，代理端口填写自己设置的port。</p><h5 id="在auto-switch页面添加规则"><a href="#在auto-switch页面添加规则" class="headerlink" title="在auto switch页面添加规则"></a>在auto switch页面添加规则</h5><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">172<span class="selector-class">.235</span><span class="selector-class">.0</span>.*，192<span class="selector-class">.235</span><span class="selector-class">.0</span>.*，<span class="selector-tag">node50</span>*等，情景模式选择<span class="selector-tag">ibme</span></span><br></pre></td></tr></table></figure><p>点击应用选项</p><h5 id="在服务器上设置start-jupyter文件"><a href="#在服务器上设置start-jupyter文件" class="headerlink" title="在服务器上设置start-jupyter文件"></a>在服务器上设置start-jupyter文件</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">vi ~/bin/start-jupyter</span><br><span class="line">填写：</span><br><span class="line"><span class="meta">#! /bin/bash</span></span><br><span class="line">bsub &lt;&lt;EOF</span><br><span class="line"><span class="comment">#BSUB -J jupyter</span></span><br><span class="line"><span class="comment">#BSUB -R span[hosts=1]</span></span><br><span class="line"><span class="comment">#BSUB -q Z-LU</span></span><br><span class="line"><span class="built_in">cd</span></span><br><span class="line">jupyter notebook --no-browser --ip=0.0.0.0 --port=10087 <span class="comment">#port自己设置一个，不要冲突</span></span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><p>(也可以使用#BSUB -q Z-BNODE)</p><h4 id="start-jupyter"><a href="#start-jupyter" class="headerlink" title="start jupyter"></a>start jupyter</h4><p>首先start-jupyter启动，会自动提交一个任务到某个节点</p><h5 id="使用节点名称连接"><a href="#使用节点名称连接" class="headerlink" title="使用节点名称连接"></a>使用节点名称连接</h5><p>接下来可以用bjobs看到jupyter被提交到了哪个节点。接下来打开本地浏览器，输入</p><figure class="highlight elixir"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">node5<span class="number">0</span>*<span class="symbol">:port</span> <span class="comment">#如node504/10087</span></span><br></pre></td></tr></table></figure><p>若使用Z-BNODE，可在浏览器填写</p><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="selector-tag">zbnode01</span><span class="selector-class">.cluster</span><span class="selector-class">.com</span><span class="selector-pseudo">:port</span></span><br></pre></td></tr></table></figure><h5 id="使用ip连接"><a href="#使用ip连接" class="headerlink" title="使用ip连接"></a>使用ip连接</h5><p>用nslookup获得节点的ip，在本地浏览器输入：</p><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="selector-tag">ip</span><span class="selector-pseudo">:port</span> #如192<span class="selector-class">.235</span><span class="selector-class">.5</span><span class="selector-class">.48</span><span class="selector-pseudo">:10087</span></span><br><span class="line">#获得<span class="selector-tag">ip</span>方法</span><br><span class="line"><span class="selector-tag">nslookup</span> <span class="selector-tag">node504</span><span class="selector-class">.cluster</span><span class="selector-class">.com</span></span><br><span class="line"><span class="selector-tag">nslookup</span> <span class="selector-tag">zbnode01</span><span class="selector-class">.cluster</span><span class="selector-class">.com</span></span><br></pre></td></tr></table></figure><p>第一次登陆需要密码，用bpeek查看任务输出，即可看到token，复制至浏览器即可使用jupyter notebook进行编程了。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;分享一点setup和linux的东西，包括使用git，使用支持markdown的笔记软件Bear，Anaconda的一些使用技巧以及jupyter在服务器上的设置。也可以在&lt;a href=&quot;https://legacy.gitbook.com/book/lulab/bioinfo-training-2018/details&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;这里&lt;/a&gt;找到更多分享&lt;br&gt;
    
    </summary>
    
      <category term="techniques" scheme="http://james20141606.github.io/categories/techniques/"/>
    
      <category term="linux" scheme="http://james20141606.github.io/categories/techniques/linux/"/>
    
    
      <category term="techniques" scheme="http://james20141606.github.io/tags/techniques/"/>
    
      <category term="bioinformatics" scheme="http://james20141606.github.io/tags/bioinformatics/"/>
    
  </entry>
  
  <entry>
    <title>eMaize_Tutorial</title>
    <link href="http://james20141606.github.io/2018/04/12/emaize-tutorial/"/>
    <id>http://james20141606.github.io/2018/04/12/emaize-tutorial/</id>
    <published>2018-04-12T15:30:17.000Z</published>
    <updated>2018-04-14T17:20:35.461Z</updated>
    
    <content type="html"><![CDATA[<p>这是为实验室写的，借由eMaize问题帮助大家简单了解机器学习基本方法和基础代码的教程。也可以在<a href="https://lulab.gitbooks.io/bioinfo/content/5%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%95%B4%E5%90%88----%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%92%8C%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/51.html" target="_blank" rel="noopener">这里</a>看到</p><p>由于jupyter notebook的强大的展示功能，本教程还用jupyter notebook组织且运行，可以获得更好的学习效果，代码在<a href="https://github.com/james20141606/somethingmore/blob/master/bioinfo.ipynb" target="_blank" rel="noopener">这里</a>,欢迎取用。<a href="http://localhost:4000/2018/04/12/setup/" target="_blank" rel="noopener">在这里</a>我简单介绍了如何配置jupyter，在<a href="https://james20141606.github.io/2018/04/10/Deep-Learning-Practice/">Deep Learning tutorial</a>中我也强烈推荐了jupyter，并且介绍了很多基于jupyter的资源，强烈建议尝试一下。<br><a id="more"></a></p><h2 id="0-背景简介"><a href="#0-背景简介" class="headerlink" title="0.背景简介"></a>0.背景简介</h2><p>该通过基因型预测表型的实例来自<a href="http://emaize.imaze.org" target="_blank" rel="noopener">eMaize challenge</a>:<br>eMaize问题要求我们以SNP作为特征，通过训练一个模型，对玉米的三个性状进行预测。<br>接下来的教程会展示从原始数据开始，如何对数据进行转换，存取，特征选择以及回归和后续分析的整个过程。本问题最基本的目标是使用6210个样本中的前4754个样本作为训练集，预测其他样本的性状<br></p><h2 id="I-上机指南"><a href="#I-上机指南" class="headerlink" title="I.上机指南"></a>I.上机指南</h2><p>本任务依赖于python语言及jupyter notebook，所需工具已安装到虚拟机。以下指南的所有代码均可在4.Emaize/jupyter_notebook/basic_tutorial.ipynb 中找到。</p><p>使用方法：</p><ul><li><p>打开终端，进入Bioinfo_Lab/4.Emaize/ 文件夹</p></li><li><p>输入jupyter notebook，等待弹出窗口，或者手动复制粘贴终端显示的网址到浏览器。</p></li><li><p>点击jupyter_notebook,再点击basic_tutorial.ipynb,即可看到本部分的教程。按照相关指南一步一步运行即可。本部分接下来的内容与basic_tutorial.ipynb中的内容一致。</p></li></ul><h4 id="jupyter-notebook基本使用指南："><a href="#jupyter-notebook基本使用指南：" class="headerlink" title="jupyter notebook基本使用指南："></a>jupyter notebook基本使用指南：</h4><p>本教程使用jupyter notebook，可以让使用者获得更好的体验，方便对代码进行修改，以及对结果进行查看和分析</p><ul><li>一段相关的代码在同一个代码框中书写 <br></li><li>同时按住shift与enter即可运行选中的代码框的代码<br></li><li>仅仅按enter键具有回车的效果</li><li><h5 id="使用上方的编辑栏："><a href="#使用上方的编辑栏：" class="headerlink" title="使用上方的编辑栏："></a>使用上方的编辑栏：</h5>点击加号在两个代码框中间插入新的代码框，删除代码框点击剪刀，中止程序点击方框</li></ul><figure class="highlight xl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">#导入必需的库</span><br><span class="line"><span class="keyword">import</span> warnings</span><br><span class="line">warnings.filterwarnings(<span class="string">'ignore'</span>)</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">from sklearn.random_projection <span class="keyword">import</span> SparseRandomProjection</span><br><span class="line">from scipy.sparse <span class="keyword">import</span> load_npz, save_npz</span><br><span class="line"><span class="keyword">import</span> scipy.stats</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> sklearn</span><br><span class="line"><span class="keyword">import</span> h5py</span><br><span class="line">from sklearn.metrics <span class="keyword">import</span> r2_score</span><br><span class="line">from scipy.stats.stats <span class="keyword">import</span> pearsonr</span><br><span class="line"><span class="keyword">import</span> pickle</span><br><span class="line">from sklearn.linear_model <span class="keyword">import</span> LinearRegression</span><br><span class="line">#<span class="keyword">import</span> xgboost</span><br><span class="line">#from xgboost.sklearn <span class="keyword">import</span> XGBRegressor</span><br><span class="line">from sklearn.linear_model <span class="keyword">import</span> Ridge</span><br><span class="line">from sklearn.kernel_ridge <span class="keyword">import</span> KernelRidge</span><br><span class="line">from sklearn <span class="keyword">import</span> neighbors</span><br><span class="line">from sklearn.ensemble <span class="keyword">import</span> RandomForestRegressor</span><br><span class="line">from sklearn.gaussian_process <span class="keyword">import</span> GaussianProcessRegressor</span><br><span class="line">from sklearn.gaussian_process.kernels <span class="keyword">import</span> DotProduct</span><br><span class="line">from tqdm <span class="keyword">import</span> tqdm_notebook <span class="keyword">as</span> tqdm</span><br><span class="line">from IPython.display <span class="keyword">import</span> display, Image</span><br><span class="line">%pylab inline</span><br></pre></td></tr></table></figure><h3 id="1-查看原始数据"><a href="#1-查看原始数据" class="headerlink" title="1.查看原始数据"></a>1.查看原始数据</h3><h4 id="1-1-数据种类"><a href="#1-1-数据种类" class="headerlink" title="1.1 数据种类"></a>1.1 数据种类</h4><ul><li>genotype：SNP数据，每个位点可能有三种情况，如AA，AT，TT <br></li><li>trait：共三种，trait1开花期，trait2株高，trait3产量，为连续值 <br></li><li>原始数据中有6210个样本，每个样本SNP位点约为190万个,<br>因为计算资源的原因，这里仅仅选取其中的5000个SNP作为示例,因为数据量的原因，结果肯定不够理想</li></ul><h4 id="1-2-数据格式"><a href="#1-2-数据格式" class="headerlink" title="1.2 数据格式"></a>1.2 数据格式</h4><p>txt存储格式不适合大数据读取的问题，对内存的占用过多。对于结构化的、能够存储为矩阵的数据，可以使用HDF5格式存取，内存占用小，读取速度快</p><h5 id="读取SNP数据，数据格式为HDF5"><a href="#读取SNP数据，数据格式为HDF5" class="headerlink" title="读取SNP数据，数据格式为HDF5"></a>读取SNP数据，数据格式为HDF5</h5><h5 id="在命令行查看数据shape的方法为："><a href="#在命令行查看数据shape的方法为：" class="headerlink" title="在命令行查看数据shape的方法为："></a>在命令行查看数据shape的方法为：</h5><ul><li>cd至文件路径下，输入：h5ls snp_5000 <br></li><li>若使用了新版h5py，可能出现无法打开的情况，此时输入HDF5_USE_FILE_LOCKING=FALSE h5ls snp_5000</li></ul><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="section">#使用h5py读取5000个SNP：</span></span><br><span class="line">with h5py.File('data/snp_5000') as f:</span><br><span class="line">snps = f[<span class="string">'snp'</span>][<span class="symbol">:</span>]</span><br><span class="line"><span class="section">#查看数据shape,h5py读取出的snps是一个矩阵，可以用.shape查看其shape</span></span><br><span class="line">snps.shape</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">查看数据内容</span></span><br><span class="line">snps</span><br></pre></td></tr></table></figure><h5 id="读取性状数据"><a href="#读取性状数据" class="headerlink" title="读取性状数据"></a>读取性状数据</h5><p>使用numpy/pandas均可读取性状数据并显示，这里用pandas展示，真正计算时一般用numpy</p><figure class="highlight nginx"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="attribute">traits</span> = pd.read_csv(<span class="string">'data/pheno_emaize.txt'</span>,delimiter=<span class="string">'\t'</span>)</span><br><span class="line"><span class="comment">#仅显示前五个,4754之后的样本的性状是未知的</span></span><br><span class="line">traits.head()</span><br></pre></td></tr></table></figure><figure class="highlight fortran"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">#pandas dataframe也可查看<span class="built_in">shape</span></span><br><span class="line"><span class="built_in">print</span> traits.<span class="built_in">shape</span></span><br></pre></td></tr></table></figure><figure class="highlight prolog"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">#查看性状的分布情况</span><br><span class="line">trait1 = np.array(traits[<span class="string">'trait1'</span>])[:<span class="number">4754</span>]</span><br><span class="line">trait2 = np.array(traits[<span class="string">'trait2'</span>])[:<span class="number">4754</span>]</span><br><span class="line">trait3 = np.array(traits[<span class="string">'trait3'</span>])[:<span class="number">4754</span>]</span><br><span class="line">fig, ax = plt.subplots(<span class="number">1</span>,<span class="number">3</span>, figsize=(<span class="number">15</span>,<span class="number">3</span>))</span><br><span class="line">ax[<span class="number">0</span>].hist(trait1,bins = <span class="number">50</span>)</span><br><span class="line">ax[<span class="number">0</span>].set_title(<span class="string">'normalized trait1 value distribution'</span>)</span><br><span class="line">ax[<span class="number">1</span>].hist(trait2,bins = <span class="number">50</span>)</span><br><span class="line">ax[<span class="number">1</span>].set_title(<span class="string">'normalized trait2 value distribution'</span>)</span><br><span class="line">ax[<span class="number">2</span>].hist(trait3,bins = <span class="number">50</span>)</span><br><span class="line">ax[<span class="number">2</span>].set_title(<span class="string">'normalized trait3 value distribution'</span>)</span><br></pre></td></tr></table></figure><p><img src="http://i1.bvimg.com/640680/30af897795c31338.png" alt="Markdown"></p><h5 id="查看训练集与测试集的划分"><a href="#查看训练集与测试集的划分" class="headerlink" title="查看训练集与测试集的划分"></a>查看训练集与测试集的划分</h5><p>下图中彩色部分为训练集性状，白色部分为待预测性状 <br><br>可以发现其划分方式并不随机，这会导致常规的机器学习方法出现一些问题，由于是基础介绍，这里不讨论如何解决这个问题。</p><figure class="highlight prolog"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">def generate_parent_table(phenotype_file):</span><br><span class="line">phenotypes = pd.read_table(phenotype_file)</span><br><span class="line">pedigree = phenotypes[<span class="string">'pedigree'</span>].str.split(<span class="string">'_'</span>, expand=<span class="symbol">True</span>)</span><br><span class="line">pedigree.columns = [<span class="string">'f'</span>, <span class="string">'X'</span>, <span class="string">'m'</span>]</span><br><span class="line">phenotypes = pd.concat([phenotypes, pedigree], axis=<span class="number">1</span>)</span><br><span class="line">phenotypes[<span class="string">'number'</span>] = np.arange(phenotypes.shape[<span class="number">0</span>])</span><br><span class="line">parent_table = phenotypes.pivot_table(values=<span class="string">'number'</span>, index=[<span class="string">'m'</span>], columns=[<span class="string">'f'</span>], dropna=<span class="symbol">False</span>)</span><br><span class="line">male_ids = [<span class="string">'m%d'</span> <span class="comment">% i for i in range(1, parent_table.shape[0] + 1)]</span></span><br><span class="line">female_ids = [<span class="string">'f%d'</span> <span class="comment">% i for i in range(1, parent_table.shape[1] + 1)]</span></span><br><span class="line">parent_table = parent_table.loc[male_ids, female_ids]</span><br><span class="line">return parent_table</span><br><span class="line">phenotype_file = <span class="string">'data/pheno_emaize.txt'</span></span><br><span class="line">parent_table = generate_parent_table(phenotype_file)</span><br><span class="line">phenotypes = pd.read_table(<span class="string">'data/pheno_emaize.txt'</span>)</span><br><span class="line">fig, ax = subplots(<span class="number">3</span>,<span class="number">1</span>, figsize=(<span class="number">20</span>, <span class="number">10</span>))</span><br><span class="line">for i in range(<span class="number">3</span>):</span><br><span class="line">trait = [<span class="string">'trait1'</span>,<span class="string">'trait2'</span>,<span class="string">'trait3'</span>][i]</span><br><span class="line">ax[i].matshow(np.take(np.ravel(phenotypes[trait].values), parent_table), cmap=cm.<span class="symbol">RdBu</span>)</span><br><span class="line">ax[i].set_title(<span class="string">'Phenotypes of training data (%s)'</span><span class="comment">%trait)</span></span><br></pre></td></tr></table></figure><h3 id="2-将SNP数据编码为向量"><a href="#2-将SNP数据编码为向量" class="headerlink" title="2. 将SNP数据编码为向量"></a>2. 将SNP数据编码为向量</h3><p>每个位点的碱基只有三种情况，不会出现更多碱基组合的可能，比如某位点仅有AA，AT，TT三种可能的情况<br><br>我们可以采取三种方式对其编码：</p><ul><li>转化为0、1、2。找到minor allele frequency（MAF），即两种碱基（如A、T）中出现频率低的那个，以A作为MAF为例，则TT为0，AT为1，AA为2，这样可以突出MAF</li><li>转化为3-bit one hot vector,$[1,0,0]^T,[0,1,0]^T,[0,0,1]^T$这样可以保持三种向量在空间距离的一致</li><li>转化为2-bit vector,则AA，AT，TT分别编为$[1,0]^T,[1,1]^T,[0,1]^T$,不需要考虑MAF<br>我们采取第三种方式<figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">def convert_2bit(seq):</span><br><span class="line">genotypes = np.zeros([6210,2])</span><br><span class="line">a = seq[1].split('/')</span><br><span class="line">for i in range(6210):</span><br><span class="line">if seq[<span class="string">4:</span>][<span class="symbol">i</span>] == a[0] + a[0]:</span><br><span class="line">genotypes[i] = np.array([0,1])</span><br><span class="line">if seq[<span class="string">4:</span>][<span class="symbol">i</span>] == a[0] + a[1]:</span><br><span class="line">genotypes[i] = np.array([1,0])</span><br><span class="line">if seq[<span class="string">4:</span>][<span class="symbol">i</span>] == a[1] + a[1]:</span><br><span class="line">genotypes[i] = np.array([1,1])</span><br><span class="line">genotypes = genotypes.astype('int').T</span><br><span class="line">return genotypes</span><br></pre></td></tr></table></figure></li></ul><h5 id="注意，接下来的步骤耗时14min"><a href="#注意，接下来的步骤耗时14min" class="headerlink" title="注意，接下来的步骤耗时14min"></a>注意，接下来的步骤耗时14min</h5><p>真实计算时此步骤使用C加速计算，这里为了连贯性仅仅展示python的方法 <br><br>可以跳过接下来的代码框步骤，直接使用处理好的结果，结果放在 /data/2bit_geno<br><figure class="highlight makefile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#该代码框可跳过以节约时间，直接运行下一个代码框</span></span><br><span class="line">geno_conv = convert_2bit(snps[1])</span><br><span class="line">for i in tqdm(range(4999)):</span><br><span class="line">geno_conv = np.concatenate((geno_conv,convert_2bit(snps[i+2])),axis =0)</span><br></pre></td></tr></table></figure></p><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="section">#读取处理成2bit格式的SNP</span></span><br><span class="line">with h5py.File('data/2bit_geno') as f:</span><br><span class="line">geno_conv = f[<span class="string">'data'</span>][<span class="symbol">:</span>]</span><br></pre></td></tr></table></figure><figure class="highlight nix"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#查看SNP的大致情况</span></span><br><span class="line">fig, <span class="attr">ax</span> = plt.subplots(<span class="attr">figsize=(5,10))</span></span><br><span class="line">ax.matshow(geno_conv[:<span class="number">300</span>,:<span class="number">200</span>],<span class="attr">cmap</span> = cm.binary_r)</span><br></pre></td></tr></table></figure><h3 id="3-特征提取与降维"><a href="#3-特征提取与降维" class="headerlink" title="3. 特征提取与降维"></a>3. 特征提取与降维</h3><ul><li>原数据每个样本有190万个SNP，转化为2bit coding后有大约380万个feature，大多数的feature可能是冗余的 <br></li><li>过多的feature使得机器学习模型无法承受，一个考虑时间开销及效果的feature数量应该在几千至几万量级 <br></li></ul><h4 id="特征选择："><a href="#特征选择：" class="headerlink" title="特征选择："></a>特征选择：</h4><p>特征选择的方法包括filter，wrapper和embedding三大类 <br><br>我们使用过如下方法： <br></p><ul><li>Mutual information:劣势在于需要将连续的性状值离散化，损失信息<br></li><li>ANOVA:通过p-value筛选feature，速度较慢，我们设计了加速ANOVA计算的算法。<br></li><li>基于模型的方法:基于广义线性模型或其他带有feature权重的机器学习模型，根据权重挑选feature<br></li></ul><h4 id="降维："><a href="#降维：" class="headerlink" title="降维："></a>降维：</h4><ul><li>PCA、SVD：劣势在于降维后的feature数量不能超过样本数量，一次性损失的feature过多<br></li><li>Random projection:基于LSH的降维方式，速度较快<br></li></ul><p>通过对问题的后续分析，我们发现对于预测绝大多数样本，基本的降维方法就已经够用<br><br>但是对于部分很难预测的样本，简单的特征选择方法也无法取得好的效果<br><br>我们根据后续开发的针对性的模型，设计了基于模型的特征选择方法，因为内容限制，不在这里使用。</p><h5 id="接下来分别使用ANOVA和Random-projection演示特征选择和降维，对于后续的计算来说，选择其中一种就可以，也可以把不同的方法拼起来使用"><a href="#接下来分别使用ANOVA和Random-projection演示特征选择和降维，对于后续的计算来说，选择其中一种就可以，也可以把不同的方法拼起来使用" class="headerlink" title="接下来分别使用ANOVA和Random projection演示特征选择和降维，对于后续的计算来说，选择其中一种就可以，也可以把不同的方法拼起来使用"></a>接下来分别使用ANOVA和Random projection演示特征选择和降维，对于后续的计算来说，选择其中一种就可以，也可以把不同的方法拼起来使用</h5><h5 id="我们会提供ANOVA、Random-projection处理上面5000个snps后的数据，以及在完整数据集上用Random-projection降维至10000个feature的数据。用于送入下一部分的回归模型。下面的三种方法的处理后的数据可以在feature-selection文件夹下找到，存储格式为HDF5，可用h5py打开"><a href="#我们会提供ANOVA、Random-projection处理上面5000个snps后的数据，以及在完整数据集上用Random-projection降维至10000个feature的数据。用于送入下一部分的回归模型。下面的三种方法的处理后的数据可以在feature-selection文件夹下找到，存储格式为HDF5，可用h5py打开" class="headerlink" title="我们会提供ANOVA、Random projection处理上面5000个snps后的数据，以及在完整数据集上用Random projection降维至10000个feature的数据。用于送入下一部分的回归模型。下面的三种方法的处理后的数据可以在feature_selection文件夹下找到，存储格式为HDF5，可用h5py打开"></a>我们会提供ANOVA、Random projection处理上面5000个snps后的数据，以及在完整数据集上用Random projection降维至10000个feature的数据。用于送入下一部分的回归模型。下面的三种方法的处理后的数据可以在feature_selection文件夹下找到，存储格式为HDF5，可用h5py打开</h5><h5 id="ANOVA数据："><a href="#ANOVA数据：" class="headerlink" title="ANOVA数据："></a>ANOVA数据：</h5><p>feature_selection/anova 包含三个性状各自的feature，大小为4000*6210</p><h5 id="Random-projection-5000-数据："><a href="#Random-projection-5000-数据：" class="headerlink" title="Random projection(5000)数据："></a>Random projection(5000)数据：</h5><p>feature_selection/randomproj_5000 从5000个SNPs降维得到，三个性状使用同一组feature,大小为1000*6210</p><h5 id="Random-projection-whole-SNPs-数据："><a href="#Random-projection-whole-SNPs-数据：" class="headerlink" title="Random projection(whole SNPs)数据："></a>Random projection(whole SNPs)数据：</h5><p>feature_selection/randomproj_whole 从所有SNPs降维得到，三个性状使用同一组feature，大小为10000*6210</p><h4 id="3-1-ANOVA"><a href="#3-1-ANOVA" class="headerlink" title="3.1 ANOVA"></a>3.1 ANOVA</h4><p>方差分析方法可以利用p值挑选feature <br><br>调用scipy.stats.f_oneway,利用SNPs和性状可以很容易地计算出p-value，但是对于大量数据来说速度较慢 <br><br>这里我们使用一种加速ANOVA计算的方法完成计算，相比于scipy.stats的方法可以提升计算速度数百倍。<br><figure class="highlight makefile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#加速ANOVA算法</span></span><br><span class="line">def fast_anova_2bit(X, y):</span><br><span class="line">y = y - y.mean()</span><br><span class="line">y2 = y*y</span><br><span class="line">N = X.shape[0]</span><br><span class="line">SS_tot = np.sum(y2)</span><br><span class="line"><span class="comment"># 10, 01, 11</span></span><br><span class="line">masks = [np.logical_and(X[:, 0::2], np.logical_not(X[:, 1::2])),</span><br><span class="line"><span class="section">np.logical_and(np.logical_not(X[:, 0::2]), X[:, 1::2]),</span></span><br><span class="line"><span class="section">np.logical_and(X[:, 0::2], X[:, 1::2])]</span></span><br><span class="line">Ni = np.concatenate([np.sum(mask, axis=0) for mask in masks]).reshape((3, -1))</span><br><span class="line">at_least_one = Ni &gt; 0</span><br><span class="line">SS_bn = [np.sum(y.reshape((-1, 1))*mask, axis=0) for mask in masks]</span><br><span class="line">SS_bn = np.concatenate(SS_bn).reshape((3, -1))</span><br><span class="line">SS_bn **= 2</span><br><span class="line">SS_bn = np.where(at_least_one, SS_bn/Ni, 0)</span><br><span class="line">SS_bn = np.sum(SS_bn, axis=0)</span><br><span class="line">SS_wn = SS_tot - SS_bn</span><br><span class="line">M = np.sum(at_least_one, axis=0)</span><br><span class="line">DF_bn = M - 1</span><br><span class="line">DF_wn = N - M</span><br><span class="line">SS_bn /= DF_bn</span><br><span class="line">SS_wn /= DF_wn</span><br><span class="line">F = SS_bn/SS_wn</span><br><span class="line">p_vals = np.ones(F.shape[0])</span><br><span class="line">ind = np.nonzero(M == 2)[0]</span><br><span class="line">if ind.shape[0] &gt; 0:</span><br><span class="line">p_vals[ind] = scipy.stats.f.sf(F[ind], 1, N - 2)</span><br><span class="line">ind = np.nonzero(M == 3)[0]</span><br><span class="line">if ind.shape[0] &gt; 0:</span><br><span class="line">p_vals[ind] = scipy.stats.f.sf(F[ind], 2, N - 3)</span><br><span class="line">return F, p_vals</span><br></pre></td></tr></table></figure></p><h5 id="注意X和y分别是什么"><a href="#注意X和y分别是什么" class="headerlink" title="注意X和y分别是什么"></a>注意X和y分别是什么</h5><p>我们需要输入进 fast_anova_2bit(X, y)的X是处理过的SNPs中前4754个样本的，y是trait1、trait2、trait3<br>因为需要分别预测三个性状，我们需要针对三个性状分别挑选特征</p><figure class="highlight fortran"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">geno_conv_train = geno_conv[:,:<span class="number">4754</span>]</span><br><span class="line">geno_conv_test = geno_conv[:,<span class="number">4754</span>:]</span><br><span class="line"><span class="built_in">print</span> geno_conv_train.<span class="built_in">shape</span></span><br><span class="line"><span class="built_in">print</span> geno_conv_test.<span class="built_in">shape</span></span><br></pre></td></tr></table></figure><figure class="highlight autohotkey"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">#分别计算三种性状下<span class="number">5000</span>个SNPs做完ANOVA的p-value</span><br><span class="line"><span class="built_in">F,</span>pval_1 = fast_anov<span class="built_in">a_2bit</span>(geno_conv_train.T,trait1)</span><br><span class="line"><span class="built_in">F,</span>pval_2 = fast_anov<span class="built_in">a_2bit</span>(geno_conv_train.T,trait2)</span><br><span class="line"><span class="built_in">F,</span>pval_3 = fast_anov<span class="built_in">a_2bit</span>(geno_conv_train.T,trait3)</span><br><span class="line">pval_1.shape</span><br></pre></td></tr></table></figure><figure class="highlight lsl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">fig, ax = plt.subplots(<span class="number">1</span>,<span class="number">3</span>, figsize=(<span class="number">15</span>,<span class="number">3</span>))</span><br><span class="line">ax[<span class="number">0</span>].hist(pval_1,bins = <span class="number">50</span>)</span><br><span class="line">ax[<span class="number">0</span>].set_title('<span class="number">5000</span> SNPs p-value for trait1 distribution')</span><br><span class="line">ax[<span class="number">1</span>].hist(pval_2,bins = <span class="number">50</span>)</span><br><span class="line">ax[<span class="number">1</span>].set_title('<span class="number">5000</span> SNPs p-value for trait2 distribution')</span><br><span class="line">ax[<span class="number">2</span>].hist(pval_3,bins = <span class="number">50</span>)</span><br><span class="line">ax[<span class="number">2</span>].set_title('<span class="number">5000</span> SNPs p-value for trait3 distribution')</span><br></pre></td></tr></table></figure><p><img src="http://i1.bvimg.com/640680/b8b10e4aac94c22f.png" alt="Markdown"><br>可以设定一个阈值，比如留下p-value前百分之四十的SNPs,根据index选取留下的SNPs<br><figure class="highlight cos"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">threshold1 = np.percentile(pval_1,<span class="number">40</span>)</span><br><span class="line">threshold2 = np.percentile(pval_2,<span class="number">40</span>)</span><br><span class="line">threshold3 = np.percentile(pval_3,<span class="number">40</span>)</span><br><span class="line"><span class="keyword">print</span> 'threshold1: <span class="built_in">%f</span>' <span class="built_in">%threshold</span>1</span><br><span class="line"><span class="keyword">print</span> 'threshold2: <span class="built_in">%f</span>' <span class="built_in">%threshold</span>2</span><br><span class="line"><span class="keyword">print</span> 'threshold3: <span class="built_in">%f</span>' <span class="built_in">%threshold</span>3</span><br></pre></td></tr></table></figure></p><figure class="highlight ini"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#返回符合条件的p-value的坐标，即可找到需要留下的SNPs的位置，注意每个SNPs占据两行</span></span><br><span class="line"><span class="attr">anova_index_1</span> = np.where(pval_1&lt;threshold1)[<span class="number">0</span>]</span><br><span class="line"><span class="attr">anova_index_1</span> = np.sort(np.concatenate((anova_index_1,anova_index_1 +<span class="number">1</span>)))</span><br><span class="line"><span class="attr">anova_index_2</span> = np.where(pval_2&lt;threshold2)[<span class="number">0</span>]</span><br><span class="line"><span class="attr">anova_index_2</span> = np.sort(np.concatenate((anova_index_2,anova_index_2 +<span class="number">1</span>)))</span><br><span class="line"><span class="attr">anova_index_3</span> = np.where(pval_3&lt;threshold3)[<span class="number">0</span>]</span><br><span class="line"><span class="attr">anova_index_3</span> = np.sort(np.concatenate((anova_index_3,anova_index_3 +<span class="number">1</span>)))</span><br></pre></td></tr></table></figure><figure class="highlight ini"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#根据p-value选取保留的SNPs，注意这一步要在所有的6210个样本上做</span></span><br><span class="line"><span class="attr">feature1_anova</span> = np.take(geno_conv,anova_index_1,axis=<span class="number">0</span>)</span><br><span class="line"><span class="attr">feature2_anova</span> = np.take(geno_conv,anova_index_2,axis=<span class="number">0</span>)</span><br><span class="line"><span class="attr">feature3_anova</span> = np.take(geno_conv,anova_index_3,axis=<span class="number">0</span>)</span><br></pre></td></tr></table></figure><h4 id="3-2-Random-projection"><a href="#3-2-Random-projection" class="headerlink" title="3.2 Random projection"></a>3.2 Random projection</h4><p>Random projection 不依赖于性状，仅仅在原SNPs数据进行降维 <br><br>Random projection可以使用scikit-learn下的sklearn.random_projection模块计算 <br><br>包括generate，transform和normalize等步骤 <br><br>这里演示从5000个feature（10000行）降维</p><h5 id="3-2-1-generate"><a href="#3-2-1-generate" class="headerlink" title="3.2.1 generate"></a>3.2.1 generate</h5><p>产生一个稀疏矩阵</p><figure class="highlight lsl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">#<span class="number">10000</span>为操作前的feature个数：<span class="number">2</span>*<span class="number">5000</span></span><br><span class="line">X = np.zeros((<span class="number">2</span>, <span class="number">10000</span>))</span><br><span class="line">#确定降维后的个数，这里定为<span class="number">1000</span>，使用sklearn random_projection 模块下的 SparseRandomProjection 函数</span><br><span class="line">proj = SparseRandomProjection(<span class="number">1000</span>)</span><br><span class="line">proj.fit(X)</span><br><span class="line">print proj.components_.shape</span><br></pre></td></tr></table></figure><h5 id="3-2-2-transform"><a href="#3-2-2-transform" class="headerlink" title="3.2.2 transform"></a>3.2.2 transform</h5><figure class="highlight ini"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">X</span>= geno_conv.T</span><br><span class="line"><span class="attr">X_</span> = proj.transform(X)</span><br></pre></td></tr></table></figure><h5 id="3-2-3-normalize"><a href="#3-2-3-normalize" class="headerlink" title="3.2.3 normalize"></a>3.2.3 normalize</h5><p>对每个feature进行normalize，避免出现过大的值<br><figure class="highlight capnproto"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler</span><br><span class="line">normalized_feeature = StandardScaler().fit_transform(X_).T</span><br></pre></td></tr></table></figure></p><p>最终大小为1000*6210，结果在feature_selection/randomproj_5000</p><h3 id="4-回归模型"><a href="#4-回归模型" class="headerlink" title="4. 回归模型"></a>4. 回归模型</h3><p>这部分通过几个常用的机器学习模型对上一部处理过的feature进行拟合和预测<br>这里使用sklearn和xgboost提供的模块，这些模块具有很好的封装，使用风格统一，使用时可以查看其官方文档<br><br>这里不介绍具体的机器学习模型的算法原理，可以参考周志华老师的《机器学习》等书进行学习。</p><h4 id="4-1-机器学习模型"><a href="#4-1-机器学习模型" class="headerlink" title="4.1 机器学习模型"></a>4.1 机器学习模型</h4><p>接下来会使用一些常用的可以用于回归的机器学习模型，可以选择其中的一种或几种对feature_selection/文件夹下的三种数据进行回归和预测。下面我们将几个模型列出，并且选择其中的一种作为示例，其他的模型可同理调用。 <br></p><h4 id="4-2-评价指标"><a href="#4-2-评价指标" class="headerlink" title="4.2 评价指标"></a>4.2 评价指标</h4><p>我们使用<script type="math/tex">r^2,pcc</script>作为衡量预测结果的指标<br></p><script type="math/tex; mode=display">r^2 = 1-\frac{SS_{res}}{SS_{tot}}$$<br>$$pcc = \frac{cov(X,Y)}{\sigma_X \sigma_Y} = \frac{E[(X-\mu_X)(Y-\mu_Y)]}{\sigma_X \sigma_Y} $$<br>我们可以绘制结果的heatmap图，散点图等进行可视化。#### 4.3 交叉验证交叉验证(Cross validation)可以帮助调参，寻找机器学习模型中的超参数 <br>一般可以使用10折或者5折交叉验证，注意在最终预测时，使用调参后的模型在整个训练集上训练，这时不再交叉验证 <br>因为交叉验证需要额外增加计算时间，因此这里只在整个训练集上训练一次，不再展示交叉验证的过程。##### 如果深究的话，本问题还有其特殊性，可以设计特殊的交叉验证方式不同的样本具有关联性CV，有的样本可能来自同一亲本，而且训练集和测试集的划分并不是随机的因此在真正解决这个问题的时候，需要考虑不同的抽样方式下的调参与训练，我们可以使用下图所示的几种抽样方式<figure class="highlight oxygene"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> <span class="function"><span class="keyword">method</span> <span class="title">in</span> <span class="params">(<span class="string">'random'</span>, <span class="string">'by_female'</span>, <span class="string">'by_male'</span>, <span class="string">'cross'</span>)</span>:</span></span><br><span class="line"><span class="keyword">with</span> h5py.File(<span class="string">'data/cv_index.%s'</span>%<span class="function"><span class="keyword">method</span>, '<span class="title">r</span>') <span class="title">as</span> <span class="title">f</span>:</span></span><br><span class="line">index_train = f[<span class="string">'0/train'</span>][:]</span><br><span class="line">index_test = f[<span class="string">'0/test'</span>][:]</span><br><span class="line">fig, ax = subplots(<span class="number">2</span>, <span class="number">1</span>, figsize=(<span class="number">16</span>, <span class="number">6</span>))</span><br><span class="line">sampling_table = np.zeros(np.prod(parent_table.shape))</span><br><span class="line">sampling_table[index_train] = <span class="number">1</span></span><br><span class="line">sampling_table = np.take(sampling_table, parent_table)</span><br><span class="line">ax[<span class="number">0</span>].matshow(sampling_table, cmap=cm.Greys)</span><br><span class="line">ax[<span class="number">0</span>].set_title(<span class="string">'Training samples (%s)'</span>%<span class="function"><span class="keyword">method</span>)</span></span><br><span class="line"><span class="function"></span></span><br><span class="line"><span class="function"><span class="title">sampling_table</span> = <span class="title">np</span>.<span class="title">zeros</span><span class="params">(np.prod(parent_table.shape)</span>)</span></span><br><span class="line"><span class="function"><span class="title">sampling_table</span>[<span class="title">index_test</span>] = 1</span></span><br><span class="line"><span class="function"><span class="title">sampling_table</span> = <span class="title">np</span>.<span class="title">take</span><span class="params">(sampling_table, parent_table)</span></span></span><br><span class="line"><span class="function"><span class="title">ax</span>[1].<span class="title">matshow</span><span class="params">(sampling_table, cmap=cm.Greys)</span></span></span><br><span class="line"><span class="function"><span class="title">ax</span>[1].<span class="title">set_title</span><span class="params">(<span class="string">'Test samples (%s)'</span>%<span class="keyword">method</span>)</span></span></span><br><span class="line"><span class="function"><span class="title">plt</span>.<span class="title">tight_layout</span><span class="params">()</span></span></span><br></pre></td></tr></table></figure>![Markdown](http://i1.bvimg.com/640680/53ed95bc3fc879c9.png)![Markdown](http://i1.bvimg.com/640680/3a9fec1a694dc533.png)![Markdown](http://i1.bvimg.com/640680/f9198785ddd4e809.png)![Markdown](http://i1.bvimg.com/640680/18e028db8839992e.png)#### 4.4 准备数据sklearn的机器学习模型一般需要提供X和y以供模型训练，然后提供新的X，模型就可以预测新的y <br>通过前面的工作，我们获得了三种不同的X(ANOVA、random projection(5000/whole))，我们还需要将X和y划分为训练集和测试集 <br>注意我们需要分别对三个性状进行预测，因此ANOVA的X是三种 <br>##### 评价模型的时候要注意，y_true的部分值缺失##### 4.4.1 准备y先处理y，y 的train和test是统一的，不受方法影响<figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="section">#y 的train和test的统一的，不受方法影响</span></span><br><span class="line">pheno<span class="emphasis">_whole = pd.read_</span>csv('data/emaize<span class="emphasis">_pheno_</span>whole',delimiter=',')</span><br><span class="line">wholepheno = &#123;&#125;</span><br><span class="line">for trait in ['trait1','trait2','trait3']:</span><br><span class="line">wholepheno[trait] = np.array(pheno_whole[trait])</span><br><span class="line">y_train = &#123;&#125;</span><br><span class="line">y_test = &#123;&#125;</span><br><span class="line">for trait in ['trait1','trait2','trait3']:</span><br><span class="line">y_train[<span class="string">trait</span>] = wholepheno[<span class="string">trait</span>][<span class="symbol">:4754</span>]</span><br><span class="line">y_test[<span class="string">trait</span>] = wholepheno[<span class="string">trait</span>][<span class="symbol">4754:</span>]</span><br></pre></td></tr></table></figure>##### 4.4.2 准备X再处理X，使用ANOVA时X要区分不同性状，Random projection由于是与性状无关的降维方法，三种性状下的feature都一样 <br>因为机器学习模型要求一般数据形式为sample*feature，因此需要对结果转置<figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">def prepare_data(method):</span><br><span class="line">if method == 'randomproj_5000':</span><br><span class="line">with h5py.File('feature<span class="emphasis">_selection/randomproj_</span>5000') as f:</span><br><span class="line">X_train = f[<span class="string">'data'</span>][<span class="symbol">:</span>][<span class="string">:,:4754</span>].T</span><br><span class="line">X_test = f[<span class="string">'data'</span>][<span class="symbol">:</span>][<span class="string">:,4754:</span>].T</span><br><span class="line">if method == 'randomproj_whole':</span><br><span class="line">with h5py.File('feature<span class="emphasis">_selection/randomproj_</span>whole') as f:</span><br><span class="line">X_train = f[<span class="string">'X'</span>][<span class="symbol">:</span>][<span class="string">:4754,:</span>]</span><br><span class="line">X_test = f[<span class="string">'X'</span>][<span class="symbol">:</span>][<span class="string">4754:,:</span>]</span><br><span class="line">if method == 'anova':</span><br><span class="line">X_train = &#123;&#125;</span><br><span class="line">X_test = &#123;&#125;</span><br><span class="line">with h5py.File('feature_selection/anova') as f:</span><br><span class="line">X_train[<span class="string">'trait1'</span>] = f[<span class="string">'feature1'</span>][<span class="symbol">:</span>][<span class="string">:,:4754</span>].T</span><br><span class="line">X_test[<span class="string">'trait1'</span>] = f[<span class="string">'feature1'</span>][<span class="symbol">:</span>][<span class="string">:,4754:</span>].T</span><br><span class="line">X_train[<span class="string">'trait2'</span>] = f[<span class="string">'feature2'</span>][<span class="symbol">:</span>][<span class="string">:,:4754</span>].T</span><br><span class="line">X_test[<span class="string">'trait2'</span>] = f[<span class="string">'feature2'</span>][<span class="symbol">:</span>][<span class="string">:,4754:</span>].T</span><br><span class="line">X_train[<span class="string">'trait3'</span>] = f[<span class="string">'feature3'</span>][<span class="symbol">:</span>][<span class="string">:,:4754</span>].T</span><br><span class="line">X_test[<span class="string">'trait3'</span>] = f[<span class="string">'feature3'</span>][<span class="symbol">:</span>][<span class="string">:,4754:</span>].T</span><br><span class="line">return X<span class="emphasis">_train,X_</span>test</span><br></pre></td></tr></table></figure>选择三种方法之一作为X,注意ANOVA方法返回的X需要指明性状<figure class="highlight mel"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">#查看anova方法的X_train X_test</span><br><span class="line">X_train, X_test = prepare_data(<span class="string">'anova'</span>)</span><br><span class="line"><span class="keyword">print</span> <span class="string">'anova method X_train shape: %s'</span> %(X_train[<span class="string">'trait1'</span>].shape,)</span><br><span class="line"><span class="keyword">print</span> <span class="string">'anova method X_test shape: %s'</span> %(X_test[<span class="string">'trait1'</span>].shape,)</span><br></pre></td></tr></table></figure><figure class="highlight maxima"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">#查看<span class="built_in">random</span> projection方法的X_train X_test</span><br><span class="line">X_train, X_test = prepare_data('randomproj_5000')</span><br><span class="line"><span class="built_in">print</span> '<span class="built_in">random</span> projection <span class="built_in">method</span> X_train shape: <span class="built_in">%s</span>' <span class="symbol">%</span>(X_train.shape,)</span><br><span class="line"><span class="built_in">print</span> '<span class="built_in">random</span> projection <span class="built_in">method</span> X_test shape: <span class="built_in">%s</span>' <span class="symbol">%</span>(X_test.shape,)</span><br></pre></td></tr></table></figure>#### 4.5 选择需要的机器学习模型接下来会提供多种机器学习模型，并且讲解其使用方法，可以选择自己喜欢的模型进行回归，也可以用sklearn或其他package提供的模型模型包括：- lr: Linear regression- ridge: Ridge regression- kr: Kernel Ridge regression- rfr: Random Forest regression- xgbr: XGBoost regression- knr: K-nearest neigbour regression- gpr: Gaussian Process regression<figure class="highlight nix"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">def Model(model):</span><br><span class="line"><span class="keyword">if</span> <span class="attr">model=='lr':</span></span><br><span class="line"><span class="attr">reg</span> = LinearRegression()</span><br><span class="line"><span class="comment">#elif model=='xgbr':</span></span><br><span class="line"><span class="comment"># reg = XGBRegressor()</span></span><br><span class="line">elif <span class="attr">model=='ridge':</span></span><br><span class="line"><span class="attr">reg</span> = Ridge()</span><br><span class="line">elif <span class="attr">model=='kr':</span></span><br><span class="line"><span class="attr">reg</span> = KernelRidge(<span class="attr">alpha</span> = <span class="number">10000</span>, <span class="attr">kernel</span> = 'polynomial',<span class="attr">degree</span> = <span class="number">3</span>)</span><br><span class="line">elif <span class="attr">model=='knr':</span></span><br><span class="line"><span class="attr">reg</span> = neighbors.KNeighborsRegressor(<span class="attr">n_neighbors=4,</span> <span class="attr">algorithm='brute')</span></span><br><span class="line">elif <span class="attr">model=='rfr':</span></span><br><span class="line"><span class="attr">reg</span> = RandomForestRegressor(<span class="attr">n_estimators=10,</span> <span class="attr">criterion='mse',</span> <span class="attr">max_depth=12,</span> <span class="attr">n_jobs=5)</span></span><br><span class="line">elif <span class="attr">model=='gpr':</span></span><br><span class="line"><span class="attr">kernel</span> = <span class="number">1.0</span> * DotProduct(<span class="attr">sigma_0=1.0)**4</span></span><br><span class="line"><span class="attr">reg</span> = GaussianProcessRegressor(<span class="attr">kernel</span> = kernel, <span class="attr">optimizer=None)</span></span><br><span class="line">return reg</span><br></pre></td></tr></table></figure>接下来可以使用三种特征（X）中的一种以及七种方法中的一种进行训练和预测 <br>这里我们以randomproj_5000作为特征，用Ridge作为回归模型演示 <br>如果用anova得到的feature，需要注意不同性状的feature不一样 <br>##### 某个机器学习模型的使用方法如下：reg.fit(X,y)用于拟合，reg.predict(X)用于预测。更多用法可以参考sklearn官方文档<figure class="highlight kotlin"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">X_train, X_test = prepare_data(<span class="string">'randomproj_whole'</span>)</span><br><span class="line">reg = Model(<span class="string">'gpr'</span>)</span><br><span class="line">y_predict = &#123;&#125;</span><br><span class="line">y_predict_train = &#123;&#125;</span><br><span class="line"><span class="keyword">for</span> <span class="class"><span class="keyword">trait</span> <span class="title">in</span> <span class="title">tqdm</span></span>([<span class="string">'trait1'</span>,<span class="string">'trait2'</span>,<span class="string">'trait3'</span>]):</span><br><span class="line">reg.fit(X_train,y_train[<span class="class"><span class="keyword">trait</span>])</span></span><br><span class="line">y_predict[<span class="class"><span class="keyword">trait</span>] = <span class="title">reg</span>.<span class="title">predict</span></span>(X_test)</span><br><span class="line">y_predict_train[<span class="class"><span class="keyword">trait</span>] = <span class="title">reg</span>.<span class="title">predict</span></span>(X_train)</span><br><span class="line"></span><br><span class="line">X_test.shape</span><br></pre></td></tr></table></figure>计算预测结果与真实值的$$r^2,pcc</script><figure class="highlight kotlin"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">test_nonan = np.where(np.isnan(np.array(pheno_whole[<span class="string">'trait1'</span>])[<span class="number">4754</span>:]) ==<span class="number">0</span>)</span><br><span class="line">pcc_train = &#123;&#125;</span><br><span class="line">pcc_test = &#123;&#125;</span><br><span class="line">r2_train = &#123;&#125;</span><br><span class="line">r2_test = &#123;&#125;</span><br><span class="line"><span class="keyword">for</span> <span class="class"><span class="keyword">trait</span> <span class="title">in</span> ['<span class="title">trait1</span>',<span class="type">'trait2'</span>,<span class="type">'trait3']:</span></span></span><br><span class="line">pcc_test[<span class="class"><span class="keyword">trait</span>] = <span class="title">pearsonr</span></span>(y_predict[<span class="class"><span class="keyword">trait</span>][<span class="title">test_nonan</span>],<span class="type">np.array</span></span>(pheno_whole[<span class="class"><span class="keyword">trait</span>])[4754:<span class="type">][test_nonan])</span></span></span><br><span class="line">pcc_train[<span class="class"><span class="keyword">trait</span>] = <span class="title">pearsonr</span></span>(y_predict_train[<span class="class"><span class="keyword">trait</span>],<span class="type">y_train[trait])</span></span></span><br><span class="line">r2_test[<span class="class"><span class="keyword">trait</span>] = <span class="title">r2_score</span></span>(y_predict[<span class="class"><span class="keyword">trait</span>][<span class="title">test_nonan</span>],<span class="type">np.array</span></span>(pheno_whole[<span class="class"><span class="keyword">trait</span>])[4754:<span class="type">][test_nonan])</span></span></span><br><span class="line">r2_train[<span class="class"><span class="keyword">trait</span>] = <span class="title">r2_score</span></span>(y_predict_train[<span class="class"><span class="keyword">trait</span>],<span class="type">y_train[trait])</span></span></span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pcc_test</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pcc_train</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">r2_test</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">r2_train</span><br></pre></td></tr></table></figure><p>可以看到预测结果并不是很好，在测试集上的pcc只有0.5左右。后续的分析可以发现，这是因为样本之间具有相关性导致的<br><br>具体的原因分析比较复杂，简单来说，因为这组测试集与训练集的样本的亲本之间亲缘关系较远，模型难以从SNPs得到的feature推断出亲本信息，导致预测结果较差。</p><p>绘制heatmap图观察预测结果 <br><br>GPR具有很强的拟合能力，总可以在训练集上得到接近1的PCC<br><figure class="highlight prolog"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">fig, ax = plt.subplots(<span class="number">2</span>,<span class="number">3</span>, figsize=(<span class="number">15</span>,<span class="number">10</span>))</span><br><span class="line">for i in range(<span class="number">3</span>):</span><br><span class="line">traits = [<span class="string">'trait1'</span>,<span class="string">'trait2'</span>,<span class="string">'trait3'</span>]</span><br><span class="line">ax[<span class="number">0</span>,i].scatter(y_predict[traits[i]][test_nonan],np.array(pheno_whole[traits[i]])[<span class="number">4754</span>:][test_nonan])</span><br><span class="line">ax[<span class="number">0</span>,i].set_title(<span class="string">'%s test set predict &amp; true value plot'</span> <span class="comment">%traits[i])</span></span><br><span class="line">line1 = [(<span class="number">-4</span>, <span class="number">-4</span>), (<span class="number">4</span>, <span class="number">4</span>)]</span><br><span class="line">(line1_xs, line1_ys) = zip(*line1)</span><br><span class="line">ax[<span class="number">0</span>,i].add_line(<span class="symbol">Line2D</span>(line1_xs, line1_ys, linewidth=<span class="number">1</span>, color=<span class="string">'red'</span>))</span><br><span class="line">ax[<span class="number">0</span>,i].set_xlim(left=<span class="number">-4</span>, right=<span class="number">4</span>)</span><br><span class="line">ax[<span class="number">0</span>,i].set_ylim(bottom=<span class="number">-4</span>, top=<span class="number">4</span>)</span><br><span class="line">ax[<span class="number">1</span>,i].scatter(y_predict_train[traits[i]],y_train[traits[i]])</span><br><span class="line">ax[<span class="number">1</span>,i].set_title(<span class="string">'%s train set predict &amp; true value plot'</span> <span class="comment">%traits[i])</span></span><br><span class="line">ax[<span class="number">1</span>,i].add_line(<span class="symbol">Line2D</span>(line1_xs, line1_ys, linewidth=<span class="number">1</span>, color=<span class="string">'red'</span>))</span><br><span class="line">ax[<span class="number">1</span>,i].set_xlim(left=<span class="number">-4</span>, right=<span class="number">4</span>)</span><br><span class="line">ax[<span class="number">1</span>,i].set_ylim(bottom=<span class="number">-4</span>, top=<span class="number">4</span>)</span><br></pre></td></tr></table></figure></p><p><img src="http://i1.bvimg.com/640680/dc071479d62b84c7.png" alt="Markdown"></p><p>绘制完整真实值与预测值的heatmap图 <br><br>从图中我们可以清晰地看出一个基本的模型的问题： <br><br>模型强烈地依赖已有信息进行预测，当未知样本的父本与已知训练集的亲缘关系较远时，模型只能依赖母本（横坐标）进行预测 <br><br>导致预测的heatmap图有明显的与母本相关的特征，而实际上子代的性状更容易被父本主导 <br></p><figure class="highlight kotlin"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">wholepre = np.concatenate((y_predict[<span class="string">'trait1'</span>],y_predict[<span class="string">'trait2'</span>],y_predict[<span class="string">'trait3'</span>])).reshape(<span class="number">3</span>,<span class="number">-1</span>)</span><br><span class="line">predictions = pd.DataFrame(wholepre.T)</span><br><span class="line">predictions.columns = [<span class="string">'trait1'</span>, <span class="string">'trait2'</span>, <span class="string">'trait3'</span>]</span><br><span class="line">predictions = predictions.set_index(np.arange(<span class="number">4754</span>,<span class="number">6210</span>))</span><br><span class="line">def normalize_phenotype(x, range_pheno=<span class="number">4.0</span>):</span><br><span class="line"><span class="keyword">return</span> (np.clip(x, -range_pheno, range_pheno) + range_pheno)/<span class="number">2.0</span>/range_pheno</span><br><span class="line"><span class="keyword">for</span> <span class="class"><span class="keyword">trait</span> <span class="title">in</span> <span class="title">traits</span>:<span class="type"></span></span></span><br><span class="line">fig, ax = subplots(<span class="number">2</span>, <span class="number">1</span>, figsize=(<span class="number">16</span>, <span class="number">6</span>))</span><br><span class="line">ax[<span class="number">0</span>].matshow(np.take(np.ravel(normalize_phenotype(pheno_whole[<span class="class"><span class="keyword">trait</span>].<span class="title">values</span>)), <span class="type">parent_table)</span>, <span class="type">cmap=cm.RdBu_r)</span></span></span><br><span class="line">ax[<span class="number">0</span>].set_title(<span class="string">'Phenotypes of whole true data (%s)'</span>%<span class="class"><span class="keyword">trait</span>)</span></span><br><span class="line"></span><br><span class="line">trait_pred = np.full(phenotypes.shape[<span class="number">0</span>], np.nan)</span><br><span class="line">trait_pred[predictions.index.tolist()] = normalize_phenotype(predictions[<span class="class"><span class="keyword">trait</span>].<span class="title">values</span>)</span></span><br><span class="line">ax[<span class="number">1</span>].matshow(np.take(trait_pred, parent_table), cmap=cm.RdBu)</span><br><span class="line">ax[<span class="number">1</span>].set_title(<span class="string">'Prediction on test data (%s 1)'</span>%traits)</span><br></pre></td></tr></table></figure><p><img src="http://i1.bvimg.com/640680/b514a9c8b7cc7943.png" alt="Markdown"><br><img src="http://i1.bvimg.com/640680/bbe789a8598188da.png" alt="Markdown"><br><img src="http://i1.bvimg.com/640680/19a5441ffb4534eb.png" alt="Markdown"></p><h3 id="后续分析"><a href="#后续分析" class="headerlink" title="后续分析"></a>后续分析</h3><h4 id="不同样本具有不同的预测难度"><a href="#不同样本具有不同的预测难度" class="headerlink" title="不同样本具有不同的预测难度"></a>不同样本具有不同的预测难度</h4><p>普通的机器学习模型在测试集上表现结果不好，但是通过多次的十字交叉抽样模拟，可以发现不同样本的预测难度不同，在大多数样本上，不需要专门设计的机器学习模型就足够表现很好</p><h4 id="样本之间具有关联性"><a href="#样本之间具有关联性" class="headerlink" title="样本之间具有关联性"></a>样本之间具有关联性</h4><p>不服从一些基本的假设，比如线性模型下，残差并不是独立的，需要考虑问题的特殊性进行额外的设计。<br><br>由于存储空间和计算时间的限制，无法展示其他有效的方法，有兴趣的同学可以查找育种领域的其他模型进行尝试。</p><h4 id="复杂的机器学习模型并不一定有效"><a href="#复杂的机器学习模型并不一定有效" class="headerlink" title="复杂的机器学习模型并不一定有效"></a>复杂的机器学习模型并不一定有效</h4><p>育种领域目前最好的模型依然是线性模型，通过特殊的设计，可以考虑到亲缘关系、显著相关的SNP(causal)以及随机效应部分<br>而寻找合适的feature是预测结果好坏的决定性因素，至今没有非常好的方法。</p><p>我们通过模拟特殊的十字交叉抽样方式发现，虽然测试集的样本不好预测，但是大多数的样本使用简单的机器学习方法就可以在大多数样本上取得较好的结果 <br><br>由于计算资源限制，下面直接展示模拟结果</p><p>我们使用一种特殊的十字交叉抽样，在训练集上抽样1000次，用来测试基本的机器学习模型结果 <br><br>我们使用了2bit coding编码的SNPs,通过random projection降维至80000 <br><br>然后使用Gaussian Process Regression作为回归模型 <br><br><img src="http://i1.bvimg.com/640680/839f4f607631b772.png" alt="Markdown"><br><img src="http://i1.bvimg.com/640680/4c1eb79af4bbb5fb.png" alt="Markdown"><br>可以看到一千次抽样的测试结果，大多数测试的PCC都比较高</p><p><img src="http://i1.bvimg.com/640680/bea286d0f130d044.png" alt="Markdown"><br>按照样本查看每个样本多次抽样的平均PCC，注意这里是有bias没有消除的<br><img src="http://i1.bvimg.com/640680/17df0a3dcaa71ada.png" alt="Markdown"></p><p>这里绘制了每个样本的平均PCC heatmap图像,可以发现大多数的样本是很好预测的，样本性状基本由父本性状主导(纵坐标为父本),但是少数亲缘关系较远的父本（图中蓝色线）就很难预测。<br><img src="http://i1.bvimg.com/640680/50adb2cb87536a3d.png" alt="Markdown"><br><img src="http://i1.bvimg.com/640680/48ee63cf0c78d296.png" alt="Markdown"><br><img src="http://i1.bvimg.com/640680/d3bb77a76c162d53.png" alt="Markdown"></p><p>不同父本的性状有显著差别，而子代的性状由于设计原因，主要由父本控制。<br><br>我们可以通过绘图查看不同父本的性状的变化<br><figure class="highlight prolog"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">male_index = np.ndarray([<span class="number">6210</span>,]).astype(<span class="string">'int'</span>)</span><br><span class="line">for i in range(<span class="number">6210</span>):</span><br><span class="line">male_index[i] = int(np.array(phenotypes[<span class="string">'pedigree'</span>])[i].split(<span class="string">'_'</span>)[<span class="number">2</span>][<span class="number">1</span>:])</span><br><span class="line">male_trait1 = np.concatenate((male_index.reshape(<span class="number">1</span>,<span class="number">-1</span>),np.array(pheno_whole[<span class="string">'trait1'</span>]).reshape(<span class="number">1</span>,<span class="number">-1</span>))).<span class="symbol">T</span></span><br><span class="line">male_trait1_bysort = male_trait1[male_trait1[:,<span class="number">0</span>].argsort()]</span><br><span class="line">fig, ax = plt.subplots(figsize=(<span class="number">16</span>,<span class="number">8</span>))</span><br><span class="line">ax.plot(male_trait1_bysort[:,<span class="number">1</span>])</span><br><span class="line">ax.set_title(<span class="string">'different males have varied values'</span>)</span><br></pre></td></tr></table></figure></p><p><img src="http://i1.bvimg.com/640680/818f1d13de885240.png" alt="Markdown"><br>以上内容简要地介绍了eMaize问题使用的一些基本的常用的机器学习方法，包括数据预处理、特征选择、降维、回归以及分析。本教程还顺便展示了一些python常用的工具包的使用，读者有时间可以慢慢体会其中的具体操作，因为jupyter notebook的可视化与交互性很强，读者可以方便地查看中间步骤的数据情况，更好地理解代码所进行的操作。<br><br>由于实际工作的步骤、数据量、变量等问题，还需要慎重考虑计算时间、任务管理等工作 <br><br>想要预测较难预测的样本仅仅靠常规的机器学习方法并不够用，将机器学习应用于生物学问题时，不能简单套用模型，还需要根据问题进行针对性的设计，才有可能取得更好的结果。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;这是为实验室写的，借由eMaize问题帮助大家简单了解机器学习基本方法和基础代码的教程。也可以在&lt;a href=&quot;https://lulab.gitbooks.io/bioinfo/content/5%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%95%B4%E5%90%88----%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%92%8C%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/51.html&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;这里&lt;/a&gt;看到&lt;/p&gt;
&lt;p&gt;由于jupyter notebook的强大的展示功能，本教程还用jupyter notebook组织且运行，可以获得更好的学习效果，代码在&lt;a href=&quot;https://github.com/james20141606/somethingmore/blob/master/bioinfo.ipynb&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;这里&lt;/a&gt;,欢迎取用。&lt;a href=&quot;http://localhost:4000/2018/04/12/setup/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;在这里&lt;/a&gt;我简单介绍了如何配置jupyter，在&lt;a href=&quot;https://james20141606.github.io/2018/04/10/Deep-Learning-Practice/&quot;&gt;Deep Learning tutorial&lt;/a&gt;中我也强烈推荐了jupyter，并且介绍了很多基于jupyter的资源，强烈建议尝试一下。&lt;br&gt;
    
    </summary>
    
      <category term="techniques" scheme="http://james20141606.github.io/categories/techniques/"/>
    
      <category term="machine learning" scheme="http://james20141606.github.io/categories/techniques/machine-learning/"/>
    
    
      <category term="codes" scheme="http://james20141606.github.io/tags/codes/"/>
    
      <category term="statistics" scheme="http://james20141606.github.io/tags/statistics/"/>
    
      <category term="techniques" scheme="http://james20141606.github.io/tags/techniques/"/>
    
      <category term="bioinformatics" scheme="http://james20141606.github.io/tags/bioinformatics/"/>
    
      <category term="machine learning" scheme="http://james20141606.github.io/tags/machine-learning/"/>
    
  </entry>
  
  <entry>
    <title>陈炳林回忆录 序言</title>
    <link href="http://james20141606.github.io/2018/04/11/auto0/"/>
    <id>http://james20141606.github.io/2018/04/11/auto0/</id>
    <published>2018-04-11T02:10:01.000Z</published>
    <updated>2018-04-12T15:22:53.537Z</updated>
    
    <content type="html"><![CDATA[<p>本文章同时被整理成一本小书，放置在我的gitbook账户下，点击<a href="https://legacy.gitbook.com/book/james20141606/grandpa-autobiography/details" target="_blank" rel="noopener">这里</a>可以阅读。</p><h1 id="Preface-前言"><a href="#Preface-前言" class="headerlink" title="Preface 前言"></a>Preface 前言</h1><p>这份不长不短的回忆录源自于清华大学的毛中特课程的一份作业，在一年以前就打算选择冯务中老师的课，原因就是打听了各个老师的任务，对这项任务非常感兴趣，无奈没有抢到课，但是却开始了帮助爷爷奶奶整理这份回忆录的过程。平时每个周末都会和爷爷奶奶视频聊天一两个小时，自从有了这个想法，就会专门和爷爷奶奶聊过去的故事，顺带鼓励他们动笔写一些。爷爷年轻时是县委组织部的笔杆子，虽然已经七十多岁了，听到我的鼓励也有些心动，多年未提笔，写起来却是收不住，听奶奶说爷爷经常凌晨四五点起来就开始写，边写边流泪，回忆幼年时的艰辛与不易。这份回忆录，讲到了爷爷中年时期即止，爷爷说，年轻的生活更加刻骨铭心，令人难忘，后来生活好转，一切顺利如意，倒也没什么可写了。<br><a id="more"></a></p><p>这份回忆录以爷爷为主要视角叙述，补充了很多和奶奶讨论之后获得的细节，家里过去非常的穷，留下的资料几乎为零，曾经爷爷的哥哥大爷试图整理一份家谱出来，也被爷爷的爸爸在大爷去世后烧毁，因此我们也觉得能够再整理出一些过去的故事非常有意义。这里面的一些故事朴实又动人，在我整理的过程中充满了感慨和感动，让我体会到祖辈们的艰苦和不屈的精神。有的故事还带来了意想不到的惊喜，比如爷爷专门回忆了他年轻时结识的一个好朋友周聚照，已经几十年联系不上了，我整理完爷爷的回忆录，对这位朋友印象深刻，因此自告奋勇帮爷爷联系，在几个可能的地点的百度贴吧发布帖子，真的找到了这位老人的家人。爷爷和奶奶非常激动，第二天就坐车前去看望，周聚照老人已经不在了，但是他的后代生活的很好，孙辈们都获得了很好的教育，考入了很好的大学，改变了自己的命运，真的很令人感慨。另一个故事，爷爷没有亲手写下来，但是还是忍不住告诉了我，就是回忆录的最后一个故事，关于正义的故事，这个发生于爷爷的父亲身上的真实的故事深深的震撼了我，让我对那个混乱的年代有了更深的体会。<br>最后还整理了一个简单的按照年份的时间表，还用一个专门的软件macfamilytree制作了一个这三四代人的家谱树，希望未来的家谱树可以越来越大，开枝散叶，生生不息。</p><h1 id="目录-Table-of-Contents"><a href="#目录-Table-of-Contents" class="headerlink" title="目录   Table of Contents"></a>目录   Table of Contents</h1><h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a><a href="https://james20141606.github.io/2018/04/11/auto0/">前言</a></h2><h2 id="Chapter-Ⅰ-我的童年"><a href="#Chapter-Ⅰ-我的童年" class="headerlink" title="Chapter Ⅰ 我的童年"></a><a href="https://james20141606.github.io/2018/04/11/auto1/">Chapter Ⅰ 我的童年</a></h2><h2 id="Chapter-Ⅱ-初中生活"><a href="#Chapter-Ⅱ-初中生活" class="headerlink" title="Chapter Ⅱ 初中生活"></a><a href="https://james20141606.github.io/2018/04/11/auto2/">Chapter Ⅱ 初中生活</a></h2><h2 id="Chapter-Ⅲ-难忘的一九五八"><a href="#Chapter-Ⅲ-难忘的一九五八" class="headerlink" title="Chapter Ⅲ 难忘的一九五八"></a><a href="https://james20141606.github.io/2018/04/11/auto3/">Chapter Ⅲ 难忘的一九五八</a></h2><h2 id="Chapter-Ⅳ-新的篇章"><a href="#Chapter-Ⅳ-新的篇章" class="headerlink" title="Chapter Ⅳ 新的篇章"></a><a href="https://james20141606.github.io/2018/04/11/auto4/">Chapter Ⅳ 新的篇章</a></h2><h2 id="Chapter-Ⅴ-休学的日子"><a href="#Chapter-Ⅴ-休学的日子" class="headerlink" title="Chapter Ⅴ 休学的日子"></a><a href="https://james20141606.github.io/2018/04/11/auto5/">Chapter Ⅴ 休学的日子</a></h2><h2 id="Chapter-Ⅵ-婚后的生活"><a href="#Chapter-Ⅵ-婚后的生活" class="headerlink" title="Chapter Ⅵ 婚后的生活"></a><a href="https://james20141606.github.io/2018/04/11/auto6/">Chapter Ⅵ 婚后的生活</a></h2><h2 id="Chapter-Ⅶ-喜上加喜-喜中有忧"><a href="#Chapter-Ⅶ-喜上加喜-喜中有忧" class="headerlink" title="Chapter Ⅶ 喜上加喜 喜中有忧"></a><a href="https://james20141606.github.io/2018/04/11/auto7/">Chapter Ⅶ 喜上加喜 喜中有忧</a></h2><h2 id="Chapter-Ⅷ-工作-崭新的篇章"><a href="#Chapter-Ⅷ-工作-崭新的篇章" class="headerlink" title="Chapter Ⅷ 工作 崭新的篇章"></a><a href="https://james20141606.github.io/2018/04/11/auto8/">Chapter Ⅷ 工作 崭新的篇章</a></h2><h2 id="Essays-短文数篇"><a href="#Essays-短文数篇" class="headerlink" title="Essays    短文数篇"></a><a href="https://james20141606.github.io/2018/04/11/auto9/">Essays    短文数篇</a></h2><h3 id="忆祖母"><a href="#忆祖母" class="headerlink" title="忆祖母"></a>忆祖母</h3><h3 id="忆母亲"><a href="#忆母亲" class="headerlink" title="忆母亲"></a>忆母亲</h3><h3 id="四伯家生活写照"><a href="#四伯家生活写照" class="headerlink" title="四伯家生活写照"></a>四伯家生活写照</h3><h3 id="八岁孩子学走路"><a href="#八岁孩子学走路" class="headerlink" title="八岁孩子学走路"></a>八岁孩子学走路</h3><h3 id="我的第一双棉鞋"><a href="#我的第一双棉鞋" class="headerlink" title="我的第一双棉鞋"></a>我的第一双棉鞋</h3><h3 id="大伯和父亲给祖母惹祸"><a href="#大伯和父亲给祖母惹祸" class="headerlink" title="大伯和父亲给祖母惹祸"></a>大伯和父亲给祖母惹祸</h3><h3 id="真实故事三则"><a href="#真实故事三则" class="headerlink" title="真实故事三则"></a>真实故事三则</h3><h3 id="我的朋友周聚照"><a href="#我的朋友周聚照" class="headerlink" title="我的朋友周聚照"></a>我的朋友周聚照</h3><h3 id="人生机遇只有一次"><a href="#人生机遇只有一次" class="headerlink" title="人生机遇只有一次"></a>人生机遇只有一次</h3><h3 id="过个革命化春节"><a href="#过个革命化春节" class="headerlink" title="过个革命化春节"></a>过个革命化春节</h3><h3 id="石人传说二则"><a href="#石人传说二则" class="headerlink" title="石人传说二则"></a>石人传说二则</h3><h3 id="Poems-诗歌九则"><a href="#Poems-诗歌九则" class="headerlink" title="Poems 诗歌九则"></a><a href="https://james20141606.github.io/2018/04/11/auto10/">Poems 诗歌九则</a></h3><h4 id="玩秋千"><a href="#玩秋千" class="headerlink" title="玩秋千"></a>玩秋千</h4><h4 id="石人山景"><a href="#石人山景" class="headerlink" title="石人山景"></a>石人山景</h4><h4 id="恋家"><a href="#恋家" class="headerlink" title="恋家"></a>恋家</h4><h4 id="四伯"><a href="#四伯" class="headerlink" title="四伯"></a>四伯</h4><h4 id="无题"><a href="#无题" class="headerlink" title="无题"></a>无题</h4><h4 id="思往昔看今朝"><a href="#思往昔看今朝" class="headerlink" title="思往昔看今朝"></a>思往昔看今朝</h4><h4 id="石人山下荡秋千"><a href="#石人山下荡秋千" class="headerlink" title="石人山下荡秋千"></a>石人山下荡秋千</h4><h4 id="咏春"><a href="#咏春" class="headerlink" title="咏春"></a>咏春</h4><h4 id="大竹园变堰潭"><a href="#大竹园变堰潭" class="headerlink" title="大竹园变堰潭"></a>大竹园变堰潭</h4><h2 id="Story-of-Justice-一个关于正义的故事"><a href="#Story-of-Justice-一个关于正义的故事" class="headerlink" title="Story of Justice    一个关于正义的故事"></a><a href="https://james20141606.github.io/2018/04/11/auto11/">Story of Justice    一个关于正义的故事</a></h2><h2 id="Chronology年表"><a href="#Chronology年表" class="headerlink" title="Chronology年表"></a><a href="https://james20141606.github.io/2018/04/11/auto12/">Chronology年表</a></h2>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文章同时被整理成一本小书，放置在我的gitbook账户下，点击&lt;a href=&quot;https://legacy.gitbook.com/book/james20141606/grandpa-autobiography/details&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;这里&lt;/a&gt;可以阅读。&lt;/p&gt;
&lt;h1 id=&quot;Preface-前言&quot;&gt;&lt;a href=&quot;#Preface-前言&quot; class=&quot;headerlink&quot; title=&quot;Preface 前言&quot;&gt;&lt;/a&gt;Preface 前言&lt;/h1&gt;&lt;p&gt;这份不长不短的回忆录源自于清华大学的毛中特课程的一份作业，在一年以前就打算选择冯务中老师的课，原因就是打听了各个老师的任务，对这项任务非常感兴趣，无奈没有抢到课，但是却开始了帮助爷爷奶奶整理这份回忆录的过程。平时每个周末都会和爷爷奶奶视频聊天一两个小时，自从有了这个想法，就会专门和爷爷奶奶聊过去的故事，顺带鼓励他们动笔写一些。爷爷年轻时是县委组织部的笔杆子，虽然已经七十多岁了，听到我的鼓励也有些心动，多年未提笔，写起来却是收不住，听奶奶说爷爷经常凌晨四五点起来就开始写，边写边流泪，回忆幼年时的艰辛与不易。这份回忆录，讲到了爷爷中年时期即止，爷爷说，年轻的生活更加刻骨铭心，令人难忘，后来生活好转，一切顺利如意，倒也没什么可写了。&lt;br&gt;
    
    </summary>
    
      <category term="爷爷回忆录" scheme="http://james20141606.github.io/categories/%E7%88%B7%E7%88%B7%E5%9B%9E%E5%BF%86%E5%BD%95/"/>
    
    
      <category term="life" scheme="http://james20141606.github.io/tags/life/"/>
    
      <category term="autobiograpy" scheme="http://james20141606.github.io/tags/autobiograpy/"/>
    
  </entry>
  
  <entry>
    <title>回忆录 CHAPTER Ⅰ  我的童年</title>
    <link href="http://james20141606.github.io/2018/04/11/auto1/"/>
    <id>http://james20141606.github.io/2018/04/11/auto1/</id>
    <published>2018-04-11T02:10:00.000Z</published>
    <updated>2018-04-12T07:56:57.045Z</updated>
    
    <content type="html"><![CDATA[<p>我出生在石桥街西夹后布袋街外祖母家那个巷子里，是一九四零年(民国二十九年)生，赶上三十年年成那年。我出生后家里有四口人，大哥已经两岁。在集镇上住，家里没地没房，不做生意，生存十分困难。后来经人介绍，父亲用卖菜的筐一头一个孩子，挑着我们去白河东沙山给地主彭山种地。地主给了草房两间，几亩薄地，生活勉强过得去。日本侵华后战乱频起，又逢灾年（指1942年七月到1943年春天的那场大灾荒，河南受灾总人数达1200万人，约三百万人死亡），祖母不愿骨肉分离，我们一家四口只好又两手空空搬到祖母借住地薛庄去（魏庄西边西边的那个庄）。<br><a id="more"></a><br>1941年春天母亲得了一场大病（奶花疮），那时我才不足一周岁，正值三十年年成，没饭吃也没奶喝，眼看着就要饿死。父亲只得将家里的一床被子和一条床单带上，徒步到老河口换点吃的。当他第三天凌晨回到沙山家里时，我和母亲两人已经两天没有吃东西了。父亲忙生火做面麸汤面水救命，我竟一口气喝了三大碗，肚子撑得得鼓鼓的，父亲说当时我肚子上的青筋都能看到，我还想再喝一些，母亲坚决不要我再喝，说否则会把人撑死，还是母亲心细。这两升麸面可是救了我的命啊。</p><p>再一次搬回薛庄后，祖母，大伯，爹妈，大哥和我六口人没吃没喝，据说那时候能够食用的榆树皮都被剥光了，树枝、豆科的角皮都吃，人吃了以后拉不出来就用竹签剜，母亲说我当时就用的这个办法，吃饭已艰难至此，总不能全家一起饿死吧，为了减轻压力，祖母带着自己平时少言寡语，木讷死板的大儿子，也就是我的大伯远走他乡要饭去。为了大家的生存，大伯也只能跟着祖母要饭去了。母亲说，为了不让我饿死，她只好把我的大哥也送到外公家，留下我自己一个孩子。好心的黄奶（她有一个终身未娶的儿子留在身边）每顿煮菜汤的时候剩下的饭跟都会让我喝，两家人的饭跟救活了我，黄奶也是我的救命恩人！</p><p>奶奶领大伯远走他乡本身就够难了，大伯一个大男人实在是委屈，进村后他就站在树下或者墙根处，不愿进院子里。可是这样怎么能讨得来饭呢。于是每顿都是祖母先讨来饭让大伯吃，吃的差不多了再去要几口饭自己吃，如果要不到，两个人就只能饿肚子。她要饭不是为了大儿子，不是为了她自己，她带走大儿子，留下我们，是为了留下家族的火种。饿死大伯只饿死一个，饿死我们一家人，不知道陈家还能不能延续下去。祖母说：“人留子孙，草留根”，当时的我们，真可谓是离离原上草，一岁一枯荣；野火烧不尽，春风吹又生了。</p><p>灾荒终于过去，祖母和大伯回家了，大伯没有饿死，祖母没有饿死，我也没有饿死，大家都没有饿死，陈氏家族总算有一线希望了。祖母很伟大，大伯的牺牲值得铭记，我还是大伯的过继儿，大伯的恩情我不会忘记。</p><p>解放前，薛庄有个大地主叫做郭老八，大名并不记得了。他每日都搬一个大圈椅，坐在槐树下纳凉，经常自言自语道：我儿强似我，要钱做什么；我儿不如我，要钱干什么？此人精明算计，在土改前他竟然大肆低价变卖土地、房屋，挥霍家产，到了土改时家产变卖一空，竟然成了贫农，我们才知道原来是人家外边有人，提前知道了形势，想躲过一劫。我的祖母、长辈因长期地无一分、椽无一根，困苦惯了，深受压榨剥削，为了有两亩地，竟然在土改的前一年全家人节衣缩食，纺花织布，买下了两亩薄地。结果第二年就土改了，你说傻不傻，没有知识和文化，真是命苦啊！因我们家里太穷，土改时定为雇农，因此将地主李氏南家最好的三间大瓦房分给我们，后来因为此处没有庭院，所以将瓦房推倒，在现在的老家旧居所用这些砖瓦盖了新房。当时还分得一张核桃木雕花木床，两把圈椅，木床不幸遗失，两把圈椅送去了寺庙中。</p><p>一九四八年，八周岁的我的得了天花，发高烧，没有吃的也没有药医，后来竟然下不了床，不会走路了，于是大伯经常用长腰带绑着我带我重学走路，这次大难不死，没有落下什么大毛病，只是让我的体质变得特别差，这也是我一生体质不好的原因。</p><p>我八岁到十岁的几年主要跟随奶奶去石人沟四伯家生活，他那里吃饱饭没有什么大问题，那几年我终身难忘，有苦有乐，有悲有喜。一九五零年的春天，我开始在尹店小学读书，三年后转学到皇路店完小上学。那时候的学校不布置作业，家里又没有一个识字的人，学的怎么样谁又知道呢？</p><p>抽空拾柴捡粪是我那时候最喜欢干的主业，当时土改给我家分了几亩地，我做梦都想买来一头牛耕地，攒粪让庄稼长得好，能够有吃有喝站到人前，这就是我当时的梦想了！父亲答应了我，买了一头全身黑色的小母牛娃，条件是我不能因此耽误了上学，这头小牛完全由我负责，我当然无比爽快地答应。每日上学和放学路上，我都不走大路，一定要从田间地埂走，割青草喂我心爱的小牛，我爱我的小牛就像现代人爱自己的宝马车一样，这牛是我的希望呀。夏季牛拴在外边，我就把床铺到它的附近，生怕有人晚上偷走它，我上学、养牛两不耽误，把小牛照顾得很好。两年之后，它产下了一头小牛犊，这可把我高兴坏了，生牛肚那晚，我一夜未眠，我想：梦想要实现了，我家现在有两头牛了啊！</p><p>既上学又养牛已经够忙活了，后来小我四岁的弟弟炳义也开始在皇路店上小学了，刚吃过饭去上学还好，他有劲就自己走，等放学回家肚子饿了，他就坐在地上耍赖，非要我背他回去，作为哥哥我就只能背着他走一段路，这可耽误了我割草了啊。</p><p>那时候家里穷，没有任何防寒、避雨的工具，夏季还好办，到了冬天，一下雪可就惨了，我只能把鞋子脱下夹在腋下（只有一双鞋，不能弄湿），脚总是会冻得钻心地痛。</p><p>小学离家有两公里地，下雪的时候就干脆不回家吃午饭了，等晚上路上结冰时再穿鞋回家，当然也是因为中午回家也没什么好吃的，不回去还能少一趟冻脚之苦。我从小喜爱咸饭，若是做了咸面条，妈绝对会给留一大碗。有时候母亲也会托人中午捎来一点红薯面馍，啃一口一个白印。吃红薯太多伤胃，我也因此落下了终身的胃病。</p><p>一九五六年小学毕业后我成功考入石桥镇上的初级中学，当时小升初率不高，所以我真是很幸运。我小学时的班主任张文彬老师亲自步行来家里送录取通知书。全家都高兴极了，家里有中学生啦，就像中了状元似的。父亲一定要留张老师吃午饭，老师答应了，那顿饭也就算是谢师宴了吧！后来我也和张老师成了好朋友，忘年之交了。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;我出生在石桥街西夹后布袋街外祖母家那个巷子里，是一九四零年(民国二十九年)生，赶上三十年年成那年。我出生后家里有四口人，大哥已经两岁。在集镇上住，家里没地没房，不做生意，生存十分困难。后来经人介绍，父亲用卖菜的筐一头一个孩子，挑着我们去白河东沙山给地主彭山种地。地主给了草房两间，几亩薄地，生活勉强过得去。日本侵华后战乱频起，又逢灾年（指1942年七月到1943年春天的那场大灾荒，河南受灾总人数达1200万人，约三百万人死亡），祖母不愿骨肉分离，我们一家四口只好又两手空空搬到祖母借住地薛庄去（魏庄西边西边的那个庄）。&lt;br&gt;
    
    </summary>
    
      <category term="爷爷回忆录" scheme="http://james20141606.github.io/categories/%E7%88%B7%E7%88%B7%E5%9B%9E%E5%BF%86%E5%BD%95/"/>
    
    
      <category term="life" scheme="http://james20141606.github.io/tags/life/"/>
    
      <category term="autobiograpy" scheme="http://james20141606.github.io/tags/autobiograpy/"/>
    
  </entry>
  
  <entry>
    <title>回忆录 CHAPTER Ⅱ  初中生活</title>
    <link href="http://james20141606.github.io/2018/04/11/auto2/"/>
    <id>http://james20141606.github.io/2018/04/11/auto2/</id>
    <published>2018-04-11T02:09:59.000Z</published>
    <updated>2018-04-12T07:57:06.487Z</updated>
    
    <content type="html"><![CDATA[<p>入学后我被分到了56丁班，和我后来的妻子是同班同学，她从小学就学习刻苦，成绩优秀，是初中部的学习部长，那时我们是纯洁的同学关系，很少有接触，几乎没有怎么说过话。<br><a id="more"></a></p><p>上初中时正值全国性反右的高潮，一九五六年在庐山会议上本来要纠左，彭德怀上了万言书，反而被毛泽东定性为反党集团，全国轰轰烈烈的反右运动开始了。学校教师有将近一半被划为右派，初中生虽然不划右派，但是对个别学生的言论也进行批判，令其退学。比如薛庄村学生王慧敏受运动刚开始时大鸣大放的影响（即向党提意见），她草书“向毛主席开一炮”，甲班学生柏长松在教工厕所门口写了“屎不一样”，抨击教师和学生生活水平不同，学生生活水平不好。这两人被查出来之后，被全校批判，勒令退学，开除学籍，丧失了继续学习的权利。教职工、学生都被这样的政治空气笼罩着，多数老师被划为右派，可我的班主任贾之广是反右运动中的左派，一副盛气凌人的气度，在丁班大搞紧张气氛，把我和几个要好的同学打成所谓的“小集团”。中央有彭、黄、张、周反革命集团，而我们这小集团是什么性质？我们这些十几岁的娃娃根本不懂，班主任这样做为了什么，我们也不懂。他组织全班同学揭发我们，每晚组织班干部揭发批判，揭什么呀，批什么呀！</p><p>小集团成员是不得随意离校的，离校要遵循请假制度，我们这些人（曹成才、翟清合、王明跃、王松林、雷清茂等人），一举一动都会被人监视着。我当时没钱在大伙就餐，只能立小灶，这小灶可不是现在改善生活的那种，我每周都得回家里挑做饭要烧的柴火，玉米糁和红薯等，可是找班主任请假，他不批准，这可让人怎么生活呀！他越是这样，我们这些好友靠的越紧，凑空就在一起互相诉苦，心情也是越来越差，气氛越来越紧张。</p><p>请不下来假，不回家拿吃的可如何上学？一次周六下午我写好了请假条让贾批准，天色已晚，学生们已经离校了，因为紧张我忘记敲门，直接推门进去了，这时我亲眼看见一个年轻的女教师正仰面躺在他的床上，二人正低声私语。这实在是够尴尬了，可我没有退出来，拿着请假条死缠着要回家，这举动自然是激怒了他，他要我出去，我想着反正他没有把我关起来，也没说不准我回家，我就摸黑偷偷溜回家了。到家后我把当时的情况讲给了父亲，他怒火中烧，说要找贾评理，。我当时不懂，不但给父亲说了此事，也对小集团的成员们讲了贾的所作所为，大家都极其不满，消息估计也传开了，让贾也知道了。父亲出于愤怒竟然也去学校找贾，因此师生间矛盾再次升级，我的对手可是年近四十的人，在国民党电台任过台长，还是反右左派，斗争经验丰富，我就像一个不会反抗的犯人一样，被牢牢地控制住了。</p><p>由于我精神压力巨大，不久就得了一种怪病：正走路时眼前一黑，头一晕，腿发软，就向前摔去，摔得鼻青脸肿。不用班主任勒令我回家，我自己也只得休学了！</p><p>贾的恶作剧并没有结束，他极其会戏弄人，命令小集团的成员用学校拉垃圾的车子拉我回家，路上这群无知的青年竟然还兴高采烈，欢声笑语，驾着垃圾车飞跑，好像我刚从恐怖的监狱里逃出来一样高兴。不管怎样，我休学回家了，当时我并不知道，学生会学习部长王若平也因小学的时候和小集团成员曹成才同班同学（年龄比她大两岁，是小集团首领），被威逼要揭发小集团，她哪知道揭发批斗什么，当然什么都说不出来。后来可恶的贾芝庞组织其他干部把矛头全对准她，向她施压，批判她，她没有经受得住，精神也不正常了，在我休学之前她就已经休学啦，被人送回家养病。结婚后我们两个谈及此事，真觉得不可思议，上帝究竟是如何安排的，让我们两个经历这九九八十一难啊。</p><p>紧接着河南又刮起了一阵反潘、杨、王风，直指潘复生，杨珏，王庭栋。纠左开始了，这才把贾芝庞给揭露了出来，将他绳之以法，判了三年刑。</p><p>被判小集团期间我心情极差，休学回家治疗了一个多月，是尹店中医李连奇给治的，每天一副中药，药引是童便。病治好后该返校了，但是大哥突然罹患脑膜炎病逝，年仅二十岁。当时他已订好婚，五八年底就该结婚了，大哥的猝然离世对家人的打击太大了，父亲因此变得精神不太正常，不思茶饭。在身体极度虚弱的情况下，大队硬是派他去云阳铁牛庙水库做工，在水库父亲也累得病倒了。</p><p>我的病刚好，大哥离世，父亲在水库也病倒了，一九五百年真是灾难性的一年，是黑色的一九五八。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;入学后我被分到了56丁班，和我后来的妻子是同班同学，她从小学就学习刻苦，成绩优秀，是初中部的学习部长，那时我们是纯洁的同学关系，很少有接触，几乎没有怎么说过话。&lt;br&gt;
    
    </summary>
    
      <category term="爷爷回忆录" scheme="http://james20141606.github.io/categories/%E7%88%B7%E7%88%B7%E5%9B%9E%E5%BF%86%E5%BD%95/"/>
    
    
      <category term="life" scheme="http://james20141606.github.io/tags/life/"/>
    
      <category term="autobiograpy" scheme="http://james20141606.github.io/tags/autobiograpy/"/>
    
  </entry>
  
  <entry>
    <title>回忆录 CHAPTER Ⅲ  难忘的一九五八</title>
    <link href="http://james20141606.github.io/2018/04/11/auto3/"/>
    <id>http://james20141606.github.io/2018/04/11/auto3/</id>
    <published>2018-04-11T02:09:57.000Z</published>
    <updated>2018-04-12T07:57:23.193Z</updated>
    
    <content type="html"><![CDATA[<p>父亲病倒后水库指挥部通知家人，去云阳水库接回父亲。家里只有我可以担当此仁，此时我已经十八岁，刚刚成年。接到通知的时候我害怕极了，父亲可千万不能出事啊。我恨不得自己能赶快长出一双翅膀飞过去接他回家。<br><a id="more"></a><br>第二天一大早，我拉着架子车就上路了，开始时我是跑着拉车，走过鸭河之后就心有余而力不足了，上坡路是绝对跑不动了，但是心里着急，还是一刻不停地走着，一天只喝了几次水，没有进食任何东西，经过一天的奔波，晚上才到云阳，因为是头一次去，走的晕头转向，肚子一天没有吃东西却也不觉饿，只是口渴难耐，说不出话。经过打听之后得知还有十公里路，没有正路可走，步行沿着河边走倒是可以快一点，但是车子放哪里呢？没有车子第二天如何拉父亲呢？可是我太急于见到父亲确定他没事，人受挫折武艺高，我找了一家修车铺，放掉车胎气，自述架子车放了炮，得补胎，我把架棚靠人家店外，下盘搬进屋子里，说好了明天来拉车付款。我顾不得吃晚饭，就按照人家指的路线摸黑去水库。天黑路生，河里不时发出响声，好像是鱼儿拍打水面，没有出过远门，没有一个人走过夜路的我，自然情绪紧张，稍有风吹草动就会东张西望。我越紧张，就越觉得背后有什么东西在跟着我，我吓得头发梢支棱着，然而再怕也不能停下，我只能一直往前行，大概在晚上十一点左右终于看到了灯光，那是水库指挥部所在地了。</p><p>指挥部工作人员刚吃过夜宵，我向负责人说明情况后，他叫炊事员把剩下的一大碗甜面片给我吃，还给我拿了几根蒜薹，饭已经凉了，可我终于有了极大的饿意，吃的狼吞虎咽、风卷残云，几分钟就吃完了，噎得直打嗝。他让我找个空铺睡下休息，并通知工地，让父亲第二天一早过来。我哪里睡得着啊，只盼着早点见到父亲，父亲接到指挥部通知说儿子在指挥部等他，已经卧床的他竟然起身一个人摇摇晃晃地往指挥部走。次日拂晓我和父亲终得相见，我们像久别重逢一般抱头痛哭，父亲安慰我说，见到你我的病就轻得多啦。我的泪水不光是因为见到了父亲，酸甜苦辣咸五味俱全，这几年我都体验了一遍啊，心中的苦楚一起涌上心头：小集团、生病、休学、兄长病殁、父亲精神失常、小弟炳都得转头虱病。正所谓男儿有泪不轻弹，只是未到伤心处。</p><p>太阳冉冉升起来啦，我和父亲一起，扶着他向云阳走去，在停架子车的店铺说明情况后取出架子车，走进旁边的一家食堂。他说：“今天我们父子俩吃炝锅面，再买一斤油条，加进去美美气气吃一顿。”父子见面，他的病好了一些，我折腾了一天一夜，但是心里轻松得多了，我只觉得自己腿酸乏力，因为自己也才病愈，又一口气跑了那么远。吃完饭，我强打精神，他坐车我拉车，往回家的路走，边走边聊。父亲见我也是强打精神，他说上坡时我下来，下坡时我再坐车，走一阵咱俩换换，我拉车你坐车。听父亲这么说，我不怕了，父亲还能活！当天夜里我们顺利到家了。别了，黑色一九五八！！</p><p>当父亲处理完这些事情后，就带着我复学，他先领我见了四中当时的校团委书记和宝兴，请他多关照我，和宝兴家在皇路店沽沱村，不知道父亲是怎么认识他的。复学前后还在学校经历了勤工俭学、大炼钢铁、白河淘铁砂（炼钢用）等运动，我的求学路可谓一波三折啊，孙辈们是难以想象的！</p><p>勤工俭学：五人一辆独轮木质手推车（当时称之为小车），去南召三岔口买柴，因为没有大路走，只能从全是乱石的河滩里推着走，我们披星戴月，买柴卖柴，赚了多少钱已经记不清啦。</p><p>炼钢铁：就在学校西南角空地处放一个炼铁炉，大风箱得六七个人同时拉才能拉动，炉子里放的是铁锅、铁盆、生铁之类的铁制品，各种废铁堆在一起烧，至于能烧出多少铁根本没有人管它，这样荒唐的事当时正在神州大地的每个角落发生，在大炼钢铁的过程中，南阳县四中的全体学生也都停课了。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;父亲病倒后水库指挥部通知家人，去云阳水库接回父亲。家里只有我可以担当此仁，此时我已经十八岁，刚刚成年。接到通知的时候我害怕极了，父亲可千万不能出事啊。我恨不得自己能赶快长出一双翅膀飞过去接他回家。&lt;br&gt;
    
    </summary>
    
      <category term="爷爷回忆录" scheme="http://james20141606.github.io/categories/%E7%88%B7%E7%88%B7%E5%9B%9E%E5%BF%86%E5%BD%95/"/>
    
    
      <category term="life" scheme="http://james20141606.github.io/tags/life/"/>
    
      <category term="autobiograpy" scheme="http://james20141606.github.io/tags/autobiograpy/"/>
    
  </entry>
  
  <entry>
    <title>回忆录 CHAPTER Ⅳ  新的篇章</title>
    <link href="http://james20141606.github.io/2018/04/11/auto4/"/>
    <id>http://james20141606.github.io/2018/04/11/auto4/</id>
    <published>2018-04-11T02:09:55.000Z</published>
    <updated>2018-04-12T08:11:41.163Z</updated>
    
    <content type="html"><![CDATA[<p>我在一九五九年夏季初中毕业，学校要求毕业生要主要报考高中、师范。高中是绝对上不起的，当教师是臭老九，经过反复的斟酌，我选择了当时自己觉得最受人崇拜的专业：农机化专业，这可是毛主席提出的四化之一，就这样我顺利录取到了郑州农机化专科学校（1960年，郑州农业机械化专科学校并入河南农学院（现河南农业大学），组建河南农学院农业机械化分院），我仿佛已经能够想象出自己开着铁牛奔驰在祖国广阔原野上的场景了。<br><a id="more"></a><br>那年全校有毕业班四个，录取农机化专业的一共有四个人，甲班的王文武，丙班的丁立志，以及丁班的我和王若平。九月份就要开学啦，我们的学校在省城郑州，郑州在哪里，路有多远，要坐汽车和火车才能到达，这些对于没出过远门的我（在石桥上学期间只到过南阳城区一次，参观李花庄铁牛耕地，面粉厂的大型面粉加工设备）来说，真是无比新鲜，更何况就要长期离开父母，相隔好几百里呢。去郑州要结伴，毕竟人多智谋广啊，我觉得约上同班同学，又录取同校的王若平是首选，我估计她也会首选我结伴同行。经过打听得知，郑州农机化专科学校和郑州计划经济学校在同一条路上（农业路），距离很近，为此又约上录取该校的赵玉珍，闫学珍结伴。</p><p>行动路线、时间、人员集合地点等确定后，我们头一天先各自步行至南阳，晚上住在闫学珍的亲戚家（地址在老一中东，现在的市第八小学附近）。第二天一大早我们乘坐南阳开往许昌的代客车（也就是货车上加一个帆布篷，车厢上固定有大连椅），到达许昌汽车站已经是下午了，经过购票排队，在火车站排队候车，终于坐上了一辆从三门峡至许昌的火车，我们于次日凌晨抵达省城火车站。经过打听得知去北郊（那时农业路附近是郑州的郊区）那趟公交车是烧木炭的车，夜里无法发车，于是我们决定坐三轮车去学校。其实当时学校在火车站设有接待站，有车接我们，可是我们奔波两天，初到省城，火车站又乱，竟然没有注意到。</p><p>从偏僻的农村到南阳首府，到许昌，到河南的省会，全国交通大动脉，漫长的路程，全新的景象，我心中的那份心情可想而知，生活，全新的生活就要开始了！</p><p>入校之后学校首先对我们进行了专业教育，介绍学校环境：大礼堂、小礼堂、校医院、图书馆、教学大楼、大体育场等，学校有高水平的篮球队，棒球队和排球队，学校的业余剧团，还能去校外演出的剧团。每周末学校都会放电影，有各种球队比赛等娱乐活动。学校有大型实习工厂，车钳电焊，车床设备齐全，所加工的游标卡尺，缸盖修理等工艺具有很先进的水平，可以当做商品售卖，有订单，能赚钱。大小汽车数量，实习用各型号链式和轮式拖拉机齐全。学校还有菜地百亩，黄河滩可耕地几百亩，每年收成很多小麦，伙食是八人一盒菜，而且吃饭不限量，管理生活的副校长在大礼堂讲话时宣布，要把大家都养成大胖子。</p><p>学校不收书本费、生活费，每人每月12.5元，除了医疗费1.5元外全是生活费。生活、学习、环境全都令人满意，四中时期的阴影全都抛到脑后了，那里的生活是崭新的，前途是无量的，只等毕业分配工作，当干部，拿工资，养家糊口了。</p><p>然而好景不长，一九六零年的秋季，左倾思潮翻涌，阶级斗争、政治运动席卷全国，粮食生产急剧下降，人民群众的生活水平降到了最低水平线（1960-1962年发生了著名的三年自然灾害，河南等粮食产地受灾尤为严重）。省城告急了，就要断炊了，刚开始学校提出“低标准，瓜菜代”，用东北运来的大豆面加上黄河滩的蒲草做窝窝头吃，稠一点的大米饭用秤称。再往后，火车运到郑州的粮食竟然来不及开进粮店，大专院校的车就已经开到火车站拿着粮本开始接货了，学校再也坚持不下去了，郑州也坚持不下去了，唯一的方法是减少城市人口，减少不种地只吃饭的人的数量，于是在六一年，人口集中的大专院校全部停办，以减轻省城压力，农村是广阔的天地，我只能重新回到广阔的农村了，我重新回到了阔别两年的家。</p><p>虽然能见到久别的父母，可是回家的心情一点都不愉快。我心事重重，学校宣布放长假，有多长？还能回校吗？连粮户关系（指粮食与户口关系，当时有农业与非农业两种户口）都又非转农了，还有机会农转非吗？看来只能做一辈子的农民啦。你放假，我放假，结伴回老家。又是经历同样的交通路线，经过一天一夜的折腾才到南阳，那时又是凌晨，我和同学王若平背着行囊，低头不语，步行走在回家的路上。天亮才走到蒲山殷庄她的家，该歇歇脚了。她慈祥的妈妈为我俩做了一顿削过皮的红薯白面疙瘩，接待诚恳热情，她哪里知道接待的人竟是她未来的女婿呀！那可是三年自然灾害期间，这顿饭我终生难忘，饿了甜似蜜，不饿蜜不甜啊，削过皮的白面面疙瘩，又香又甜不塞牙，怎么能轻易忘记呢，这是我在若平家吃的第一顿饭。</p><p>结伴同行去郑，结伴同行回家，感谢您招待的这顿饭，感谢我的老同学，再见吧老同学，再见！不送啦！我又迈步向家的方向走去，可我像霜打了一样，无精打采，徒步走回魏庄的家里。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;我在一九五九年夏季初中毕业，学校要求毕业生要主要报考高中、师范。高中是绝对上不起的，当教师是臭老九，经过反复的斟酌，我选择了当时自己觉得最受人崇拜的专业：农机化专业，这可是毛主席提出的四化之一，就这样我顺利录取到了郑州农机化专科学校（1960年，郑州农业机械化专科学校并入河南农学院（现河南农业大学），组建河南农学院农业机械化分院），我仿佛已经能够想象出自己开着铁牛奔驰在祖国广阔原野上的场景了。&lt;br&gt;
    
    </summary>
    
      <category term="爷爷回忆录" scheme="http://james20141606.github.io/categories/%E7%88%B7%E7%88%B7%E5%9B%9E%E5%BF%86%E5%BD%95/"/>
    
    
      <category term="life" scheme="http://james20141606.github.io/tags/life/"/>
    
      <category term="autobiograpy" scheme="http://james20141606.github.io/tags/autobiograpy/"/>
    
  </entry>
  
  <entry>
    <title>回忆录 CHAPTER Ⅴ 休学的日子</title>
    <link href="http://james20141606.github.io/2018/04/11/auto5/"/>
    <id>http://james20141606.github.io/2018/04/11/auto5/</id>
    <published>2018-04-11T02:09:53.000Z</published>
    <updated>2018-04-12T08:11:53.357Z</updated>
    
    <content type="html"><![CDATA[<p>回乡后的我很快就适应了农村的生活，我是地道的农民的儿子，但是我心里还是惦记着上学时的生活，惦记着我的老同学，他们现在生活的怎么样，他/她们在干什么呢？老家尹店每年农历四月初八的庙会规模大，总是有两台戏，约老同学来赶庙会、叙叙旧应该是不错的选择！于是同班同学王若平和赵玉珍应约前往，逛完庙会又去了我的家里，父母热情地接待了她们，也让她们感动不已。<br><a id="more"></a><br>同样的命运，同样的环境，同样的人生遭遇，五年的同窗同学生活，我和王若平见面有太多想说的，有太多的共同语言，我们倾诉着各自的遭遇、生活中的喜怒哀乐，缘分就是这么奇妙，所谓日久生情，时间久了，我们的关系从普通的同学关系，变得不太一样了，内心仿佛多了些什么？</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;回乡后的我很快就适应了农村的生活，我是地道的农民的儿子，但是我心里还是惦记着上学时的生活，惦记着我的老同学，他们现在生活的怎么样，他/她们在干什么呢？老家尹店每年农历四月初八的庙会规模大，总是有两台戏，约老同学来赶庙会、叙叙旧应该是不错的选择！于是同班同学王若平和赵玉珍应约前往，逛完庙会又去了我的家里，父母热情地接待了她们，也让她们感动不已。&lt;br&gt;
    
    </summary>
    
      <category term="爷爷回忆录" scheme="http://james20141606.github.io/categories/%E7%88%B7%E7%88%B7%E5%9B%9E%E5%BF%86%E5%BD%95/"/>
    
    
      <category term="life" scheme="http://james20141606.github.io/tags/life/"/>
    
      <category term="autobiograpy" scheme="http://james20141606.github.io/tags/autobiograpy/"/>
    
  </entry>
  
  <entry>
    <title>回忆录 CHAPTER Ⅵ  婚后的生活</title>
    <link href="http://james20141606.github.io/2018/04/11/auto6/"/>
    <id>http://james20141606.github.io/2018/04/11/auto6/</id>
    <published>2018-04-11T02:09:51.000Z</published>
    <updated>2018-04-12T08:11:58.734Z</updated>
    
    <content type="html"><![CDATA[<p>燕尾山拾柴历险记</p><p>经过双方父母见面，我们这两个老同学确立了夫妻关系，于一九六二年农历七月十一日喜结良缘，开始了又一段崭新的生活。我不仅是一名失学的学生，农村的农民，还是一名丈夫，而且很快，就要成为父亲了。</p><p>一九六二年的农历闰四月，家里连一点可以吃的东西都没有了。已经怀了身孕的妻子只能靠着白水煮红薯干充饥，想吃点别的什么食物都是奢望。老家每年四月八号的庙会总会有亲朋好友和同学们来家里，断炊了还拿什么招待呢？我决定跟随邻居，去燕尾山拾柴变卖来赚些钱买点粮食。<br><a id="more"></a><br>为什么偏要去燕尾山呢，因为那座山较险峻，海拔高，很少有人涉足，所以山上容易弄到枯枝烧柴。我们出发的时候只带了能吃七天的玉米糁，脚穿妻子给我做的新布鞋。南召人说：南召到路上，七十二道脚不干。前四月山区还有些冷，河水依然冰冷，一会脱鞋淌水，一会儿穿上走山路。上山的时候要拉着荆藤向上爬，下来可就难多了，所谓上山容易下山难之说，这时才体会的格外清楚。夜晚露宿在山岔里，盖那床又脏又薄的褥子。四月的山区，夜间比平地还要冷上好几度，晚上睡觉是怎么也暖不热的。</p><p>玉米糁吃光了，拾到的柴还不够一车，没东西吃了，我和邻居李连有大叔只能饿肚子。李大叔不知道怎么想到办法，从山区的支部书记老婆那里弄来了免费的玉米糁，不过是发了霉的，像蚂蚱牙似的，不管怎么样，吃的问题总算是解决了。</p><p>支部书记夫人四十多岁，穿着扎花鞋，还用红头绳扎着头发，有着山里人特有的气质。我们一日三餐都吃着她提供的发霉了的玉米糁，饿了甜似蜜，填饱肚子第一，我们也不挑。不知道那位支书夫人是不是出于可怜之心，第二天又给我们送来了发粘的芥菜丝让我们陪着饭吃，李连有大叔竟然感动得跪地磕头致谢，场面令我也感到震撼，我也从内心深处感谢这位女菩萨的施舍。</p><p>终于拾够了一整车的柴，我们满载而归，上山坡时你推我拉，互相帮助，饿了找背风处生火做饭，天黑人困了找山坡背风处睡觉。遮风避雨的小宿舍几分钟就可以搭建好，我们也没有功夫像现在的野营爱好者那样花那么多力气。用拉车的人常备的支杆将车把支牢固，取下小铺盖，裹上被子于车下即可酣然入睡好解乏。不失眠，超过席梦思，赛过金銮殿。</p><p>这趟北山拾柴历经十天，竟卖了六斤肉（瘦了六斤），脸瘦了一大圈，出发时穿的那双新布鞋也已经破了，脚跟露在外边了。全家人在家吃着清水煮红薯片等着我，而我上山拾柴那几天，想吃红薯片汤都难啊，当时的生活就是艰难至此。</p><p>苦味是五味之首，有苦才有甜，这是最质朴却深刻的人生哲理。卖掉一半柴火用来购粮食，另一半留下烧锅，顺利地度过了难关。这年五月初一，我的长子陈华军出生了，你说我的心里该有多甜啊，这样的苦难过后的甜，真是世上最甜的滋味了。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;燕尾山拾柴历险记&lt;/p&gt;
&lt;p&gt;经过双方父母见面，我们这两个老同学确立了夫妻关系，于一九六二年农历七月十一日喜结良缘，开始了又一段崭新的生活。我不仅是一名失学的学生，农村的农民，还是一名丈夫，而且很快，就要成为父亲了。&lt;/p&gt;
&lt;p&gt;一九六二年的农历闰四月，家里连一点可以吃的东西都没有了。已经怀了身孕的妻子只能靠着白水煮红薯干充饥，想吃点别的什么食物都是奢望。老家每年四月八号的庙会总会有亲朋好友和同学们来家里，断炊了还拿什么招待呢？我决定跟随邻居，去燕尾山拾柴变卖来赚些钱买点粮食。&lt;br&gt;
    
    </summary>
    
      <category term="爷爷回忆录" scheme="http://james20141606.github.io/categories/%E7%88%B7%E7%88%B7%E5%9B%9E%E5%BF%86%E5%BD%95/"/>
    
    
      <category term="life" scheme="http://james20141606.github.io/tags/life/"/>
    
      <category term="autobiograpy" scheme="http://james20141606.github.io/tags/autobiograpy/"/>
    
  </entry>
  
  <entry>
    <title>回忆录 CHAPTER Ⅶ 喜上加喜 喜中有忧</title>
    <link href="http://james20141606.github.io/2018/04/11/auto7/"/>
    <id>http://james20141606.github.io/2018/04/11/auto7/</id>
    <published>2018-04-11T02:09:49.000Z</published>
    <updated>2018-04-12T08:12:06.547Z</updated>
    
    <content type="html"><![CDATA[<p>我和若平两人背景相同，志同道合，结婚后朝夕相处，可以永不分离，两个人形影不离，生活觉得甜蜜极了，结伴扛着农具干活去，在乡下的生活倒也过得有滋有味。我的长子陈大军出生后，全家都高兴坏了。父亲说，长子得了长孙，家里人丁兴旺了起来，再困难也得办几桌酒席，让亲朋好友齐聚一堂，共享快乐。父亲是非常能干厉害的人，在那么困难的条件下（三年灾害刚刚过去），操办了全席酒宴，我由衷地感激父亲。<br><a id="more"></a><br>开席了，村里面很多年没有人家办全席了，男女老幼满庭院，孩童们将白馒头偷偷揣进怀里，大人们猜枚行令，真是热闹非凡。</p><p>两点多啦，酒宴临近撤席，有的亲戚朋友到院外互相问候、攀谈，准备道别。女人们争相到妻子床边亲眼看一看儿子的容貌。为了准备这十几桌酒席，我忙活了好几天，这会是个空隙，我搬个小凳子坐在大门外左侧池塘边那棵桃树下稍事休息，成家啦，得子啦，还是个小子呢！我忍不住开心地笑着。这棵桃树桃儿满枝头，硕果累累，这树是我亲手所栽，一点点培育长大的呢！农家人的快乐、幸福与满足一齐涌上心头，多久都没有这么快乐过了呢！</p><p>正想着心里的事，有两个推着自行车的人朝大门口走去，询问陈炳林、王若平两人是否住在这里。我抬头望去，两个人的面孔好生熟悉，这是哪里来的客人呢，怎么这个时候才到，我该怎么招待呀。再仔细一看，原来是郑州学校的班主任何老师和教汽托课的邢仁军老师，我赶忙跑步过去迎接。我心里震惊得说话都语无伦次了，我说，老师您吃饭了没有，您咋知道我和王若平的家在这里？在校时我俩是他们的学生，现在我们已经成为了夫妻，孩子都出生了，正好赶上办酒席，真的有点尴尬。</p><p>邢仁军老师教汽车拖拉机课，他语言精练易懂实践经验丰富，实习课上他经常让我操作示范给大家看，对我器重有加，青眼相看，我也非常喜欢他，他是我心中的偶像。班主任和我最喜欢的老师来到家里，自然是非常激动。</p><p>接下来的消息更是让人激动不已，老师说：祝贺你们双喜临门，学校也要复课啦，这是入学通知书，交给你们。我双手接住了两份沉甸甸的通知书。</p><p>事前没有任何的讯息，二位老师又千里迢迢将入学通知书递到了手里，我既惊喜又感激。我说：老师，我和王若平已经结婚生子了，学校还会要我们吗？老师的回答是坚定的：这个问题是国家造成的，你们没有过错，学校会接收你们的。你们在校时的表现我是知道的，王若平还是班干部，学生会的学习委员哩，放心吧，学校一定会收你们的！</p><p>我校能复课开学，乃是占了“农”字的光。因着毛主席提出“农业的根本出路在机械化”的批示得以率先复课，其他的院校都还没消息呢。</p><p>瞬间，地道的农民又可以上学啦，可以重新当城里人，读书人了。做了爸妈的人要去上学啦，儿子怎么办呢？在农村出了这样的新鲜事，你说稀奇不稀奇？</p><p>距离开学只剩下两个月了，谁去谁留？都去都留？欣喜之后是烦恼，这个问题始终困扰着我俩，萦绕在我们心头。全家人也争论不休，我们征求了岳父母的意见，也和内兄写信联系，大家的意见是，都去上学。岳父母资助了一些费用，内兄也汇来了钱款。再难也得坚持下去啊，妻子也强调，不管谁去谁留都可能给婚姻和家庭带来变故，而我们都极其渴望重新上学，改变命运，因此最终我们达成了一致，一起复课学习。</p><p>两个月飞速过去，我俩要离开这个家了，要离开儿子了，儿子虽然个头不小，可是因为他母亲怀他时严重营养不良，这时竟然连头也直不起来。儿子让我们割舍不下，心疼不已，后来在沽沱村找到奶妈，草草安置后，含泪依依不舍地离开了家，我们重新踏上了求学之路。</p><p>可怜天下父母心，儿子是母亲身上掉下来的肉，儿行千里母担忧，如今是母行千里，留儿子在家里。在最后一年的学生生涯里，儿子的身影时刻萦绕在她的心头，她上课经常走神，清净时甚至能隐约听到儿子啼哭的声音，身边没人时她就经常把自己为儿子做的一身小衣服，连同帽子摆在自己床上，摆弄摆弄看是否合身。她只要听到楼上女教师儿子的啼哭声就会不由自主地流眼泪。这一年应该说是短暂的，对我们来说却是无比漫长的。春节到了，本来是个千载难逢的探望家人的机会，团聚一堂的机会，可以和家人还有儿子团聚了，可是因为没有经费，我俩只能回去一个人，于是妻子回家，我一人留在学校，我会情愿吗，实在是无奈之举啊。妻子为了既能探望婆家人，又能见到父母，拉着架子车把婆婆和儿子拉到外婆家，以达到和儿子分秒不离的目的。留在学校的我心中是什么滋味，就让各位读者自己品味吧！</p><p>在这最后的学生生活里，因为精神压力大，我曾多次生病，眼睛患结膜炎、角膜炎，还有胸膜炎，胃病，失眠等疾病。由于视力极度下降，怕光、怕风、流泪，我无法进行毕业笔试考试，后来经校方特批，部分课程改成口述答题。坚持就是胜利，最后我竟然以优异的成绩顺利完成考试，毕业了。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;我和若平两人背景相同，志同道合，结婚后朝夕相处，可以永不分离，两个人形影不离，生活觉得甜蜜极了，结伴扛着农具干活去，在乡下的生活倒也过得有滋有味。我的长子陈大军出生后，全家都高兴坏了。父亲说，长子得了长孙，家里人丁兴旺了起来，再困难也得办几桌酒席，让亲朋好友齐聚一堂，共享快乐。父亲是非常能干厉害的人，在那么困难的条件下（三年灾害刚刚过去），操办了全席酒宴，我由衷地感激父亲。&lt;br&gt;
    
    </summary>
    
      <category term="爷爷回忆录" scheme="http://james20141606.github.io/categories/%E7%88%B7%E7%88%B7%E5%9B%9E%E5%BF%86%E5%BD%95/"/>
    
    
      <category term="life" scheme="http://james20141606.github.io/tags/life/"/>
    
      <category term="autobiograpy" scheme="http://james20141606.github.io/tags/autobiograpy/"/>
    
  </entry>
  
  <entry>
    <title>回忆录 CHAPTER Ⅷ 工作,崭新的篇章</title>
    <link href="http://james20141606.github.io/2018/04/11/auto8/"/>
    <id>http://james20141606.github.io/2018/04/11/auto8/</id>
    <published>2018-04-11T02:09:47.000Z</published>
    <updated>2018-04-12T08:12:12.567Z</updated>
    
    <content type="html"><![CDATA[<p>一九六四年八月中旬，我和妻子顺利毕业了，回宛后住在现在的新华宾馆等地方人事部门分配工作，妻子被分配到原安皋拖拉机站，我被安排到茶庵拖拉机站，过着两地分居的生活。</p><p>我在茶庵期间，单位的一把手杨永江是原来的茶庵区副区长，人很能干精明。他是姑婆家（槐树湾姑爷妹妹的老公）亲戚，待我非常的好，我就和他住在一个房间，他既是领导，也是朋友、同事，我们无话不说，因此工作上我就不怕了，谁也不敢欺负我。一九六四年八月十五日到达工作岗位后，我赶上了发工资，领到了人生的第一笔工资，成了国家的正式干部。<br><a id="more"></a><br>当时交通不便利，从茶庵到南唐路这十几里不通车，一下雨雪连步行都困难，虽说我们夫妻二人同在南阳县，可是相见格外的难，茶庵至南阳二十公里，在南阳东南角，安皋距南阳二十五公里。平时我们工作繁忙，一年四季在农村犁地，昼夜在田间摸爬滚打，一收车回单位就要修车，两人相见甚难。每年有一次表彰先进大会（在县里开），是唯一的可以见面的机会。</p><p>六四年十一月，小四清运动开始，茶庵是第一批重点，收车回单位搞小四清运动，气氛紧张得很。职工、干部人人自危，整天开会，宣扬着坦白从宽、抗拒从严的一套，工作队员凶神恶煞，非常的阴森恐怖。</p><p>我刚分配工作到茶庵，没有四不清问题，经过审查后被认定为运动积极分子，亲眼目睹了那场小四清不小的整人运动。单位李书祥被逼把头往砖块上撞，一般干部周文甫吓得尿裤裆。会计穆胜力被逼得悬梁，那天晚上正是我值班看守他，当晚戏园有演出，我想去看两眼，就告诉穆胜力找本书看看，不要乱想，你没有贪污，肯定会弄清楚的。我反锁房门就去了戏园，当我走到茶庵街跃进门旁时，天冷打了一个喷嚏，皮带断了，穿着棉裤没法走路，只好原路返回。我开了他房间的门，油灯灭啦，掏出火柴一划，人也不见了，又划了一根火柴，才看清楚他已经站在了办公桌凳子上做好了悬梁自尽的充分准备：绳子套在脖子上，只待用脚蹬凳子，顷刻间就会命丧黄泉。只消再晚三分钟，小穆成了冤死鬼，我也会因为失职而受到处分。我赶紧帮他下来，经过我耐心地劝导，他不再有了寻死的想法，我当然也轻松了，我了解他，他为人正直，我向他保证，会让他见一下他刚出生还没见过一次的儿子，后来我也帮他实现了。我主动请缨参与穆胜力贪污案，查案的过程就不详述了，最后终于查明是冤假错案，他没死，而且清白了，后来调到了安皋区拖拉机站继续干会计。运动后期小穆跪在我面前，跪谢我的救命之恩，我当然也理解他的心情，当年我也被人冤枉过，我发誓这辈子都不能冤枉别人，不能整人，更要帮助被冤枉的人，这是我做人的准则。</p><p>我因为在运动中表现突出，被定为中共预备党员，我是一九六四年十一月在茶庵区拖拉机站入党的，入党介绍人是机务队长王风杰。因有行政负责人杨永江推荐，在四清运动中表现好，查案立功，我在六六年初被调至县拖拉机总站，也就是现在的农机局。</p><p>到任后我被分配在机务科工作，任技术员。从此进城了，有了自己的办公室、办公用品、单人房和各种办公设施，取暖设施也齐全。城乡是有一定差别的，我开始了崭新的生活，这也为妻子从安皋调进南阳奠定了基础，在南阳生活、安家，养儿子。我一九六六年调南阳后经做工作，王若平于一九六九年被调县农机局修配厂当车工、新的生活开始啦。</p><p>一九六六年六月十六日，中央下达通知，一场轰轰烈烈的大革命开始了。那时候我才二十几岁，刚刚步入社会，缺乏经验，文革的是非曲直都让别人写吧，全国都一个模式，说不清楚，写不完，就全部省略了吧！反正我不是当权者，当然不受批判，性格决定我也不做亏心事，不做过激的事，因此也没留下大后患。那场大运动对我们这些从校门步入社会的大小知识分子，年轻人来说，实在是弊大于利，害了一大批风华正茂的年轻人，甚至有人为此付出了生命的代价。</p><p>现在年纪大了，经验和教训积累了很多，有很多感慨。年轻人易冲动，好鼓动，感情用事，不瞻前顾后，不思考后果，不听长者劝告，一意孤行，一头撞到南墙上，会吃大亏的。儿孙们都要记住，做任何事都要三思而后行，要多思考，深思熟虑后再行动，还要多请教别人，三个臭皮匠赛过诸葛亮。</p><p>文革后期成立了革命委员会，“农林水机电革命委员会”，把五个单位合为一体，我被安排在农业局担任政工工作，接着去李华庄生物制药厂担任政工工作，职位虽然不高，但是因为厂里两个领导互不相让，反而大权旁落，让我成了实际的做决策的人，生活、工作都顺心如意。</p><p>一九七六年调到了县委办公室，几个月后正式安排在县委组织部监察科任科员（即现在的纪委前身），这岗位最容易得罪人，因为负责的就是管干部查处违纪领导干部，这里面水很深，因为经验不足，方法欠缺，我想纪律监察不得罪人才怪哩！</p><p>一九七八年开始对技术人员评定职称，那时人们对职称看得很重，有职称的人可以让子女享受农转非的优惠政策，我一心评定职称，执意干老本行，回到农机局，这一干就干到了退休。</p><p>性格决定了我，祖辈们的教诲也帮助我成长，我不办亏心事，不做伤天害理的事，不管到哪里都有很好的人缘，工作不马虎，虽无大功，但也无大过，没有官帽但有实权，顺顺溜溜几十年，两袖清风肚子圆，落下的毛病是自己的问题啦。退休之日，我还是个普普通通的公务员，痛耶，悲耶，喜耶？知足也！<br>这一生酸甜苦辣咸，可谓是五味都齐全了！</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;一九六四年八月中旬，我和妻子顺利毕业了，回宛后住在现在的新华宾馆等地方人事部门分配工作，妻子被分配到原安皋拖拉机站，我被安排到茶庵拖拉机站，过着两地分居的生活。&lt;/p&gt;
&lt;p&gt;我在茶庵期间，单位的一把手杨永江是原来的茶庵区副区长，人很能干精明。他是姑婆家（槐树湾姑爷妹妹的老公）亲戚，待我非常的好，我就和他住在一个房间，他既是领导，也是朋友、同事，我们无话不说，因此工作上我就不怕了，谁也不敢欺负我。一九六四年八月十五日到达工作岗位后，我赶上了发工资，领到了人生的第一笔工资，成了国家的正式干部。&lt;br&gt;
    
    </summary>
    
      <category term="爷爷回忆录" scheme="http://james20141606.github.io/categories/%E7%88%B7%E7%88%B7%E5%9B%9E%E5%BF%86%E5%BD%95/"/>
    
    
      <category term="life" scheme="http://james20141606.github.io/tags/life/"/>
    
      <category term="autobiograpy" scheme="http://james20141606.github.io/tags/autobiograpy/"/>
    
  </entry>
  
  <entry>
    <title>回忆录 短文数篇</title>
    <link href="http://james20141606.github.io/2018/04/11/auto9/"/>
    <id>http://james20141606.github.io/2018/04/11/auto9/</id>
    <published>2018-04-11T02:09:45.000Z</published>
    <updated>2018-04-12T08:12:18.675Z</updated>
    
    <content type="html"><![CDATA[<h1 id="忆祖母-饮水思源"><a href="#忆祖母-饮水思源" class="headerlink" title="忆祖母 饮水思源"></a>忆祖母 饮水思源</h1><p>没有共产党就没有新中国，共产党像太阳，照得万物亮堂堂。赠给万物生命和力量。而祖母是陈氏家族的根蒂。象征家族生命的传承，没有她就没有这个家族的复兴和希望。<br><a id="more"></a><br>祖母娘家姓张，南阳县石桥镇东夹后街，家庭背景一般。她身高1 米七以上，面容慈祥可亲，虽是小脚，走起路来腰杆挺直硬朗，农耕、纺织、厨艺样样精良。她能把面条切的像粉丝一样。祖母勤劳、善良、乐施好善，她甚至还是一个土郎中、村医生、还会给人接生。她是一块经得起任何利器砥砺的玉石。经历了社会、家庭、婚姻的种种挑战，不屈不挠，引领一个支离破碎的陈氏家族拼搏向前。她坚信困难、黑暗总会过去，曙光就在前头。她是一个中国封建社会女强人的代表，带领这个家族迈向曙光。她有着普通男人都无法承受的经历。苦水、眼泪通通咬牙咽下，一生的磨难从不外扬。直到生命垂危时，祖母已无法进食，才汇集家人，字字血、声声泪地诉说那可歌可泣，悲壮的人生经历。我们劝她休息一会儿，可祖母不愿意停下，我们用棉花沾着水滋润祖母的嘴唇，泣而无声地听祖母讲述。祖母已无多少力气，声音颤抖无力。记下了，您血和泪的哭诉我们记下了，嘱托、希望我们也记下了，到今天我可以说，您当年的期望，我没有辜负，您的后代没有辜负您。</p><p>1962年9月2日（农历八月初四），祖母去世，去了天堂，临走是那样满足安详。祖母享年90岁。此时我已结婚，临终时她的孙媳妇也在身边。虽说当时条件不好，我们还是为她举行了隆重的葬礼，因为她是这个家族的基石，没有她就没有家族的希望和未来（祖母葬于皇路店魏庄祖坟处，此处是一块风水宝地，一弯河水环抱，继续保佑着陈家的未来）。</p><p>祖父去世的时候，我的大伯还是个孩子，父亲才几个月大。祖母擦干眼泪，振奋精神，带领着一群未成年人在生死线上挣扎。（大伯、父亲、四伯、大表姐、二表姐和母亲）。</p><p>堂伯幼年父母双亡，自己没有生活能力，奶奶将其抚养长大。大姑结婚后剩下两女，得病身亡。两个女儿也要交由她们的外婆抚养。我的母亲是童养媳，八岁进入陈家。</p><p>这哪里算是一个家啊，这就是一个托儿所、幼儿园。管吃管住管穿戴，所有人的吃穿住全部仰仗着祖母，还要把大家抚养成人、成家结婚。祖母好像办了一个慈善机构，只是没有捐款、没有住房、没有耕地，只有一群苦命的儿童。</p><p>祖父死后，祖母带着六个孩子四处流浪，从石人沟到濛山，雷家沟，柳树沟，再到薛庄落脚，最后到魏庄，经历了土改，终于有了房子。</p><p>祖母身材高大，体魄健康。是她给下一代遗传了魁梧的身材。她传授给父亲很多生存技能、道德规范。父亲说，奶奶传给他最宝贵的精神财富是：坚强、果断、不畏艰难，相信车到山前必有路，没有过不去的火焰山。教育父亲不能只会种地，还要学会做些生意、广交朋友，以仁者之心待人接物。多做善事，好人定会有好报。她还说，男儿有泪不轻弹，动不动就哭是弱者的表现。祖母这样一位坚强勇敢又有智慧的女性，是后代的楷模。</p><p>做好人，办好事是有分寸的；在那个混乱黑暗的年代，人善被人欺，马善被人骑，好汉子出嘴上，好马出腿上。连这个道理都不敢讲怎能称得上是男人？父亲一生都在践行祖母教诲，受益匪浅。祖母教育父亲的这一切，父亲言传身教，又教了我一些，我也体悟了一些人生经验。</p><p>祖母不是日月星辰，不是盛满油的汽油灯，她是一个在困难的年代独撑一个家的身材高大、身影伟岸的女人，她在黑暗的年代，像萤火虫一样发出微光，引领者她的儿孙们在腥风血雨中摸爬滚打着走向了光明。</p><h1 id="忆母亲"><a href="#忆母亲" class="headerlink" title="忆母亲"></a>忆母亲</h1><p>母亲忠厚、淳朴、善良，性格不急不躁，很少对儿女大发雷霆。从不举手打骂我们。她和邻里关系和睦，待人接物大方。与人与事无争，能忍能让，百善孝为先是她留给下一代的宝贵精神财富，在这一点上她是典范。</p><p>在处理邻里关系时，有时我们兄妹会和别人家孩子争抢甚至打骂，当双方家人参与时，她总会自责揽下责任，向对方赔礼，矛盾自然化解。再如，过去我们生活困难，有时连油盐、面粉都要向别人家暂借，母亲归还时总会比借来的多还一些，还面时，总是要把面瓢按瓷实了，她说，这样下次别人才会愿意借给咱。母亲的生活的智慧可见一斑。</p><p>关于孝道，母亲对奶奶比对她的亲妈还要亲，从不在别人面前说奶奶的一句坏话，在别人面前提到奶奶，都是称呼妈，不似现在很多人家的婆媳关系一样。</p><p>我和兄妹从小生活水平就很低，母亲为了不让祖母受苦，让她和大伯一同开灶，让生活好点。分灶后祖母在大魏庄住，我们在现在的地方住，每逢过年过节稍微改善生活，妈想到的第一碗饭就是祖母的，不把这第一碗饭送过去，是不准开饭的。</p><p>母亲记忆力很强，语言表达能力超过常人，她虽然不识字，但是对过去发生的事情，总能绘声绘色地演绎出来，讲话点滴不漏，就像是把当时的现场复现出来一样，我很佩服母亲这样的能力。</p><p>母亲的四不精神：不热、不冷、不饿、不累。</p><h3 id="不热"><a href="#不热" class="headerlink" title="不热"></a>不热</h3><p>夏季很热，母亲除了做家务、做饭，还要去地里帮干农活，回家后父亲和我们乘凉休息，母亲还要再干家务，做一大家人的饭。大家劝她，歇会儿再说吧，天太热了，母亲说:我不热。</p><h3 id="不冷"><a href="#不冷" class="headerlink" title="不冷"></a>不冷</h3><p>过去的冬天非常冷，没有任何取暖的设备，到了春节前，母亲彻夜不睡，做针线活到天亮还做不完。我睡在被窝里尚且浑身冰冷，醒来问母亲，这么冷怎么还一夜不睡在做针线活，母亲说：我不冷。<br>可我看到，母亲的手指和手背都冻得开裂了。</p><h3 id="不饿"><a href="#不饿" class="headerlink" title="不饿"></a>不饿</h3><p>过去的生活非常困难，很少有改善生活的时候，如果哪顿饭能有点味道、汤稠一点，孩子们就会抢着吃，我们会说，妈你也吃点吧，要不就没了，母亲就说，你们吃吧，我不饿。<br>那时候我还小，搞不懂，我们都饿坏了，母亲怎么老是不饿。</p><h3 id="不累"><a href="#不累" class="headerlink" title="不累"></a>不累</h3><p>人不是钢铁，是血肉之躯，当看到母亲满头大汗，甚至快要倒下的时候，我问母亲，妈你累不累，她说：我不累。</p><p>母亲说人的一生要多做善事，不做亏心事。做了坏事、亏了心，神仙会知道的。她说自己儿女多，受贫薄，哪有功夫念弥陀，可我心里一直在念啊。</p><p>母亲像一架永不生锈，不会损坏的钢铁机器，一直在转动。贫困、劳累、饥饿早已将她摧垮，只是她不愿说，不愿表达，钢铁之躯也有停止工作的一天。</p><p>母亲于1985年腊月初一晚上十点，坐在靠椅上洗脚时心脏停止了跳动。时年66岁。母亲含辛茹苦，把我们兄妹六人养大，还未及报恩，她还没有真正过上好日子，就这么离开了我们。连一粒救命的药都来不及吃。母亲死时没有痛苦，可我心里痛苦呀。我没有完全尽到儿子应尽的责任，自责也没有用了。望天下所有的儿女们，父母在时要抽空多陪伴，尽点孝心，多些体谅，少些抱怨。说什么现在我太忙，抽不出时间，现在还困难，等有钱了再说，可是人都没了，还能孝顺谁去。树欲静而风不止，子欲养而亲不待。这样的道理，还是早点想通了好，要不然只能空留下遗憾了！</p><h1 id="四伯家生活写照"><a href="#四伯家生活写照" class="headerlink" title="四伯家生活写照"></a>四伯家生活写照</h1><p>四伯成年后离开了奶奶，也就是他的三婶，独自回到石人沟生活，他在石人沟居住于自己的山坡边修建的“金銮殿”。这栋房子全由四伯一人独立建造，石头、土坯做墙体，木棍、竹子做房梁，竹子做椽子。房子深约三米半、长约六米。</p><p>房内家当摆设：靠西山墙放置土坯床一个，靠东山墙土锅台一个，小铁锅一个，铲锅刀一把，木勺子一个，竹刷子一把，没有盛水缸，只有一个小瓦罐（吃水方便，随用随提，所以也不用水缸）。没有案板，只有木锨板大小的一块板做案板。</p><p>他用竹子编了一个大竹篓，用来盛红薯干、花生之类专用。用泥制缸来盛放大米，洗脸盆是一个的烂一半的瓦盆。那时候没有电，屋内也没有油灯，实在需要照明时，用灰麻杆或者竹子点燃后照明。这房子除了门没有窗户，只在前墙体留个洞。冬季用稻草堵死不让进风。这座房子周围环境优美，生态环境好，门前的小溪清澈见底，四季都不干涸，水源于山上的雨水与山泉，常年流水不断，水清冽甘甜，十分解渴。山泉处挖了大坑，那里是我夏季玩水嬉戏洗澡的好地方。</p><p>现在的水库是当年大竹园的位置，竹园周围，房前屋后植被茂盛，春天时百花争艳、开的分外灿烂明艳，林间、尤其是竹园，百鸟争鸣，从早吵到晚。房后的葛花树、梨树、桃树、杏树和不知名的野草野花交相辉映，这样的原生态花园现在可是难觅了。</p><h1 id="八岁孩子学走路"><a href="#八岁孩子学走路" class="headerlink" title="八岁孩子学走路"></a>八岁孩子学走路</h1><p>1948年八周岁时，我得了天花。当时医疗条件极差，而且天花患者年龄越大越不易治愈，我接连发高烧，连路都不会走了。我是大伯的过继儿，他看在眼里，急在心里。他说：万万不敢出什么事，要不将来百年后我那杆大旗谁抗啊。后来他常用自己系腰的长腰带兜着我两个胳膊，教我重学走路。当时家里人多，生活水平很差，大伯分灶吃饭，他有严重胃病，每逢做了好吃的饭，他都偷偷暗示我到他那儿吃点，慢慢的我的身体逐步恢复。大伯晚年时胃病很重，我参加工作后，虽然工资很低，每次回家都要给他留几块钱，谢救命之恩。他过世时我在茶庵区拖拉机站工作。路程远，交通不便，没能通知我及时返回送他最后一程。那杆大旗是我的长子陈华军扛的，那时华军年幼，是被人抱着扛起来的。他扛和我扛是一样的。几十年了。<br>逢年过节，我都不忘给他送好多纸钱（大伯的坟在魏庄，和父母的坟在一处），他不会缺钱花的，这恩情我要永远记得呀。</p><h1 id="我的第一双棉鞋"><a href="#我的第一双棉鞋" class="headerlink" title="我的第一双棉鞋"></a>我的第一双棉鞋</h1><p>在我印象中，上初中前的冬季我从未穿过棉鞋，因为脚被冻坏了，成年后冬天不穿棉鞋脚就会生冻疮。母亲跟着哄老三的时候，有一年冬天脚又冻着了，走路一瘸一拐。母亲问我：怎么走路一瘸一拐的，我不小心脱口而出：都怪你小时候不给我做棉鞋，脚留下冻疮的根。我还补充道：从记事起，冬天没穿过棉鞋。母亲回答我说：是你不记得了，你小时候还穿连脚裤呢，这不是棉的吗。这话说的我啼笑皆非，想起来真是好笑。<br>因为家穷兄妹多，印象中冬季不穿棉鞋像是事实。1955年我的干姐（苏大妮，家住白河东长嘴村）在春节前给我做了双偏开口气眼棉鞋，捎信给我，让我年根去拿。为了这双鞋我起早赶路去她家里。可返回时道路开冻了，泥泞不堪。我不舍得穿，手拿棉鞋，光脚往回走。这鞋在我心中是宝贝，要留到大年初一穿，能舍得叫沾上泥吗？</p><h1 id="大伯和父亲给祖母惹祸"><a href="#大伯和父亲给祖母惹祸" class="headerlink" title="大伯和父亲给祖母惹祸"></a>大伯和父亲给祖母惹祸</h1><p>大伯成年后只会干农活，少言寡语，性格内向，不善交流，说话简单粗暴，不好听。有一次他挑完茅粪后，粪桶在堰潭里清洗，一群妇女在堰潭里洗衣服，骂他是猪狗不如之人。他破口还人家，言语粗俗，引一群妇女和家里男人与他厮打。大伯用挑粪扁担将人打伤，我的父亲见状，非但不劝架，反而参加打斗。打斗的对手是丁老庄的一户有钱人家，状告官府要陈家道歉、赔偿药费，甚至扬言要抓人坐牢。祖母只得四处找人说情，道歉，还要四处借钱，变卖衣物，摆了十几桌宴席，才算平息了此事。我未吸取长辈教训，有时因头脑发热，办事简单粗暴，也办过一些错事。父亲年轻时个子高大，长相不差，十几岁时去小石桥做生意，返家时路遇土匪，以为父亲家里有钱，将他绑架。父亲被麻绳困住双手，膏药贴住双眼，口被毛巾堵上，投进了红薯窖里。土匪向家里要钱，却没有料到此人是穷光蛋，后经人周旋，祖母又摆了几桌酒席，才将父亲放回家，祖母难啊！</p><h1 id="真实故事三则-2016年八月二十五"><a href="#真实故事三则-2016年八月二十五" class="headerlink" title="真实故事三则 2016年八月二十五"></a>真实故事三则 2016年八月二十五</h1><h2 id="一-龙闸水"><a href="#一-龙闸水" class="headerlink" title="一 龙闸水"></a>一 龙闸水</h2><p>小时候的一年夏天，上游山区下暴雨，小岔河发了大水（就在父母坟地西北方靠河边的高粱地），水在那里打旋，不往下走。当水势退去，那里青蛙、癞蛤蟆、大小蛇类、鱼虾横尸遍野。我亲眼目睹，觉得现象怪异。那是块龙地啊，龙闸水啊，人亦是属龙的吗？</p><h2 id="二-怪病吃钱喉"><a href="#二-怪病吃钱喉" class="headerlink" title="二 怪病吃钱喉"></a>二 怪病吃钱喉</h2><p>在皇路店上小学期间，每天太阳升起时总要肚子疼上一阵，有一天，当一轮红日冉冉升起时，我正站着看火球慢慢升起，就是在现在的老家住宅处，突然感到肚子巨疼无比，豆大的汗珠直流。父亲叫家人赶紧生办法找生大烟膏喝，要不会疼死人，郭保国的父亲郭老三准备去犁地，路过家门口。他手持挂了一大串铜钱的长杆旱烟袋，边吸边走，见此情景，他让找点黄豆嚼嚼看看什么样，结果没有任何效果。接着他从烟袋上取下一枚铜钱放我嘴里让我用牙咬。这铜钱用牙咬着竟然像是被软化了一样，马上被咬成了小碎渣，他让我和着一口水咽下，顷刻肚子不疼了。他说这病叫吃钱喉，得不到及时治疗会丧命的。这病人畜都会患，吃钱喉现代医学如何解释呢？</p><h2 id="三-一庹长大蛇戏水"><a href="#三-一庹长大蛇戏水" class="headerlink" title="三 一庹长大蛇戏水"></a>三 一庹长大蛇戏水</h2><p>1950年我已在尹店小学上学，校址在三皇姑庙院内，我小时候胆小怕事，弟弟陈炳义胆大顽皮，他小我四岁，但夜里要办事时我总会约他一起办。一年夏天的晚上母亲突然患病，头疼欲裂，呕吐不止，需要去贾庄请医生。那时从魏庄到贾庄没有正路（至今也没有一条正路），得从地埂荒路走。常听村里人说，去贾庄过河渡口处看见过一条大蛇，长约一米五，平时藏在大石堆中（石头是国民党时期想修鸭河水库所以从蒲山运来的），因经常听人提起，本来对此处就心生忌惮，夜里伸手不见五指，河里正是水势充沛，可别让我真的遇上大蛇了。过河时我让炳义走前面，我紧跟其后，刚一下水，果真惊动了那条大蛇，吓得我一头栽到了河里。当时没有桥、没有任何照明的设施，我们拿着灰麻杆照路，究竟有多长并没有看清。到了第二年发洪水，那条蛇被冲到尹店榆树林杂草堆中，发现时已经死了，才知道大约有一庹（两臂左右平伸，掌心向前，两手指尖之间的距离，通常叫做一庹）那么长。</p><h1 id="我的朋友周聚照"><a href="#我的朋友周聚照" class="headerlink" title="我的朋友周聚照"></a>我的朋友周聚照</h1><h4 id="2016-5"><a href="#2016-5" class="headerlink" title="2016.5"></a>2016.5</h4><p>我和妻子于1962年农历七月成婚，那是物资奇缺，正值困难时期。结婚时为购置食品，父亲没少犯愁。不仅肉类奇缺，鸡子鱼鸭更难买，就连最普通的莲菜都买不到。后来找到南召县农场的杨厂长，给了些莲藕代用。当时的农场就在村旁边。那时年轻人结婚能办几桌全席极少，城镇里工人干部也很少能做到。父亲能为我们办那么丰盛的婚宴全席，让我终身难忘。</p><p>那时穿衣凭布票，每人每年才几尺布票。结婚时我们俩把上学时各自的被褥，拆洗后合在一起，算是新婚床铺，为了新婚当天能在床上铺上新床单以示吉庆，我们的太平洋床单还是借用郭保玉的，举行完婚礼即刻归还，当时连床单都没有，真是难为妻子了，至今仍然觉得心怀歉疚。可妻子无怨无悔，当时也都不讲排场。结婚头一年，总得给岳父家送点薄礼，这是农村老习惯。那是买不到猪肉，我在犯愁送什么礼。正为难时，父亲在新野县王庄的朋友王叔造访，说养了一头小猪，春节给免费送些猪肉。因为心急，我于当年腊月二十三就前去取肉。家里距离新野约六十公里，我还没有骑单车这么远过。骑到南阳时我已筋疲力尽，屁股像涂了辣椒水一样，再也不想坐自行车的破座椅了。中午我啃了些干粮，还得继续南下，骑骑推推，到沙堰时天色已晚，还不知王庄离此地还有多远，便到处打听。</p><p>当打听到沙堰区政府门口时，见到一个年轻人站在那里。他仔细打量着我胸前戴的郑州农机化专科学校校徽，当时正是放长假时期，我是有意戴上的，他看我的模样就热情攀谈起来，我后来得知他是区里通信员，他说自己叫周聚照。我们坐在接待室里，他看我非常疲惫，像是没吃晚饭，就到厨房让炊事员弄来一大碗水饺和两个火烧吃。他的热情深深打动了我，我表示了谢意后要离开，他却执意不让走，说我俩都是男人，不必嫌弃，就一起睡在大床上，大冬天就不要在晚上赶路了，这里离王庄还是十几里呢。我太累了，也因为他的热情就住下了。我们不似初见，倒像是久别重逢的老友一样，海阔天空地聊着，我竟忘了一天的劳累，忘怀地聊着笑着。第二天我醒来时，他为我准备了一瓶热水供洗漱，还端来了饭菜。饭后他甚至请假陪我去王庄，还要让我骑他的新自行车，他骑着我的旧车去。到了王庄，王叔热情接待了我们，当二人分别时，他非要给我全国粮票十斤，并给我布票几尺，做送别朋友之礼，留作纪念。我推辞不过，万分感激地收下了。返回时路况熟了，又因为在百里之外交了一位热情大方、诚恳可爱、彬彬有礼的朋友，心情大好，骑车也不觉得累了。</p><p>我们两个多次书信联系，他还专程来魏庄看望我。后来因为各自工作变化、地址变化失去联系。</p><p>好人一生平安，我的朋友周聚照会幸福的，祝你晚年生活愉快，儿孙满堂，笑口常开。福如东海长流水，寿比南山不老松。</p><p>老友陈炳林追忆。</p><h5 id="注："><a href="#注：" class="headerlink" title="注："></a>注：</h5><p>在大家的努力下，爷爷奶奶在2017年春天得以寻回当年老友一家的联系方式，并去探望，周爷爷已经不在人世，其后人生活幸福美满，子孙受到很好的教育，所谓好人必有好报。</p><h1 id="人生机遇只有一次"><a href="#人生机遇只有一次" class="headerlink" title="人生机遇只有一次"></a>人生机遇只有一次</h1><p>一九七九年我在县委组织部监察科工作，三十九岁，符合军分区提拔武装部领导的条件。在自己不知情的情况下，军分区给南阳县武装部发来命令，调我去石桥人武部当部长，兼任副书记。我当时非常高兴，可我的妻子却高兴不起来，坚决反对我去石桥任职。她以一人无法照料四个孩子为由百般阻止。那可不是组织部普通的人事调动，而是一张盖有由军事部门大印的命令，压在县武装部军事科长余科长办公桌玻璃板下的命令。一个党员是该无条件服从的，怎么办？县人武部长告诉我，一个党员干部，在这个命令下达后，没有特殊理由是不能不去的，除非身体方面的原因，我找到了关系要好的文副部长，他给我出了个妙主意：我患有胃下垂，严重溃疡，你拿我的片子，换成你的名字，让医生开诊断证明，兴许管用。我果真照办，骗过了分区领导。一般人对于个人升迁、提拔都持积极态度，甚至有人送礼说情，削尖脑袋往上爬，我却死活不领情不赴任，少见稀奇。那时才三十九岁。直到我光荣退休，依然是大科员一名，真是个大笨蛋。不过这几十年妻子从没埋怨过我无能，唉，都快八十岁的人了，想这些干什么，都过去了，无官一身轻，钱多是非多，过好余生吧。<br>注：石桥人武部已配好了办公室、办公桌等，配好了火炉水瓶，连年终救济三十元都评定好了，因为主要理由是负担大而不上任，对得起组织吗？</p><h1 id="过个革命化的春节"><a href="#过个革命化的春节" class="headerlink" title="过个革命化的春节"></a>过个革命化的春节</h1><p>一九六五年拖拉机总站提出过革命化的春节，春节不放假，奋战生产一线的口号。大年初一那天我的车组（链式东方红五四车）被调到高庙乡王连庄村耕地。成员有：杨志堂、杜保清、侯西荣等五人，那天天非常的冷，天寒地冻，大雪纷飞。我们提出过革命化的春节，要有革命化的表现，不扰民，拉人做些食物吃就行了。他们找到了一个双目失明，独自一人靠算卦活命的老头家的一间草房，白馍、熟肉、粉条都有，队里说这个瞎眼的老头干净、会做饭，可是中午到他家吃饭时，发现这些食物全都烧糊黏住了，一股乌鸦的臭味。大家都拒绝吃这顿大年初一的年饭，发牢骚。快到中午时总站机务科副科长郭孔印冒着雪骑着破自行车走了三十多公里到我们车组慰问，他也够辛苦了，而我们只能把气撒在他身上，村民围了一大片看热闹。<br>后来村民编了一个顺口溜：拖拉机板真吊旦，大肉白馍叫随便赞，初一中午搞绝食，提抗议要发难，说是穿着蟒袍衣，走上大街叫看看。<br><em>这般形象：一身灰满脸油，不是拖拉机手是老球，远看是要饭，近看像卖炭的，走到跟前一看是拖拉机站的。</em>有油没菜拖拉机光坏（出故障，犁不成地），有菜没酒拖拉机不会走，有酒有菜，地犁得又好又快。<br>*吃花卷馍，犁花卷地：代耕唐河县桐河村，拖拉机进了村，来了荷枪实弹护卫军，犁地时持枪站在地头护卫，怕敌人破坏车辆。当晚请南阳曲剧团演戏（野火春风斗战城），戏台观看席摆桌凳茶水茶具，专供拖拉机手用，这也够风光了。吃饭二人一席（因两人一班车不停，换班吃饭），每人一条红烧或者糖醋大鲤鱼。<br>睡觉：换下夜班后露宿野地（野餐大多送到了地头，吃后便睡），天作被，地作床，处处都是席梦思，上边盖得太平洋，呼噜呼噜入梦乡。<br>卫生条件：在本站辖区耕地，任务重，为不误时间，村民都将饭菜送到地头，不可能一年四季处处有水，吃饭时不洗手、不洗脸，抓把黄土做洋碱，两手一搓就开饭。吐口吐沫在手指上，手指身上蹭就算干净了，就可以吃饭了，省时间就地取材，又天然。<br>机耕结束，某次全县人员集中总部开表彰大会，大学生牛某某，工作衣服的缝开了，用铁丝串着，就这还露屁股露大腿（这人本来就邋遢），后来县里决定发劳动布，专门用来补工作服。</p><h1 id="石人传说二则"><a href="#石人传说二则" class="headerlink" title="石人传说二则"></a>石人传说二则</h1><h2 id="一"><a href="#一" class="headerlink" title="一"></a>一</h2><p>一个晚上，石老婆正在石人山碾米，有一个路人从石碾边上的山路经过问路。石婆指完路，又问路人：你看这米碾到这样中不中啊？那人抓起一大把米细看，说道：中啦，然后转身上路。第二天那路人洗手时，发现指甲缝里有东西，一看是金米，他后悔极了，早知道是金米，何不抓上一大把？</p><h2 id="二"><a href="#二" class="headerlink" title="二"></a>二</h2><p>过去农家干农活，收工晚，吃完晚饭就睡了，只有小油灯作为照明设备，夏夜里吃晚饭都坐外边，这样凉快些。那时陈家人口很多，盛饭时你来我往，好不热闹。有好几次家人隐隐觉得有一身材高大之人也来往盛饭，但天黑看不清楚，也没人在意，有一次祖爷在暗处吃晚饭，此人又趁着空隙盛饭，祖爷猛扑过去，把饭勺扣到他的头上，于是那人慌忙跑了。第二天祖爷上石人山砍柴时，发现石人头上有大米饭，这才醒悟过来，原来头天晚上盛饭的人是石人呀。</p><p>注：此为祖母所讲的故事。1951年我重返石人山，亲眼见到地质人用绿帆布将石人周围拉起来，进行勘探、研究，现在石人肚脐处的那个洞就是搞地质的人挖的，至于发现了什么就不得而知了。</p>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;忆祖母-饮水思源&quot;&gt;&lt;a href=&quot;#忆祖母-饮水思源&quot; class=&quot;headerlink&quot; title=&quot;忆祖母 饮水思源&quot;&gt;&lt;/a&gt;忆祖母 饮水思源&lt;/h1&gt;&lt;p&gt;没有共产党就没有新中国，共产党像太阳，照得万物亮堂堂。赠给万物生命和力量。而祖母是陈氏家族的根蒂。象征家族生命的传承，没有她就没有这个家族的复兴和希望。&lt;br&gt;
    
    </summary>
    
      <category term="爷爷回忆录" scheme="http://james20141606.github.io/categories/%E7%88%B7%E7%88%B7%E5%9B%9E%E5%BF%86%E5%BD%95/"/>
    
    
      <category term="life" scheme="http://james20141606.github.io/tags/life/"/>
    
      <category term="autobiograpy" scheme="http://james20141606.github.io/tags/autobiograpy/"/>
    
  </entry>
  
  <entry>
    <title>回忆录 童年的诗</title>
    <link href="http://james20141606.github.io/2018/04/11/auto10/"/>
    <id>http://james20141606.github.io/2018/04/11/auto10/</id>
    <published>2018-04-11T02:09:42.000Z</published>
    <updated>2018-04-12T08:12:26.086Z</updated>
    
    <content type="html"><![CDATA[<center><font color="#DC143C" size="5"> 玩秋千 石人山情节 </font> </center><p align="right">2016.9.11</p><center>茅草小屋金銮殿，桃杏梨花来装点。</center><center>春日争相露笑脸，葛花树上玩秋千。</center><center>若有同龄孩童伴，情景定会不一般。</center><center>良辰美景一人享，亦开心来亦孤单。</center><a id="more"></a><center><font color="#DC143C" size="5"> 石人山景 </font> </center><center>石人山景美若画，儿时记忆难忘下。</center><center>高山顶峰衬其峻，云雾缭绕摹其神。</center><center>谷满林竹花锦绣，潺潺流水汇清泉。</center><br><br><center><font color="#DC143C" size="5"> 恋家 </font> </center><center>少时寄住石人山，犹似修隐仙景间。</center><center>天亮登山陪桑蚕，入夜土炕头边玩。</center><center>山风怒吼夜狼嚎，柴房家犬牙齿露。</center><center>一日三餐尚果腹，思乡遥望东方愁。</center><br><br><center><font color="#DC143C" size="5"> 四伯 追思堂伯父 </font> </center><center>幼时父母皆亡殁，凄凉处境人怜惜。</center><center>尝尽人间冷与暖，变得木讷而寡言。</center><center>不养鸡鸭不养猪，不种蔬菜只种谷。</center><center>厨中并无盐油醋，舌尖哪有美味留。</center><br><br><center><font color="#DC143C" size="5"> 无题 </font> </center><center>民国三十年，恰逢吾降生。</center><center>上天不下雨，庄家未收成。</center><center>家园遭天灾，倭寇来侵略。</center><center>人在襁褓中，母亲患大病。</center><center>断了生命水，差点送性命。</center>- 注：生命之水指母亲奶水。<center><font color="#DC143C" size="5"> 思往昔看今朝 </font> </center><p align="right">2015.3</p><center>忆往昔峥嵘岁月稠，</center><center>想痛处，满脸沟壑老泪流。</center><center>看今朝国盛家富，</center><center>家和睦儿孙老人事事顺溜。</center><br><br><center><font color="#DC143C" size="5"> 忆石人涧茅草小屋“金銮殿” </font> </center><p align="right">2016.9.9</p><center>石人身高八丈三，一半身在云雾间。</center><center>石婆闲来无去处，身贴悬崖避风寒。</center><center>谷底花树高又俊，秀拔挺立直冲天。</center><center>房后园中绿竹林，粗若碗口真稀罕。</center><center>门前小河水不断，源自上游山泉涧。</center><center>春风吹佛小山村，桃杏梨花齐斗艳。</center><center>吾辈此时何处去，葛藤树上荡秋千。</center><br><br><center><font color="#DC143C" size="5"> 咏春 石人山旧居 </font> </center><center>茅草小屋金銮殿，桃杏梨花来装点。</center><center>三亩翠竹作盆景，门前溪水流不断。</center><center>欲问清水何处来，遥指山谷泉水涧。</center><center>春色美景独享有，葛花树上荡秋千。</center><br><br><center><font color="#DC143C" size="5"> 大竹园变堰潭 </font> </center><center>旧时竹园改了观，现已变成大堰潭。</center><center>鱼儿为觅漂浮时，头儿伸出乱眨眼。</center><center>若想尝下新鲜味，一网下去捞两篮。</center><ul><li>注：原来的大竹园在58年时，竹子被砍伐，修成了大堰潭，原生态的景观也被破坏殆尽</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;center&gt;&lt;font color=&quot;#DC143C&quot; size=&quot;5&quot;&gt; 玩秋千 石人山情节 &lt;/font&gt; &lt;/center&gt;
&lt;p align=&quot;right&quot;&gt;2016.9.11&lt;/p&gt;

&lt;center&gt;茅草小屋金銮殿，桃杏梨花来装点。&lt;/center&gt;
&lt;center&gt;春日争相露笑脸，葛花树上玩秋千。&lt;/center&gt;
&lt;center&gt;若有同龄孩童伴，情景定会不一般。&lt;/center&gt;
&lt;center&gt;良辰美景一人享，亦开心来亦孤单。&lt;/center&gt;
    
    </summary>
    
      <category term="爷爷回忆录" scheme="http://james20141606.github.io/categories/%E7%88%B7%E7%88%B7%E5%9B%9E%E5%BF%86%E5%BD%95/"/>
    
    
      <category term="life" scheme="http://james20141606.github.io/tags/life/"/>
    
      <category term="autobiograpy" scheme="http://james20141606.github.io/tags/autobiograpy/"/>
    
  </entry>
  
  <entry>
    <title>回忆录 关于正义的故事</title>
    <link href="http://james20141606.github.io/2018/04/11/auto11/"/>
    <id>http://james20141606.github.io/2018/04/11/auto11/</id>
    <published>2018-04-11T02:09:40.000Z</published>
    <updated>2018-04-12T08:12:32.736Z</updated>
    
    <content type="html"><![CDATA[<p>那是上世纪第二个十年发生的故事，年代久远，记忆都已褪去了鲜活的颜色，就像黑白照片一样，边缘还笼罩着一层薄雾。故事是老家的一位年迈的智者在弥留之际讲述的，这个故事在他心中藏了很多年，这个充满了残忍的谋害、热血的沸腾、正义的伸张的故事，糅合在那个特殊的年代，被人藏在心里，藏得太久，当在最后的时刻讲述那个青春飞扬的故事的时候，竟然讲得那么辛苦艰难。<br>这就是那个改变了一个家庭命运的故事：<br><a id="more"></a></p><h1 id="男人"><a href="#男人" class="headerlink" title="男人"></a>男人</h1><p>那是一片静谧安详的山区，早些年发生的案子的影响已经渐渐消散，家里的情景却一年不如一年，战争发生的地方虽远，国运衰微、民生凋敝却是不争的事实，连山里人都能感受到生活的愈加不易。那个男人起了个大早，辞别家人，赶着家里的两头黄牛去集镇上卖牛赚些钱。待慢悠悠赶到集镇，谈妥价钱，卖掉家里的两只黄牛的时候，天色已晚了。男人来不及赶回家里，打算住在女儿的婆家，男人一定想不到，女儿的婆家所在的村庄，成了自己后来的埋骨处，也想不到，冥冥之中，自己的后代会重新寻到这个地方，生活繁衍，发生了那么多的故事。</p><p>男人就这么住下了，他性格粗犷，又住在女儿家，一天的奔波疲乏令他把外出小心、财不外露等等谨慎的想法都忘却了。没想到女儿的公公和道上的土匪多有来往，并不是什么干净之人，这个见钱眼开的奸人并没有觉得女儿的娘家对自己有什么忌讳的，一眼便看到了男人卖牛赚得的钱。在这个混乱的年代，在那片混乱的地区——三县交界、土匪横行，民不聊生的地带，正义女神很久没有光顾了，举头三尺望到的也不是神明，而是贪婪、残忍和血腥的恶魔。</p><p>第二天一早，赶着回家的男人轻松地回家了，家里的两个孩子可能都等不及了吧，才一天没见，已经想念还没满周岁的小儿子了，妻子做的午饭说不定能赶上呢，想到这里，男人的脚步又加快了些。走到石磨岭的时候，男人觉得出了一些异样，这一路总有被人跟着的感觉，正想着时，石头后面跳出四个穷凶极恶的土匪，男人楞了一下，怎么自己的亲家也在？还没回过神来，一声怪异的巨响传来，声音总是跑的比子弹快一些，片刻之后，男人还没看清子弹掠起的衣角的飘动，就已经倒在了血泊里。</p><h1 id="小海"><a href="#小海" class="headerlink" title="小海"></a>小海</h1><p>小海今年十二岁，在中国人心中，这个岁数代表了某种成长的标记，仿佛跨过了这个岁数，一切就突然变得不太一样一般。小海从小跟着母亲到处讨生活，对于没有父亲的孩子来说，这一切也习以为常了，不知道是雷家沟还是柳树沟，还是走到了某个庄暂住，一年年都是由一个身材高大，隐忍坚强的女人领着几个孩子到处走过来的。</p><p>小海不到一岁的时候父亲就消失了，他所认识的父亲是庄上乱坟岗矗立的一个坟包，母亲也说不清楚父亲究竟是怎么死的。小海知道母亲很苦，需要一个人拉扯着几个孩子，小海不知道父亲刚死的时候母亲有没有哭过，但这不是个给人机会终日以泪洗面的时代，几个孩子的命运都在母亲的身上系着，小海只是难过，如果父亲不死，母亲也不用那么累，他们几个小孩子也可以少受一些欺负，父亲为什么会离开得那么早呢，小海会想念父亲，那个记忆里连一点残影都找不到的男人。</p><h1 id="姐夫"><a href="#姐夫" class="headerlink" title="姐夫"></a>姐夫</h1><p>直到那一天，小海的大姐夫找到了小海，大姐夫是一个勇敢无畏的人，那个时代能立足的人都有些本事，大姐夫花了很长时间摸清楚了岳父的死因，把那段十一年前的故事讲给了小海听。也许一百年前的孩子比现在的孩子要更坚强勇敢一些，更能经受的起那个风雨飘摇的时代，但是没有哪一个时代的孩子能够经受得了这样的故事，一个充满着血海深仇的故事，一个让人想哭哭不出，想怒不知如何怒的故事。小海十二岁了，他一瞬间就长大了。</p><p>姐夫告诉了小海是谁杀了父亲，告诉了小海土匪的行踪，更重要的是，小海有了武器，一支土枪，一支有杀伤力的土枪。这情节就像美国的著名电影《这个杀手不太冷》的情节一样，只不过小海并没有那么强悍有力的装备，但是小海有更深的仇恨，更强烈的复仇的念头在心中燃烧，姐夫陪着小海练习枪法，练习如何用这只土枪，完成一个故事，一个关于父亲、家庭、正义与复仇的故事。</p><p>时间过得很快，小海的枪法日益精进，不需要督促，就已经打得有模有样。见时机已到，两人不愿多等，动手的时候差不多到了。姐夫和小海商量好了，由小海亲自开枪，姐夫的枪法自然更好一些，但是这是亲手解决杀父仇人的机会，没有人更有资格开第一枪了，如果小海失手了，再由姐夫补上第二枪。</p><h1 id="复仇"><a href="#复仇" class="headerlink" title="复仇"></a>复仇</h1><p>那天早上，小海和复仇女神一起睁开了眼，复仇女神和正义女神沉睡的太久了，今天的场面，她们不想再缺席了。小海一身长褂，一双轻巧的布鞋，和姐夫一起，尾随着土匪，仿佛时间倒流了一般，十几年前的一幕就要复现，只是这次角色颠倒，到了正义女神和复仇女神出场的时候了。土匪停下了脚步，两个尾随的人也停了下来，小海的身子微微动了一下，把枪握的更紧了一点，微风轻轻抚动他的衣角，小海的手插到衣兜里，看了一眼姐夫，点了点头。 他轻缓又坚决地掏出枪，举到和腰部平齐，放在胯旁，身子微微扭动，做好瞄准，左手改在枪身上，以便更好地固定，小海的眼睛微微眯起一点，驻立在射程以内的路旁。他的一席长衣把枪完美地遮挡住，哪怕经验丰富的土匪回头看到他，也不会想到死神的枪膛已经上好，只等着一声脆响，便会张牙舞爪地向他扑来。</p><p>小海一定没看过好莱坞拍摄的西部牛仔片，但是现在的这一幕像极了牛仔，牛仔准备妥当，只等着命运的裁决，男人最英姿飒爽的一刻并不是刻意摆出来的，小海人生中最英武帅气的一刻就在此时了，此刻他才十二岁，还没有成年后一米八的魁伟身姿，但是这一刻，他就是侠客、是武士、是英雄、是死神、是正义的化身。小海的脑海中在电光火石的一瞬间究竟想了什么，恐怕自己也早已忘了，随着一声简简单单的脆响，裁决结束了，不需要第二枪了，第一颗子弹画着凌厉的直线，带着凌厉的意志，穿透了胸膛，穿透了迷雾，穿透了小海的童年，艰辛的悲痛的回忆。</p><p>十二岁，一席长衣飘荡，往事了结，新的人生就要开始了。</p><h1 id="后来的故事："><a href="#后来的故事：" class="headerlink" title="后来的故事："></a>后来的故事：</h1><p>小海和姐夫用同样的方法分别跟踪另外两个土匪并且杀掉了他们，当地丘陵密布，跟踪着稍微容易一点，但是杀父凶手中的主谋，父亲的亲家在两次生死关头都因为旁人路过逃过一劫，小海喃喃着：再一再二，没有再三再四，也许他的命不该由我了结，于是罢了手。</p><p>那个罪大恶极的凶手并没有能够逃脱正义女神的惩罚，正义会迟到，但是正义绝不缺席。仿佛被诅咒了一般，他子嗣断绝，这份恶并没有传下去。</p><p>小海长大成人之后，靠着顽强勇敢、恪守孝悌、尊师重道的品性，在神明的指引与保护下，走过一道道的难关，开枝散叶、传承香火，家族重又兴旺繁盛。</p><p>一百年后，以及更远的未来，小海的后代低声传说着这个故事，这个醇厚绵长、畅快淋漓、激动人心的正义的故事。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;那是上世纪第二个十年发生的故事，年代久远，记忆都已褪去了鲜活的颜色，就像黑白照片一样，边缘还笼罩着一层薄雾。故事是老家的一位年迈的智者在弥留之际讲述的，这个故事在他心中藏了很多年，这个充满了残忍的谋害、热血的沸腾、正义的伸张的故事，糅合在那个特殊的年代，被人藏在心里，藏得太久，当在最后的时刻讲述那个青春飞扬的故事的时候，竟然讲得那么辛苦艰难。&lt;br&gt;这就是那个改变了一个家庭命运的故事：&lt;br&gt;
    
    </summary>
    
      <category term="爷爷回忆录" scheme="http://james20141606.github.io/categories/%E7%88%B7%E7%88%B7%E5%9B%9E%E5%BF%86%E5%BD%95/"/>
    
    
      <category term="life" scheme="http://james20141606.github.io/tags/life/"/>
    
      <category term="autobiograpy" scheme="http://james20141606.github.io/tags/autobiograpy/"/>
    
  </entry>
  
  <entry>
    <title>回忆录 年表</title>
    <link href="http://james20141606.github.io/2018/04/11/auto12/"/>
    <id>http://james20141606.github.io/2018/04/11/auto12/</id>
    <published>2018-04-11T02:09:38.000Z</published>
    <updated>2018-04-12T11:29:50.477Z</updated>
    
    <content type="html"><![CDATA[<h1 id="年表"><a href="#年表" class="headerlink" title="年表"></a>年表</h1><p>  <font size="5"><strong>1940</strong></font> 阴历七月初九出生于石桥街西夹后布袋街外祖母家的巷子里。<br>        辗转生活于薛庄、沙山、石人沟、魏庄。<br>  <font size="5"><strong>1948</strong></font> 患天花<br>  <font size="5"><strong>1949</strong></font> 新中国成立<br>  <font size="5"><strong>1952</strong></font> 土改，在魏庄分得三间大瓦房和几亩地<br>  <font size="5"><strong>1956</strong></font> 读初中<br>  <font size="5"><strong>1959</strong></font> 秋季，就读于郑州农业机械化专科学校（一年后更名为河南<br>        农学院农机分院），现为河南工学院。<br><a id="more"></a><br>  <font size="5"><strong>1961</strong></font> 全国大中院校放长假，停课回家，在生产队种地。<br>  <font size="5"><strong>1962</strong></font> 农历七月十一与王若平结婚，住在魏庄<br>  <font size="5"><strong>1963</strong></font> 阴历五月初一，大儿子陈华军出生于魏庄，送至沽沱抚养。八月下旬重回郑州上学，九月一日报道。<br>  <font size="5"><strong>1964</strong></font> 八月中旬毕业分配在茶庵工作，妻子分配至安皋，两地分居。<br>  <font size="5"><strong>1966</strong></font> 四月，调动至地方国营南阳县拖拉机总站机务科（农机局前<br>       身）<br>  <font size="5"><strong>1967</strong></font> 阴历五月二十九，二儿子陈永出生于南阳地区中心医院。阴  历六月十三，儿媳杜思桂出生<br>  <font size="5"><strong>1968</strong></font> 秋天，妻子担任拖拉机修配厂车工。<br>  <font size="5"><strong>1969</strong></font> 阴历四月初三，三儿子陈斌出生于南阳市医院出生。<br>  <font size="5"><strong>1971</strong></font> 调动至农业局工作，在李华庄药厂担任政工组长。<br>        阴历十一月十七，小儿子陈涛出生于南阳地区中心医院出生。<br>  <font size="5"><strong>1972</strong></font> 在煤厂街盖房子，由妻子主要负责。<br>  <font size="5"><strong>1974</strong></font> 妻子调动至南阳县计委仓库保管员，一边工作一边照顾四个<br>       儿子。<br>  <font size="5"><strong>1976</strong></font> 调动至县委办公室，工作几个月时间。<br>  <font size="5"><strong>1976</strong></font> 阴历十月十九，儿媳宋爱平出生。<br>  <font size="5"><strong>1977</strong></font> 调至组织部工作。<br>  <font size="5"><strong>1978</strong></font> 阴历十月初一，儿媳刘延娇出生。<br>  <font size="5"><strong>1980</strong></font> 阴历三月二十九，儿媳张娟出生。<br>  <font size="5"><strong>1980</strong></font> 阴历七月十二父亲陈锡海逝世，葬于皇路店魏庄<br>  <font size="5"><strong>1983</strong></font> 调动至县人事局右派改正办公室工作，主要负责右派改正。<br>  <font size="5"><strong>1984</strong></font> 调动至南阳县农机局监理站工作。<br>  <font size="5"><strong>1985</strong></font> 腊月初一母亲吕金兰逝世，葬于皇路店魏庄<br>  <font size="5"><strong>1989</strong></font> 阴历八八年腊月十九陈华军与范惠杰结婚、腊月二十陈永与  杜思桂结婚。<br>  <font size="5"><strong>1989</strong></font> 阴历九月初八孙女陈旭鲲出生于南阳地区中心医院。<br>  <font size="5"><strong>1996</strong></font> 妻子退休，55岁。单位为县物资局金属公司。<br>  <font size="5"><strong>1995</strong></font> 阴历八月二十五孙子陈旭鹏出生于南阳地区中心医院。<br>  <font size="5"><strong>1999</strong></font> 阴历二月二十六陈斌与宋爱平结婚。<br>  <font size="5"><strong>2000</strong></font> 因身体原因退休，60岁。<br>  <font size="5"><strong>2001</strong></font> 阴历正月初七孙女陈泓妤出生于南阳市县医院。<br>  <font size="5"><strong>2002</strong></font> 阴历十一月初九陈涛与刘延娇结婚。<br>  <font size="5"><strong>2003</strong></font> 阴历六月初二孙女陈奕霏出生于南阳市县医院。<br>  <font size="5"><strong>2004</strong></font> 阴历三月初九陈华军与张娟结婚。<br>       阴历十一月初四孙女陈佳欣出生于南阳市县医院。<br>  <font size="5"><strong>2012</strong></font> 阴历五月十二孙女陈映竹出生于南阳地区中心医院。<br>  <font size="5"><strong>2016</strong></font> 阴历一五年腊月十三孙子陈旭宏出生于南阳市县医院。<br>  <font size="5"><strong>2017</strong></font> 阴历三月十九，重建老家魏庄的住宅。<br>  <font size="5"><strong>2017</strong></font> 阴历九月初三孙女陈旭鲲与刘一佳结婚。<br>  <font size="5"><strong>2017</strong></font> 年末，老家新房建成</p><h1 id="Family-Tree"><a href="#Family-Tree" class="headerlink" title="Family Tree"></a>Family Tree</h1><p><img src="http://i1.bvimg.com/640680/9a8c9278815028fd.png" alt="Markdown"><br><img src="http://i1.bvimg.com/640680/a4d0b660e1f1400f.png" alt="Markdown"></p>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;年表&quot;&gt;&lt;a href=&quot;#年表&quot; class=&quot;headerlink&quot; title=&quot;年表&quot;&gt;&lt;/a&gt;年表&lt;/h1&gt;&lt;p&gt;  &lt;font size=&quot;5&quot;&gt;&lt;strong&gt;1940&lt;/strong&gt;&lt;/font&gt; 阴历七月初九出生于石桥街西夹后布袋街外祖母家的巷子里。&lt;br&gt;        辗转生活于薛庄、沙山、石人沟、魏庄。&lt;br&gt;  &lt;font size=&quot;5&quot;&gt;&lt;strong&gt;1948&lt;/strong&gt;&lt;/font&gt; 患天花&lt;br&gt;  &lt;font size=&quot;5&quot;&gt;&lt;strong&gt;1949&lt;/strong&gt;&lt;/font&gt; 新中国成立&lt;br&gt;  &lt;font size=&quot;5&quot;&gt;&lt;strong&gt;1952&lt;/strong&gt;&lt;/font&gt; 土改，在魏庄分得三间大瓦房和几亩地&lt;br&gt;  &lt;font size=&quot;5&quot;&gt;&lt;strong&gt;1956&lt;/strong&gt;&lt;/font&gt; 读初中&lt;br&gt;  &lt;font size=&quot;5&quot;&gt;&lt;strong&gt;1959&lt;/strong&gt;&lt;/font&gt; 秋季，就读于郑州农业机械化专科学校（一年后更名为河南&lt;br&gt;        农学院农机分院），现为河南工学院。&lt;br&gt;
    
    </summary>
    
      <category term="爷爷回忆录" scheme="http://james20141606.github.io/categories/%E7%88%B7%E7%88%B7%E5%9B%9E%E5%BF%86%E5%BD%95/"/>
    
    
      <category term="life" scheme="http://james20141606.github.io/tags/life/"/>
    
      <category term="autobiograpy" scheme="http://james20141606.github.io/tags/autobiograpy/"/>
    
  </entry>
  
  <entry>
    <title>Deep Learning Practice</title>
    <link href="http://james20141606.github.io/2018/04/10/Deep-Learning-Practice/"/>
    <id>http://james20141606.github.io/2018/04/10/Deep-Learning-Practice/</id>
    <published>2018-04-10T13:23:55.000Z</published>
    <updated>2018-04-14T17:20:25.227Z</updated>
    
    <content type="html"><![CDATA[<p>这是在实验室Learning Club上的分享，整理到这里。</p><h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>这次learning club希望和大家分享一下如何更好地通过实践来踏进深度学习的领域，深度学习是个快速发展的、牛人云集、前景广阔的领域，深入的研究需要深厚的数学功底和工程实现能力，但是踏入深度学习的领地也绝不那么困哪，大多数的深度学习算法都使用基于python的平台写成，易读性很强，而且相关的资源和原理及代码实现过程中的问题解答都十分充分，很适合快速地学习。<br><a id="more"></a></p><p>也许大家也都有这样的经历：到处看到关于人工智能解决了某某问题的报道，对神经网络、卷积等概念有粗浅的印象，知道深度学习很强大，探讨着和深度学习相关的话题，却是否没有真正去试图学一学，正所谓Talk is Cheap, Show Me the Code. 其实最好的入门方法就是动手试一试。<br>仅仅从入门的角度来看，在现在这个时间节点，搭建起一个基本的神经网络，乃至应用深度学习解决一个自己正在研究的问题，都比以往任何时候简单很多。</p><p>在这个分享中我主要简单介绍了深度学习相关的资源、平台、教程，通过几个例子展示如何利用深度学习平台搭建模型解决一个具体的问题。</p><h2 id="深度学习"><a href="#深度学习" class="headerlink" title="深度学习"></a>深度学习</h2><h3 id="什么是深度学习"><a href="#什么是深度学习" class="headerlink" title="什么是深度学习"></a>什么是深度学习</h3><p><strong>深度学习（deep learning）</strong>是机器学习的分支，是一种试图使用包含复杂结构或由多重非线性变换构成的多个处理层对数据进行高层抽象的算法。<br>深度学习是机器学习中一种基于对数据进行表征学习的算法。观测值（例如一幅图像）可以使用多种方式来表示，如每个像素强度值的向量，或者更抽象地表示成一系列边、特定形状的区域等。而使用某些特定的表示方法更容易从实例中学习任务（例如，人脸识别或面部表情识别）。深度学习的好处是用<strong>非监督式或半监督式的特征学习和分层特征提取高效算法来替代手工获取特征</strong>。</p><p>深度学习是一个框架，包含多个重要算法: </p><ul><li>Convolutional Neural Networks (CNN) 卷积神经网络</li><li>Recurrent neural Network (RNN)  循环神经网络</li><li>AutoEncoder 自动编码器，包括Deep Auto Encoder (DAE) Variational Auto Encoder (VAE)等</li></ul><p>增强学习(Reinforcement Learning)与深度学习的结合也创造了许多了不起的成果，如AlphaGo。</p><h3 id="资源"><a href="#资源" class="headerlink" title="资源"></a>资源</h3><h4 id="书籍资源"><a href="#书籍资源" class="headerlink" title="书籍资源"></a>书籍资源</h4><p><a href="https://www.gitbook.com/book/hit-scir/neural-networks-and-deep-learning-zh_cn/details" target="_blank" rel="noopener">神经网络与深度学习电子书</a><br><a href="http://www.deeplearningbook.org/" target="_blank" rel="noopener">Deep Learning</a>  <strong>(Recommend)</strong><br><a href="https://github.com/exacity/deeplearningbook-chinese" target="_blank" rel="noopener">深度学习中文版</a></p><h4 id="课程资源"><a href="#课程资源" class="headerlink" title="课程资源"></a>课程资源</h4><p><a href="http://cs229.stanford.edu/" target="_blank" rel="noopener">CS229: Machine Learning</a> <strong>(Recommend)</strong></p><p><a href="http://cs231n.stanford.edu/" target="_blank" rel="noopener">Stanford University CS231n: Convolutional Neural Networks for Visual Recognition</a> <strong>(Recommend)</strong></p><p><a href="http://cs224d.stanford.edu/" target="_blank" rel="noopener">CS224d: Deep Learning for Natural Language Processing</a></p><h2 id="快速实践入门"><a href="#快速实践入门" class="headerlink" title="快速实践入门"></a>快速实践入门</h2><h3 id="为什么从实践开始"><a href="#为什么从实践开始" class="headerlink" title="为什么从实践开始"></a>为什么从实践开始</h3><h4 id="内力和剑法哪个更重要"><a href="#内力和剑法哪个更重要" class="headerlink" title="内力和剑法哪个更重要"></a>内力和剑法哪个更重要</h4><p>深度学习新手的入门路径问题：了解模型的内部原理，和快速实现模型解决问题，哪个更重要呢？这篇文章就是关于入门的两种方式的争论：</p><p><a href="https://mp.weixin.qq.com/s/R-29UGMvHyBp8OkWk7zdpw" target="_blank" rel="noopener">深度学习究竟怎么入门？两位Google大神掀起剑气之争</a></p><p>这篇文章讲的非常好，我认为其中的很多大牛的争论和思想已经远远超出深度学习，适用于非常广泛的领域。下面列一下两派人士的观点：</p><h5 id="“气宗”"><a href="#“气宗”" class="headerlink" title="“气宗”"></a>“气宗”</h5><p>代表人物：谷歌大脑的研究员David Ha，<a href="https://arxiv.org/pdf/1704.03477.pdf" target="_blank" rel="noopener">SketchRNN</a>之父。</p><p>观点：从零开始（<strong>用纯Python、numpy</strong>、甚至JS）实现全连接网络、卷积神经网络、RNN、反向传播、SGD，然后用小训练集来训练这些模型是一种学习神经网络如何工作的好方法。在跳到框架上之前，应该花时间从这里收获宝贵的直觉力。</p><h5 id="“剑宗”"><a href="#“剑宗”" class="headerlink" title="“剑宗”"></a>“剑宗”</h5><p>代表人物：谷歌研究员François Chollet，<a href="https://keras.io/" target="_blank" rel="noopener">Keras</a> 之父。</p><p>观点：实现神经网络能教你怎样实现神经网络，让你从算法上理解它们的工作原理。<strong>但这不能教会你它们是做什么的，或者说能否实现哪些功能</strong>。要学习这些，你应该把它们应用到一系列真实问题上去。</p><h5 id="以气驭剑？"><a href="#以气驭剑？" class="headerlink" title="以气驭剑？"></a>以气驭剑？</h5><p>徒手搭模型，显然不是速成之法。那么，花这么多时间“打坐练气”的意义何在？追求的，当然是一个以气驭剑。<br>David Ha说，深度学习框架都是些样板化的模型，入门就用框架，会限制眼界，让你泯然众人。从零开始徒手搭模型就不一样了，对于那些跨行业入门深度学习的人来说，<strong>有机会从自己的独特视角，看到大多数人忽略的东西。</strong></p><p>在斯坦福CS231n课上就要求学生将神经网络的向前和向后传递都用numpy实现一遍（清华大学的人工神经网络课程也选择了这样的教学方式）。学生有意见：拿个框架（tensorflow、keras等）就能自动算的东西，我为什么要徒手来搭？<br>最重要的原因是，徒手将向前和向后传递都实现一遍才能真正理解其中的工作原理。如果单纯依赖框架的自动计算，在反向传播出现问题时就无法应对。</p><p>一些资深码农在入行深度学习时，选择的就是这样的方式，比如闭关花一周时间用C++实现了神经网络的前向、反向传播算法和各种细节，以及故意什么课程文献都不看，自己思考几个月。</p><p><strong>但是</strong>，这对“学生”本身的基础要求非常高：名校计算机系的高材生或者资深码农可以在练气中获得很大的收获。未必适合普通人用来入门</p><h5 id="一招制敌？"><a href="#一招制敌？" class="headerlink" title="一招制敌？"></a>一招制敌？</h5><p>对于上面这种学习方法，Fast.ai创始人Jeremy Howard路过参与到话题中来，吐槽了“气宗”的最强大势力：高校里的计算机专业。他说，计算机学位教育非要让人先从底层开始构建一堆东西，然后才能学习那些抽象的东西，结果他有很多朋友大一没读完就退学了。</p><p>枯燥，确实是徒手搭建神经网络的一大缺陷。而Keras之父Chollet对这种方法的反对是出于别的原因。他觉得这种方法，实在不够学以致用。<strong>招式内部原理分析得再清楚，不知道该用在哪儿，也是白搭。</strong></p><p>Chollet举了很多例子来说明这一观点：<br>练习手写奇异值分解（SVD）有什么用？写完还是不知道SVD能干什么，<strong>把它用到各种数据集上看结果，才能获得直观的印象</strong>。<br>研究生们2000年就会用C语言写神经网络了，可是他们对神经网络的理解，可能还不如一名只会鼓捣框架的2018年高中生，毕竟现在有更多的应用环境。</p><p>现在很多人从没动手实现过神经网络的底层，但是对使用神经网络挺熟练的，知道这个东西如何工作。10年后，这样的人可能会占90%，就像现在的软件工程师了解操作系统，但基本都没开发过操作一样。</p><p>他还来了一段现身说法：“我2009年念书的时候，第一次用C语言写了神经网络，<strong>从中学到的C比神经网络多</strong>。后来又过了好几年我才开始理解神经网络能干什么，为什么有用。关键在于更好的工具+现实世界数据集上的应用。”</p><p>总之，时代在进步，下一代学的东西就是更抽象，不该拿老一套方法来要求学生们了。<strong>他建议学生们去参加Kaggle竞赛，除了神经网络之外也用一用其他的机器学习模型，再通过可视化方法来探索其中的特征。</strong></p><p>“剑招”（tensorflow、keras、pytorch等平台）已经打包好了。Chollet希望未来的学生们知道什么时候该出哪一招，对于内部的原理，其实不必深究。</p><p>搞清细节未必是一开始就必须做的，我们可以折中一下，懂基本概念，平台使用，参数调整，读懂并会用别人代码，懂结果的基本分析，就已经可以算入门了。</p><h3 id="用jupyter-notebook学习各种入门教程"><a href="#用jupyter-notebook学习各种入门教程" class="headerlink" title="用jupyter notebook学习各种入门教程"></a>用jupyter notebook学习各种入门教程</h3><p>Jupyter notebook是适合python以及其他语言的开发环境，非常适合快速地进行实验以及学习。</p><p>Jupyter官方提供的的一些范例  <a href="https://github.com/jupyter/jupyter/wiki/A-gallery-of-interesting-Jupyter-Notebooks" target="_blank" rel="noopener">A gallery of interesting Jupyter Notebooks</a>，涵盖了很多用jupyter展示的不同的学科、语言、以及机器学习、数据科学等问题的入门教程。用这些教程可以更加可视化、更加方便地学习python和一些具体的python实践项目。</p><p>在服务器运行jupyter并在本地浏览器打开的设置在这里。<a href="https://lulab.gitbooks.io/bioinfo-training-2018/content/19-xupeng-chen-linux.html" target="_blank" rel="noopener">jupyter服务器配置</a></p><p><a href="https://github.com/kuleshov/cs228-material/blob/master/tutorials/python/cs228-python-tutorial.ipynb" target="_blank" rel="noopener">CS228 python and numpy tutorial</a></p><p>这是著名的CS231n和CS228课程给学生共同准备的，<strong>入门深度学习必备的python和numpy的知识</strong>，可以让读者快速了解如何使用numpy对矩阵和张量进行操作。而且是用jupyter写的，可读性非常好，基本一个命令一个代码框，非常方便反复的尝试和理解。</p><h4 id="动手实践理解卷积："><a href="#动手实践理解卷积：" class="headerlink" title="动手实践理解卷积："></a>动手实践理解卷积：</h4><h5 id="神经网络在做什么"><a href="#神经网络在做什么" class="headerlink" title="神经网络在做什么"></a>神经网络在做什么</h5><p>这是一个很google的研究院制作的，给初学者体验神经网络是如何work的网页。非常有意思，可以让读者一下子就可视化地明白神经网络究竟在做什么。可以有很多选项来调试你的网络：</p><ul><li>不同的函数对图案有不同的分割方式</li><li>通过增加隐藏层的数量，增加每层神经元数量</li><li>挑选不同的激活函数，如ReLU，tanh，Sigmoid</li><li>调整学习率、Bacth size，Cross validation的比例</li></ul><p>可以直观地看到随着训练轮数增加损失函数在训练集和测试集上的变化，如果遇到了过拟合现象，还可以增加正则化项。可以看到这个全连接网络相邻层神经元之间的连接权重，甚至还可以自己手动调整权重。还可以看到每个神经元在“做什么”，即把图案分成了什么样子，你可以直观的看到通过线性和非线性组合，一个多层的简单神经网络就可以把一个分类或回归问题做的相当好。<br><a href="https://playground.tensorflow.org/#activation=tanh&amp;batchSize=10&amp;dataset=circle&amp;regDataset=reg-plane&amp;learningRate=0.03&amp;regularizationRate=0&amp;noise=0&amp;networkShape=6,5,5,4&amp;seed=0.88851&amp;showTestData=false&amp;discretize=false&amp;percTrainData=50&amp;x=true&amp;y=true&amp;xTimesY=true&amp;xSquared=true&amp;ySquared=true&amp;cosX=false&amp;sinX=true&amp;cosY=false&amp;sinY=true&amp;collectStats=false&amp;problem=regression&amp;initZero=false&amp;hideText=false" target="_blank" rel="noopener">A Neural Network Playground</a>. <strong>(Recommend)</strong></p><h5 id="卷积神经网络为什么有效？"><a href="#卷积神经网络为什么有效？" class="headerlink" title="卷积神经网络为什么有效？"></a>卷积神经网络为什么有效？</h5><p>卷积神经网络可以看做是对视觉神经系统的一种简单模拟<br>1981 年的诺贝尔医学奖，颁发给了 David Hubel（出生于加拿大的美国神经生物学家） 和TorstenWiesel，以及 Roger Sperry。前两位的主要贡献，是<strong>“发现了视觉系统的信息处理”，视觉皮层是分级的。</strong></p><p>对于不同的物体，人类视觉是通过逐层分级，来进行认知的：<br><img src="http://i2.bvimg.com/640680/fefc8e3a01a27627.jpg" alt="Markdown"><br><br><br>在最底层特征基本上是类似的，即各种物体的非常抽象的边缘，越往上，越能提取出此类物体的一些<strong>特征</strong>（轮子、眼睛、躯干等），到更高级的层，不同的高级特征最终组合成相应的图像，从而能够让人类准确的区分不同的物体。</p><p>可不可以模仿人类大脑的这个特点，构造多层的神经网络，较低层的识别初级的图像特征，若干底层特征组成更上一层特征，最终通过多个层级的组合，最终在顶层做出分类呢？<br>这是许多深度学习算法（包括CNN）的灵感来源。其中非常重要的一步就是对图像从低级到高级的特征提取，Lecun等人发现了一种非常好的提取特征的方法，即卷积操作。</p><p>卷积网络通过一系列方法，成功将数据量庞大的图像识别问题不断降维，最终使其能够被训练。CNN最早由Yann LeCun提出并应用在手写字体识别上（MINST）。LeCun提出的网络称为LeNet，其网络结构如下：<br><img src="http://i2.bvimg.com/640680/989124bdbb83b921.jpg" alt="Markdown"></p><p>这是一个最典型的卷积网络，由卷积层、池化层、全连接层组成。其中卷积层与池化层配合，组成多个卷积组，逐层提取特征，最终通过若干个全连接层完成分类。</p><p>卷积核(filter)，感受野，卷积核越小总参数量越小计算量也越小，但是能看到的区域小（折中：Dilated CNN）。<br>Feature map：每一层输入都是一系列叠在一起的二维feature map，与该层的不同卷积核相乘形成新的feature map。<br>此外还有pooling、padding等概念大家可以自行搜索。</p><h5 id="使用opencv中的卷积函数对图像进行变换"><a href="#使用opencv中的卷积函数对图像进行变换" class="headerlink" title="使用opencv中的卷积函数对图像进行变换"></a>使用opencv中的卷积函数对图像进行变换</h5><p>接下来就通过<strong>代码</strong>展示一下卷积是如何实际起作用的，通过opencv中的卷积函数可以对一张计算机视觉领域的经典图片进行一系列的操作。<br><br><br><img src="http://i2.bvimg.com/640680/af93d6a2daaba726.png" alt="Markdown"></p><p><a href="https://pan.baidu.com/s/1L1Wf6tMuA49JtW3yiPDbhQ" target="_blank" rel="noopener">代码在这里</a>  密码: qecg</p><p>代码参考了<a href="https://blog.csdn.net/zouxy09/article/details/49080029" target="_blank" rel="noopener">这篇文章</a>，可以发现通过简单的卷积操作（通过卷积核对原图像矩阵做线性变换）就可以实现边缘检测、锐化、浮雕等多种操作。<br>卷积操作天然地模拟了人眼对物体轮廓的检测，通过多个卷积、池化层的组合，可以解决深度学习领域不同的问题。目前除了图像识别、检测、分割会用到卷积，其他很多需要对数据特征进行抽象和提取的深度学习方法都会广泛的用到卷积操作，甚至包括自然语言处理、结构预测等等。</p><h3 id="深度学习平台介绍"><a href="#深度学习平台介绍" class="headerlink" title="深度学习平台介绍"></a>深度学习平台介绍</h3><p>动手实践深度学习，重要的就是选择一个合适的平台，一个好的深度学习平台会把一些底层的组件实现完毕供人调用，比如卷积核、RNN cell，以及经过优化的反向传播必备的求梯度的方法，如果你手工用numpy实现以下CNN的反向传播，和Tensorflow之类的平台比一下训练的速度，就会发现这些平台在底层做了很多优化，大大提高的训练效率，这在深度学习中非常重要。</p><h4 id="Tensorflow-amp-Keras-Recommend"><a href="#Tensorflow-amp-Keras-Recommend" class="headerlink" title="Tensorflow &amp; Keras.  (Recommend)"></a>Tensorflow &amp; Keras.  (Recommend)</h4><h5 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h5><p>目前应用最广泛的是谷歌家开发的Tensorflow，但是代码较为复杂，实现起来不是很容易。有很多基于tensorflow的高层库，其中最广受欢迎的是<a href="https://mp.weixin.qq.com/s/R-29UGMvHyBp8OkWk7zdpw" target="_blank" rel="noopener">剑气之争</a>中的剑宗代表人物Chollet开发的Keras。</p><p>安装起来颇为简单<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">Tensorflow安装</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> pip install tensorflow      <span class="comment"># Python 2.7; CPU support (no GPU support)</span></span></span><br><span class="line"><span class="meta">$</span><span class="bash"> pip3 install tensorflow     <span class="comment"># Python 3.n; CPU support (no GPU support)</span></span></span><br><span class="line"><span class="meta">$</span><span class="bash"> pip install tensorflow-gpu  <span class="comment"># Python 2.7;  GPU support</span></span></span><br><span class="line"><span class="meta">$</span><span class="bash"> pip3 install tensorflow-gpu <span class="comment"># Python 3.n; GPU support </span></span></span><br><span class="line"><span class="meta">#</span><span class="bash">Keras安装</span></span><br><span class="line">pip install keras</span><br></pre></td></tr></table></figure></p><p>还有其他的安装方式，参考：<br><a href="https://www.tensorflow.org/install/install_linux" target="_blank" rel="noopener">Tensorflow 安装</a><br><a href="https://keras.io/#installation" target="_blank" rel="noopener">Keras 安装</a></p><h5 id="文档"><a href="#文档" class="headerlink" title="文档"></a>文档</h5><p>Tensorflow和Keras都有很详细的文档支持，包括热心用户翻译的中文文档（已经成文官方中文文档）和社区。比如<a href="https://keras-cn.readthedocs.io/en/latest/" target="_blank" rel="noopener">Keras中文文档</a>,<a href="http://www.tensorfly.cn/" target="_blank" rel="noopener">TensorFlow中文社区-首页</a></p><p>因为两个库还处于快速迭代更新的阶段，所以有的时候去它们的github仓库搜索相关的代码也是个解决问题的方式。</p><p><a href="https://github.com/keras-team/keras" target="_blank" rel="noopener">Keras github</a> 该项目共有接近30,000 star</p><p><a href="https://github.com/tensorflow/tensorflow" target="_blank" rel="noopener">Tensorflow github</a>  该项目共有超过90,000 star</p><p>更推荐入门和简单使用时选择Keras，可以和Tensorflow无缝衔接，但是语法简单很多。</p><h5 id="Tensorboard监控学习情况"><a href="#Tensorboard监控学习情况" class="headerlink" title="Tensorboard监控学习情况"></a>Tensorboard监控学习情况</h5><p>Tensorboard是Tensorflow的一大利器，Keras也可以无缝使用Tensorboard，而且在Keras中的使用要简单得多。<br><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">model.fit(images_train, y_train, <span class="attribute">batch_size</span>=32,nb_epoch=150,validation_split=0.2,</span><br><span class="line">callbacks[model_checkpoint,EarlyStopping(<span class="attribute">monitor</span>=<span class="string">'CrossEntropyLoss'</span>, <span class="attribute">patience</span>=10),TensorBoard(log_dir='path)])</span><br><span class="line"></span><br><span class="line"><span class="comment">#训练完成后，输入：</span></span><br><span class="line">tensorboard <span class="attribute">--logdir</span>=path</span><br><span class="line"><span class="comment">#即可在本地浏览器打开查看训练情况</span></span><br></pre></td></tr></table></figure></p><p>Tensorboard可以帮助可视化loss曲线的变化，各项指标的变化，重要参数的分布，计算图的图像等，可以清楚地看到整个网络的逻辑，这些对于分析训练的过程，调整参数都非常有用。</p><p>参考链接：<a href="https://blog.csdn.net/sinat_33761963/article/details/62433234" target="_blank" rel="noopener">06：Tensorflow的可视化工具Tensorboard的初步使用 - CSDN博客</a></p><h4 id="Pytorch-Recommend"><a href="#Pytorch-Recommend" class="headerlink" title="Pytorch  (Recommend)"></a>Pytorch  (Recommend)</h4><p>作为深度学习平台的后起之秀，被Facebook接管的pytorch虽然发布不是很久，但是极受推崇</p><p>在Pytorch发布不到三个月的时候，CS231n课程上已经把作业的实现平台列为Tensorflow和Pytorch二选一，而且在评价各个平台时认为‘<strong>Pytorch is Best</strong>’</p><p>深度学习领域的大神贾扬清说：“如果你想认真学machine learning，那请不要用keras，我一般收到的反馈是，keras做简单的东西容易，一旦你要做点真research，就很难改，因为包装太多。”<br>而Tensorflow虽然可以用来做真正的research，但是它的的问题在于其对于代码水平要求非常高，整个系统抽象层次比较高，有些臃肿，其静态图的构建方式使得更改网络结构很不方便。因此在考虑了模型构建和调试时间的情况下，Pytorch反而效率更高，对人类友好得多。CS231n的著名讲师、Tesla的AI主管Karpathy就这样评价Pytorch:</p><p><img src="http://i2.bvimg.com/640680/58bc934f371dcd6f.jpg" alt="Markdown"></p><p>Pytorch优点：</p><ul><li>入门简单，上手快，堪比Keras。</li><li>代码清晰，设计直观，符合人类直觉。</li><li>Facebook托管，质量不需担心，迭代很快。</li><li>定位是快速实验研究，使用动态图设计，构建新层很迅速</li><li>社区友好，问题解答迅速<br><img src="http://i2.bvimg.com/640680/71440c6ba2a94251.jpg" alt="Markdown"><br>可以看到，pytorch的代码的简洁性已经和numpy做普通的科学计算差不多了，比Tensorflow要简洁许多。</li></ul><p><strong>安装pytorch</strong>：</p><p>打开页面<a href="http://pytorch.org/" target="_blank" rel="noopener">PyTorch</a> ，选择所需要的配置即可，最简单的方法是使用conda安装，非常简介，如linux，python2.7，CUDA8.0，运行下面命令即可：<br><figure class="highlight cmake"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda <span class="keyword">install</span> pytorch torchvision -c pytorch</span><br></pre></td></tr></table></figure></p><p>最后用Pytorch创始人的话结束：<br>PyTorch，就让学生们能充分利用普通Python代码的灵活性和能力，来构建、训练神经网络。这样，他们就能解决更广泛的问题。<br>PyTorch的另一个好处是，它能让学生们更深入地了解每个算法中发生了什么。用TensorFlow那样的静态计算图库，你一旦声明性地表达了你的计算，就把它发送到了GPU，整个处理过程就是一个黑箱。<br>但是通过动态的方法，你可以完全进入计算的每一层，清楚地看到正在发生的情况。我们认为学习深度学习的最佳途径就是通过编程、实验，动态的方法正是我们的学生所需要的。</p><h4 id="高层库"><a href="#高层库" class="headerlink" title="高层库"></a>高层库</h4><p>包括<a href="https://zh.gluon.ai/" target="_blank" rel="noopener">Gluon</a>、<a href="https://github.com/deepmind/sonnet" target="_blank" rel="noopener">Sonnet</a>、<a href="http://tflearn.org/" target="_blank" rel="noopener">TFLearn</a>、<a href="https://github.com/tensorlayer/tensorlayer" target="_blank" rel="noopener">TensorLayer</a>等<br>代码一般更加简洁，适合快速上手学习，但是可能不方便进行深入的研究。</p><h4 id="关于平台的一些教程"><a href="#关于平台的一些教程" class="headerlink" title="关于平台的一些教程"></a>关于平台的一些教程</h4><p>这里推荐的教程都是用jupyter notebook写的，这样非常适合自己一步一步运行代码，尤其适合学习pytorch和keras</p><h5 id="深度学习框架PyTorch：入门与实践"><a href="#深度学习框架PyTorch：入门与实践" class="headerlink" title="深度学习框架PyTorch：入门与实践"></a>深度学习框架PyTorch：入门与实践</h5><p>这个github仓库是<a href="https://github.com/chenyuntc/pytorch-book" target="_blank" rel="noopener">深度学习框架PyTorch的相关代码</a>，也可以作为一个独立的PyTorch入门指南和教程。也是用jupyter notebook教学的，而且Pytorch的特性导致在学习的过程中可以方便地查看变量，很适合初学者，而且该教程用Pytorch实现了几个非常有趣的例子，比如用这两年很火的生成对抗网络(GAN)做了一个生成动漫头像的模型，对图片进行风格迁移以及用神经网络写诗，虽然这不是作者的原创，但是作者的讲解稍微完善，本身就是为了向读者讲解用的，因此更加易于学习</p><h5 id="Keras-Tutorials-Recommend"><a href="#Keras-Tutorials-Recommend" class="headerlink" title="Keras Tutorials   (Recommend)"></a>Keras Tutorials   (Recommend)</h5><p><a href="https://github.com/xingkongliang/Keras-Tutorials" target="_blank" rel="noopener">Keras-Tutorials</a><br>中国人用jupyter notebook写的教程，利用几个经典问题和模型教授Keras相关知识。</p><h4 id="几个不错的基于深度学习平台的学习资源"><a href="#几个不错的基于深度学习平台的学习资源" class="headerlink" title="几个不错的基于深度学习平台的学习资源"></a>几个不错的基于深度学习平台的学习资源</h4><h5 id="动手学深度学习（基于GLUON）-Recommend"><a href="#动手学深度学习（基于GLUON）-Recommend" class="headerlink" title="动手学深度学习（基于GLUON） (Recommend)"></a>动手学深度学习（基于GLUON） (Recommend)</h5><p><a href="https://zh.gluon.ai/" target="_blank" rel="noopener">动手学深度学习 — GLUON</a></p><p>Gluon是基于MXNet的接口，我认为Gluon虽然比较小众，但是代码的清新简洁程度和Pytorch差不多，也很适合用来学习深度学习中的基本概念。</p><p>这个教程是GLUON的开发者亲自完成的，我认为写的非常漂亮，讲解的非常清晰又不乏深度，因此把这个教程排到了第一位。它从线性回归、逻辑回归、感知机开始一路讲到CNN、RNN这样的基础网络，再到计算机视觉和自然语言处理两大经典领域，而且非常注重实践，比如它有一个章节专门讲了个如何用Gluon实战Kaggle上的一个对狗的图片进行分类的问题<a href="https://zh.gluon.ai/chapter_computer-vision/kaggle-gluon-dog.html" target="_blank" rel="noopener">实战Kaggle比赛——使用Gluon识别120种狗</a></p><p>该文档还在不断更新，最近几个月增加了很多新内容，是一个很好的可以跟着学的教程。</p><p>该教程还有视频教学<a href="https://discuss.gluon.ai/t/topic/753" target="_blank" rel="noopener">《动手学深度学习》第一季课程汇总 - 课程信息 - MXNet/Gluon论坛</a></p><h5 id="DeepLearningProject"><a href="#DeepLearningProject" class="headerlink" title="DeepLearningProject"></a>DeepLearningProject</h5><p>一个比较受欢迎的原Harvard TA的对机器学习和深度学习的介绍，每个部分都不是很长，讲解的文字部分比较多，也很适合入门学习。<br><a href="https://spandan-madan.github.io/DeepLearningProject/" target="_blank" rel="noopener">DeepLearning Project</a></p><p>Github:<a href="https://github.com/Spandan-Madan/DeepLearningProject" target="_blank" rel="noopener">An in-depth machine learning tutorial introducing readers to a whole machine learning pipeline from scratch.</a></p><h5 id="DeepSchool-io"><a href="#DeepSchool-io" class="headerlink" title="DeepSchool.io"></a>DeepSchool.io</h5><p><a href="https://github.com/sachinruk/deepschool.io" target="_blank" rel="noopener">deepschool.io: Deep Learning tutorials in jupyter notebooks.</a></p><p>也是个比较受欢迎的深度学习入门教学，也是用jupyter notebook写的，而且顺带讲了一些机器学习的概念，所以也很适合初学者来学习，包括带正则化的线性回归，如何调超参数，XGBoost和sklearn的使用，同时还有一些高级的模型和应用，比如迁移学习，强化学习、对抗网络、语义分析等等。</p><h5 id="其他类似的学习资源"><a href="#其他类似的学习资源" class="headerlink" title="其他类似的学习资源"></a>其他类似的学习资源</h5><ul><li>Fastai出的深度学习教学课程也十分好</li></ul><p><a href="https://github.com/fastai/fastai" target="_blank" rel="noopener">The fast.ai deep learning library, lessons, and tutorials</a></p><ul><li>一个讲了keras和Tensorflow的教程，从MLP到RNN的实现都讲了</li></ul><p><a href="https://github.com/leriomaggio/deep-learning-keras-tensorflow" target="_blank" rel="noopener">Introduction to Deep Neural Networks with Keras and Tensorflow</a></p><ul><li>DeepLearningZeroToAll，这个教程还有视频，用了mxnet、Pytorch、keras在内的多个平台，也很受欢迎。</li></ul><p><a href="https://github.com/hunkim/DeepLearningZeroToAll" target="_blank" rel="noopener">DeepLearningZeroToAll: TensorFlow Basic Tutorial Labs</a></p><h2 id="动手实践部分"><a href="#动手实践部分" class="headerlink" title="动手实践部分"></a>动手实践部分</h2><h3 id="气宗派：徒手实现simple-neural-network"><a href="#气宗派：徒手实现simple-neural-network" class="headerlink" title="气宗派：徒手实现simple neural network"></a>气宗派：徒手实现simple neural network</h3><p><a href="https://github.com/zotroneneis/machine_learning_basics" target="_blank" rel="noopener">这个系列</a>是一个女程序员做的，笔记写的相当好，有详细的公式推导，有很多基础的机器学习的算法和讲解。如果想深入理解诸如前向与反向传播的梯度计算等细节的话可以仔细研究，同样是用jupyter notebook写的，在能够讲解数学原理比较深入的那些教程中应该算是比较友好易学的一个。</p><h3 id="MNIST手写数据识别问题-level-1"><a href="#MNIST手写数据识别问题-level-1" class="headerlink" title="MNIST手写数据识别问题 level 1"></a>MNIST手写数据识别问题 level 1</h3><p>使用Keras方便地搭建MLP和CNN等神经网络解决一个具体的图像识别问题<br><img src="http://i2.bvimg.com/640680/35dfa92b54debbe6.png" alt="Markdown"></p><p>MNIST是一个手写数据集，被誉为机器学习的果蝇(Geofrrey Hinton)，几乎是每一个入门者会尝试的数据集。</p><p>这里使用了两种网络<strong>MLP和CNN</strong>来解决手写数据的分类问题，从载入数据、可视化数据、one hot coding、搭建网络、训练以及测试都有展示。</p><p>还可以使用<strong>Tensorboard</strong>可视化训练过程。</p><p><a href="https://pan.baidu.com/s/1L1Wf6tMuA49JtW3yiPDbhQ" target="_blank" rel="noopener">代码在这里</a>  密码: qecg</p><h3 id="MNIST手写识别及语义分析-level-2"><a href="#MNIST手写识别及语义分析-level-2" class="headerlink" title="MNIST手写识别及语义分析 level 2"></a>MNIST手写识别及语义分析 level 2</h3><p>这是清华大学人工神经网络课程的四次作业：使用numpy实现MLP和CNN，以及使用Tensorflow实现CNN和RNN并用于解决手写数字识别以及语义分割问题，这四个任务的详细要求、数据集和解答已经上传到百度网盘，可以作为一个练习。<a href="https://pan.baidu.com/s/1L1Wf6tMuA49JtW3yiPDbhQ" target="_blank" rel="noopener">代码在这里</a>  密码: qecg。注意在作业中，大多数的代码是写好的，只有少量代码需要自己添加</p><ul><li>用numpy实现MLP解决手写识别</li><li>用numpy实现CNN解决手写识别</li><li>用tensorflow基本函数实现CNN模块与其他结构解决手写识别</li><li>用tensorflow基本函数实现RNN基本模块解决语义分析</li></ul><p>同时这个作业和CS231n的作业十分相像，大家也可以参考这个github仓库：<br><a href="https://github.com/bruceoutdoors/CS231n" target="_blank" rel="noopener">一个学生的CS231n作业</a>  </p><p>它的代码是用jupyter notebook记录的。用numpy实现MLP和CNN的过程相当繁琐，因此用jupyter notebook来学习应该更容易一些。</p><h3 id="使用U-net探索Deepshape问题基本介绍"><a href="#使用U-net探索Deepshape问题基本介绍" class="headerlink" title="使用U-net探索Deepshape问题基本介绍"></a>使用U-net探索Deepshape问题基本介绍</h3><p><a href="https://pan.baidu.com/s/1L1Wf6tMuA49JtW3yiPDbhQ" target="_blank" rel="noopener">代码在这里</a>  密码: qecg</p><h4 id="数据形式"><a href="#数据形式" class="headerlink" title="数据形式"></a>数据形式</h4><p>将同一个序列片段重复128次，用16维one hot向量编码，每个片段形成128<em>128</em>16通道的图片</p><h4 id="U-net"><a href="#U-net" class="headerlink" title="U-net"></a>U-net</h4><p>U-net属于一种全卷积网络（FCN），是解决图像分割尤其是医疗图像问题的非常经典和有用的网络，因为全卷积的设计，可以最终输出图像每个像素点的预测值，而不仅仅是一个或几个数值.<br>一个经典的U-net如图所示<br><img src="http://i2.bvimg.com/640680/acf000d769f49e4f.png" alt="Markdown"></p><p><a href="https://blog.csdn.net/hduxiejun/article/details/71107285" target="_blank" rel="noopener">U-Net 简介</a><br><a href="https://arxiv.org/abs/1505.04597" target="_blank" rel="noopener">U-Net: Convolutional Networks for Biomedical Image Segmentation</a></p><h4 id="设计"><a href="#设计" class="headerlink" title="设计"></a>设计</h4><p>经典的U-net、VGG、Resnet等网络框架，网络上不但有大量的代码，而且还提供了预训练的模型，即在一些公开的数据集，比如ImageNet上预训练过的网络，将其权重存储下来，Keras提供了model. load_model以及load_weights方便用户导入他人提供的模型权重，还可以通过save存储自己的训练好的模型，以备预测或者以后使用。<br><figure class="highlight nix"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">model</span> = UNET_128()</span><br><span class="line"><span class="attr">optim</span> = Adam()</span><br><span class="line">model.compile(<span class="attr">optimizer=optim,</span> <span class="attr">loss=CrossEntropyLoss(model,10),</span> <span class="attr">metrics=[binary_accuracy_with_nan,binary_crossentropy_with_nan,MSE(model)])</span></span><br></pre></td></tr></table></figure></p><p>这里我们采取了Adam作为优化器，它是一种比普通的梯度下降更为有用的方法，Keras在这里列出了多种优化器供选择<a href="https://keras-cn.readthedocs.io/en/latest/other/optimizers/" target="_blank" rel="noopener">优化器Optimizer - Keras中文文档</a><br>Loss function我们使用了自己定义的交叉熵，这是因为我们需要对网络进行一些额外的设计：根据我们所生成的图像，其必然是<strong>对称的</strong>，但是在开始尝试时，模型并不能顾及到这一点，因此我们对模型进行了一些额外的设计：<br>（要注意一点，为了模型能够顺利地进行求梯度以便进行训练，Keras、Tensorflow都要求相关的损失函数和网络层定义必须使用其自己提供的相关函数，而不能用numpy直接做矩阵操作。比如用K.sum()代替np.sum()进行求和。）</p><h5 id="使用Lambda函数按列对图像求平均值"><a href="#使用Lambda函数按列对图像求平均值" class="headerlink" title="使用Lambda函数按列对图像求平均值"></a>使用Lambda函数按列对图像求平均值</h5><figure class="highlight lisp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">final = Lambda(<span class="name">lambda</span> x: K.sum(<span class="name">x</span>, axis=1), output_shape=(<span class="number">128</span>,))(<span class="name">conv_final</span>)</span><br><span class="line">#sum by columns output a vector of <span class="number">128</span> elements</span><br><span class="line">final_ = Reshape((<span class="number">128</span>,), input_shape=(<span class="number">128</span>,<span class="number">1</span>))(<span class="name">final</span>)</span><br><span class="line">final =Lambda(<span class="name">lambda</span> x: x/128.<span class="number">0</span>, output_shape=(<span class="number">128</span>,))(<span class="name">final_</span>)</span><br></pre></td></tr></table></figure><p>因为虽然最终预测结果依然是图像，但是我们的y是一维的向量，我们需要讲对整幅图像的预测值做按行或者按列的平均，再和true value比较。</p><h5 id="维持对称性设计"><a href="#维持对称性设计" class="headerlink" title="维持对称性设计"></a>维持对称性设计</h5><p>这里我们使用了特殊的交叉熵函数，交叉熵是一种经典的衡量分类准确率的损失函数，Keras同样提供了很多种损失函数可供选择<a href="https://keras.io/losses/" target="_blank" rel="noopener">Losses - Keras Documentation</a>，这里要特别注意的是数据中的y有很多NaN值，因此不考虑缺失值点。<br>在我们定义的CrossEntropyLoss损失函数中，我们额外增加了一项用MSE来衡量对称性。通过Keras提供的函数model.get_layer，我们可以方便的操作对网络内部的某层进行操作，我们对网络的最后一个卷积层进行对角线对称性的衡量，添加进损失函数中，在接下来的训练中，预测出的结果的对称性得到了很好的改善。这里也可以体现出深度学习通过损失函数的设计来约束模型的训练就可以达到自己的某种目的“方便”。<br><figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">binary_crossentropy_with_nan</span><span class="params">(y_true, y_pred)</span></span><span class="symbol">:</span></span><br><span class="line">    not_nan = tf.logical_not(tf.is_nan(y_true))</span><br><span class="line">    y_true = tf.boolean_mask(y_true, not_nan)</span><br><span class="line">    y_pred = tf.boolean_mask(y_pred, not_nan)</span><br><span class="line">    <span class="keyword">return</span> K.mean(K.binary_crossentropy(y_pred, y_true), axis=-<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#define new loss using output layer and hidden layer</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CrossEntropyLoss</span>(<span class="title">object</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(<span class="keyword">self</span>, model,alpha)</span></span><span class="symbol">:</span></span><br><span class="line">        <span class="keyword">self</span>.layer = model.get_layer(index = -<span class="number">8</span>).output</span><br><span class="line">        <span class="keyword">self</span>.__name_<span class="number">_</span> = <span class="string">'CrossEntropyLoss'</span></span><br><span class="line">        <span class="keyword">self</span>.alpha = alpha</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__call__</span><span class="params">(<span class="keyword">self</span>,y_true, y_pred)</span></span><span class="symbol">:</span></span><br><span class="line">        <span class="keyword">return</span> binary_crossentropy_with_nan(y_true, y_pred) + <span class="keyword">self</span>.alpha * mean_squared_error(<span class="keyword">self</span>.layer,tf.transpose(<span class="keyword">self</span>.layer, perm=[<span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">3</span>]))</span><br></pre></td></tr></table></figure></p><p>使用model.summary还可以查看网络的结构，每层的数据的维度。</p><h5 id="训练"><a href="#训练" class="headerlink" title="训练"></a>训练</h5><p>接下来就是用数据训练数据，可以定义的参数包括每批投入的样本量，轮数，是否混合，cross validation的比例，是否需要提前中止训练，训练过程保存到Tensorboard，模型权重的保存等等<br><figure class="highlight jboss-cli"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">model_checkpoint = ModelCheckpoint<span class="params">('newunet__row_col_weights_mse.hdf5', <span class="attr">monitor</span>='binary_accuracy_with_nan', <span class="attr">save_best_only</span>=True)</span></span><br><span class="line"></span><br><span class="line">def Model<span class="params">(images_train,y_train,cv,seq_counts)</span>:</span><br><span class="line">    model.fit<span class="params">(images_train, y_train, <span class="attr">batch_size</span>=32, <span class="attr">nb_epoch</span>=150,<span class="attr">verbose</span>=1,<span class="attr">shuffle</span>=True,<span class="attr">validation_split</span>=0.2,<span class="attr">callbacks</span>=[model_checkpoint,EarlyStopping(<span class="attr">monitor</span>='CrossEntropyLoss',<span class="attr">patience</span>=10,<span class="attr">verbose</span>=0)</span>,TensorBoard<span class="params">(<span class="attr">log_dir</span>=path)</span>])</span><br><span class="line">    model.save<span class="params">(path)</span></span><br></pre></td></tr></table></figure></p><p>使用K.function函数，可以查看一个输入数据每一层的输出情况，再通过画图可视化之后，非常适合对模型做进一步的分析。<br><figure class="highlight makefile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">inp = model.input                                           <span class="comment"># input placeholder</span></span><br><span class="line">outputs = [layer.output for layer in model.layers]          <span class="comment"># all layer outputs</span></span><br><span class="line">functor = K.function([inp]+ [K.learning_phase()], outputs ) <span class="comment"># evaluation function</span></span><br><span class="line"><span class="comment"># Testing</span></span><br><span class="line">layer_outs = functor([images_train[80], 1.])</span><br></pre></td></tr></table></figure></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;这是在实验室Learning Club上的分享，整理到这里。&lt;/p&gt;
&lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;p&gt;这次learning club希望和大家分享一下如何更好地通过实践来踏进深度学习的领域，深度学习是个快速发展的、牛人云集、前景广阔的领域，深入的研究需要深厚的数学功底和工程实现能力，但是踏入深度学习的领地也绝不那么困哪，大多数的深度学习算法都使用基于python的平台写成，易读性很强，而且相关的资源和原理及代码实现过程中的问题解答都十分充分，很适合快速地学习。&lt;br&gt;
    
    </summary>
    
      <category term="techniques" scheme="http://james20141606.github.io/categories/techniques/"/>
    
      <category term="machine learning" scheme="http://james20141606.github.io/categories/techniques/machine-learning/"/>
    
    
      <category term="deep learning" scheme="http://james20141606.github.io/tags/deep-learning/"/>
    
  </entry>
  
</feed>
