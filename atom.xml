<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>WonderLand</title>
  
  <subtitle>Somnium &amp; Somniator</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://www.cmwonderland.com/"/>
  <updated>2018-07-22T21:00:50.954Z</updated>
  <id>https://www.cmwonderland.com/</id>
  
  <author>
    <name>James Chen</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Synaptic Partner and Cluster Project</title>
    <link href="https://www.cmwonderland.com/2018/07/14/summerintern_Synaptic_Partner_and_Cluster_Project/"/>
    <id>https://www.cmwonderland.com/2018/07/14/summerintern_Synaptic_Partner_and_Cluster_Project/</id>
    <published>2018-07-15T03:58:06.000Z</published>
    <updated>2018-07-22T21:00:50.954Z</updated>
    
    <content type="html"><![CDATA[<p>It is part of my computational task during my summer intern in Lichtman lab</p><ul><li>It is part of the big synapse project. Also the challenge 3 of <a href="https://cremi.org" target="_blank" rel="noopener">CREMI</a></li></ul><p>The codes related are here: <a href="https://github.com/james20141606/Summer_Intern/tree/master/synaptic_partner" target="_blank" rel="noopener">Summer_Intern/synaptic_partner at master · james20141606/Summer_Intern · GitHub</a></p><ul><li>Also synapse clustering is summarized here, codes related are here: <a href="https://github.com/james20141606/Summer_Intern/tree/master/synapse_cluster" target="_blank" rel="noopener">Summer_Intern/synapse_cluster at master · james20141606/Summer_Intern · GitHub</a></li></ul><a id="more"></a><hr><h1 id="first-two-weeks"><a href="#first-two-weeks" class="headerlink" title="first two weeks"></a>first two weeks</h1><h2 id="creteria"><a href="#creteria" class="headerlink" title="creteria"></a>creteria</h2><p>I try to understand the task3, I read through metrics provided by CREMI website, and find the data processing and evaluation scripts provided by Cremi organization.<br><a href="https://github.com/cremi/cremi_python/blob/master/cremi/evaluation/synaptic_partners.py" target="_blank" rel="noopener">cremi_python/synaptic_partners.py at master · cremi/cremi_python · GitHub</a>. </p><p>They made it very hard to use their pipeline to store and process data. Since we have our own pipeline, I only use the core function about evaluation. By rewriting the scripts I understand how to calculate F-score, it is harder to fully understand the criteria since it needs many steps to calculate, so it is essential to read the original scripts.</p><p>I have summarized the metrics and show it to zudi and donglai.</p><p><img src="http://i2.tiimg.com/640680/ad617dc9489cb429.png" alt="Markdown"></p><p>This means we need the location data and neuron_id for wrong partner evaluation</p><h3 id="steps-to-calculate-F-score"><a href="#steps-to-calculate-F-score" class="headerlink" title="steps to calculate F-score"></a>steps to calculate F-score</h3><ul><li>cost_matrix :<ul><li>pre_post_locations get pre and post location，may consider offset shift       get rec and gt location</li><li>pre_post_labels  get pre and post segmentation( GT neuron_id) as rec_labels  segmentation[pre]  return the pixel value of a certain coordinates for further comparison of wrong partners for further comparison of wrong partners</li><li>create cost matrix，find the bigger one of rec and gt location as matrix size  init value is  2*matching_threshold</li><li>use cost function to calculate, for every rec and gt pair, input rec_locations[i], gt_locations[j], rec_labels[i], gt_labels[j], matching_threshold</li><li>in cost function, set max_cost = 2*matching_threshold，fisrt if labels1 != labels2(rec_labels[i], gt_labels[j]), from pre_post_labels we can know rec and gt comes from different segmentation(neuron).return max_cost</li><li>cost function continue, use np.linalg.norm to calculate pre and post L2 distance, if any of it is larger than thres, return max_cost. Else return average of pre and post. i.e. if  wrong partner (rec and gt not in same seg),  or FP: succeed thres</li><li>if none of them happens, return average of pre and post</li><li>return to cost_matrix, if distance less than thres, add potential pair count by 1, through loop, cost_matrix is filled with numbers, but the loop traverse rec_locations，gt_locations, positions not traverseed is max_cost</li></ul></li><li>match using Hungarian method<blockquote><p>All detected pairs that have both annotations inside the matching areas of a ground truth pair are considered potential matches. Of all potential matches, we find true matches by solving an assignment problem minimizing the Euclidean distance. Unmatched detected pairs are considered FP, unmatched ground truth pairs FN. The final score is the F1-score of the FPs and FNs.</p><ul><li>use linear_sum_assignment(costs - np.amax(costs) - 1)  np.amax(costs) almost is max_cost(2*matching_threshold)<ul><li>scpiy的linear<em>sum_assignment <strong>solves assign problems</strong>, which is one of the tricky part in this criteria.<br>let X be a boolean matrix where X[i,j] =1 if row i is assigned to column j. Then the optimal assignment has cost $$min \sum_i \sum_j C</em>{ij}X_{ij}$$, it finds the minimum Euclidean distance in all potential pairs. So we can submit all locations in a random way, it doesn’t matter.<ul><li>retain the assigned pairs by distance less than threshold</li></ul></li></ul></li></ul></blockquote></li><li>unmatched in rec = FP，all pairs detected by models subtracted by cost matrix’s pairs are FP</li><li>unmatched in gt = FN，all GT pairs subtracted by cost matrix’s pairs are FN</li><li>all ground truth elements - FN = TP</li><li>Then we can calculate fscore, precision, recall, fp, fn, filtered_matches</li></ul><h2 id="Study-others’-methods"><a href="#Study-others’-methods" class="headerlink" title="Study others’ methods"></a>Study others’ methods</h2><h4 id="1st-place"><a href="#1st-place" class="headerlink" title="1st place"></a>1st place</h4><p>Last month Funkey put a paper <strong>Synaptic partner prediction from point annotations in insect brains</strong> it is the  top in leaderboard method. They combined synapse detection and partner identification into one steps. They train a 3D Unet，directly predict directed edges formed by voxels. Balance computational resource and coverage of synaptic partner. Then calculate score of all edges from one segment to another segment. Threshold and certify candidate synapse，for candidate synapse, calculate the mass center of all related edges, getting coordinates of synapse’s pre and post.</p><ul><li>3D U-Net architecture to directly identify pairs of voxels that are pre- and postsynaptic to each other</li><li>formulate the problem of synaptic partner identiﬁcation as a classiﬁcation problem on long-range edges between voxels to encode <strong>both the presence of a synaptic pair and its direction</strong>. This formulation allows us to <strong>directly learn from synaptic point annotations</strong> instead of more expensive voxel-based synaptic cleft or vesicle annotations.</li><li>The proposed representation also allows us to learn from synaptic point annotations only, since we do not rely on labeled synaptic features, such as synaptic clefts or vesicle clouds.</li></ul><p>From fig 1we can have a connectome matrix, actually it is what the evaluation scripts do(they are written by funkey group too.)</p><h4 id="2nd-place-by-previous-postdoc-in-Hanspeter-group"><a href="#2nd-place-by-previous-postdoc-in-Hanspeter-group" class="headerlink" title="2nd place by previous postdoc in Hanspeter group"></a>2nd place by previous postdoc in Hanspeter group</h4><p>Use 3D convolutional neural network for two steps.<br>3D U-net+3D CNN, Use 3D U-net to learn labels by formulate a function, then prune it by 3D CNN<br><a href="https://www.dropbox.com/s/vug579prnxt454n/miccai18syn.pdf?dl=0" target="_blank" rel="noopener">https://www.dropbox.com/s/vug579prnxt454n/miccai18syn.pdf?dl=0</a><br>学习代码<a href="https://github.com/paragt/EMSynConn" target="_blank" rel="noopener">GitHub - paragt/EMSynConn: One algorithm to detect synaptic location AND connectivity, both dyadic and polyadic, in Electron Microscopy volume.</a></p><h4 id="4th-place-FN-is-good"><a href="#4th-place-FN-is-good" class="headerlink" title="4th place, FN is good"></a>4th place, FN is good</h4><p>Use Asymmetric Unet<br><a href="https://github.com/nicholasturner1/Synaptor" target="_blank" rel="noopener">GitHub - nicholasturner1/Synaptor: Processing voxelwise Convolutional Network output trained to predict synaptic clefts for connectomics</a></p><h2 id="Cluster"><a href="#Cluster" class="headerlink" title="Cluster"></a>Cluster</h2><p>I did some data visualization for cerebellum and CREMI data which will be used in clustering work. Also implement some computer vision algorithm for feature extraction.<a href="https://github.com/james20141606/Summer_Intern/blob/master/synapse_cluster/jupyter/visualize_cerebellum_sample.ipynb" target="_blank" rel="noopener">visualize cerebellum</a></p><p>I am considering to use deep learning based clustering methods, to do feature selection and clustering simultaneously. Also it is essential to consider model interpretability. </p><p>Resources: <a href="http://elektronn.org/" target="_blank" rel="noopener">ELEKTRONN - Convolutional Neural Network Toolkit in Python. Fast GPU acceleration and easy usage.</a> is used for generate skeletons using deep learning.</p><hr><p> Week 3<br>The main focus of week 3 is on NMJ labeling and synapse prediction project. So the progress of clustering and synaptic partner project is not much.</p><h2 id="cluster"><a href="#cluster" class="headerlink" title="cluster"></a>cluster</h2><p>I have further considered <strong>clustering</strong>  project. Since we will have a huge amount of data to cluster, it is a natural thought to use deep learning based clustering method, which I have mentioned last time. After discussion with donglai and zudi, they also agree we can use a (variational) auto-encoder to cluster and analyze the synapse. If have time, I may try 3D VAE to cluster the synapse, but it need some preprocessing work. We may at first align and rotate the 2D image for better results.</p><p>I have read some papers including<br><a href="https://arxiv.org/pdf/1610.07584.pdf" target="_blank" rel="noopener">Learning a Probabilistic Latent Space of Object Shapes via 3D Generative-Adversarial Modeling</a><br><a href="https://arxiv.org/pdf/1801.07648.pdf" target="_blank" rel="noopener">Clustering with Deep Learning:Taxonomy and New Methods</a></p><p>Which I thought will be useful</p><h2 id="synaptic-partner"><a href="#synaptic-partner" class="headerlink" title="synaptic partner"></a>synaptic partner</h2><p>I have read and summarized some paper and codes last week on synaptic partner project. This week zudi and I discuss about previous work’s strategy. We have decided that I read and study the synaptic partner codes written by a previous postdoc. </p><p>Now the codes is incomplete in github, later we may get the complete codes. The codes isn’t in a very good structure, and task 3 is harder and more complex than synapse prediction, so it will take me some time to fully understand the codes and make more improvements on it.</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;It is part of my computational task during my summer intern in Lichtman lab&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;It is part of the big synapse project. Also the challenge 3 of &lt;a href=&quot;https://cremi.org&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;CREMI&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The codes related are here: &lt;a href=&quot;https://github.com/james20141606/Summer_Intern/tree/master/synaptic_partner&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Summer_Intern/synaptic_partner at master · james20141606/Summer_Intern · GitHub&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Also synapse clustering is summarized here, codes related are here: &lt;a href=&quot;https://github.com/james20141606/Summer_Intern/tree/master/synapse_cluster&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Summer_Intern/synapse_cluster at master · james20141606/Summer_Intern · GitHub&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
      <category term="summer intern" scheme="https://www.cmwonderland.com/categories/summer-intern/"/>
    
    
      <category term="deep learning" scheme="https://www.cmwonderland.com/tags/deep-learning/"/>
    
      <category term="project" scheme="https://www.cmwonderland.com/tags/project/"/>
    
      <category term="summer intern" scheme="https://www.cmwonderland.com/tags/summer-intern/"/>
    
      <category term="neural science" scheme="https://www.cmwonderland.com/tags/neural-science/"/>
    
      <category term="Jeff Lichtman" scheme="https://www.cmwonderland.com/tags/Jeff-Lichtman/"/>
    
  </entry>
  
  <entry>
    <title>Synapse Prediction</title>
    <link href="https://www.cmwonderland.com/2018/07/14/summerintern_Synapse_Prediction/"/>
    <id>https://www.cmwonderland.com/2018/07/14/summerintern_Synapse_Prediction/</id>
    <published>2018-07-15T02:50:06.000Z</published>
    <updated>2018-07-22T22:58:59.401Z</updated>
    
    <content type="html"><![CDATA[<p>It is part of my computational task during my summer intern in Lichtman lab</p><ul><li>It is part of the big synapse project. Also the challenge 2 of <a href="https://cremi.org" target="_blank" rel="noopener">CREMI</a></li></ul><p>The codes related are here: <a href="https://github.com/james20141606/Summer_Intern/tree/master/synapse_prediction" target="_blank" rel="noopener">Summer_Intern/synaptic_partner at master · james20141606/Summer_Intern · GitHub</a></p><a id="more"></a><hr><h1 id="First-two-weeks"><a href="#First-two-weeks" class="headerlink" title="First two weeks"></a>First two weeks</h1><h2 id="summarize-and-rewrite-some-data-augmentation-repo"><a href="#summarize-and-rewrite-some-data-augmentation-repo" class="headerlink" title="summarize and rewrite some data augmentation repo"></a>summarize and rewrite some data augmentation repo</h2><p>The three main repos are gunpowder from funke lab, EM-data, EM-seglib from Sebastian lab.</p><p>Gunpowder is quite complicated since it includes a whole pipeline of data preprocessing. It defines some batch provider and batch filter at first, making it harder to understand the data augmentation steps in the middle. Since it is written in python2 and designed for caffe, I rewrite and summarize, compare the gunpowder’s four augmentation methods with Sebastian’s codes.</p><p>Funke said in his paper about CREMI challenge 2 that data augmentation is the most important steps in the whole pipeline. I rewrite the four kinds of data augmentation methods combining Funkey and Sebastian’s codes.<br><a href="https://github.com/james20141606/Summer_Intern/blob/master/synapse_prediction/jupyter/gunpowder.ipynb" target="_blank" rel="noopener">gunpowder</a><br><a href="https://github.com/james20141606/Summer_Intern/blob/master/synapse_prediction/jupyter/gunpowder_cremi.ipynb" target="_blank" rel="noopener">gunpowder on cremi</a></p><ul><li>simple augmentation</li><li>intensity augment</li><li>ElasticAugment</li><li>DefectAugment</li></ul><h2 id="simple-augmentation"><a href="#simple-augmentation" class="headerlink" title="simple augmentation"></a>simple augmentation</h2><p>class gunpowder.SimpleAugment(mirror_only=None, transpose_only=None)<br>Randomly mirror and transpose all Arrays and Points in a batch.</p><p>simple augment can be achieved by EM-segLib/em_segLib/transform.py!<br>in this repo, only transpose_only_xy=True is used!</p><h2 id="intensity-augment"><a href="#intensity-augment" class="headerlink" title="intensity augment"></a>intensity augment</h2><p>class gunpowder.IntensityAugment(array, scale_min, scale_max, shift_min, shift_max, z_section_wise=False)<br>Randomly scale and shift the values of an intensity array.</p><ul><li>array (ArrayKey) – The intensity array to modify.</li><li>scale_min (float) –</li><li>scale_max (float) –</li><li>shift_min (float) –</li><li>shift_max (float) –<br>The min and max of the uniformly randomly drawn scaling and shifting values for the intensity augmentation. Intensities are changed as:</li></ul><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="selector-tag">a</span> = <span class="selector-tag">a</span>.mean() + (a-<span class="selector-tag">a</span>.mean())*scale + shift</span><br></pre></td></tr></table></figure><ul><li>z_section_wise (bool) – Perform the augmentation z-section wise. Requires 3D arrays and assumes that z is the first dimension.</li></ul><h3 id="Randomly-scale-and-shift-the-values-of-an-intensity-array，"><a href="#Randomly-scale-and-shift-the-values-of-an-intensity-array，" class="headerlink" title="Randomly scale and shift the values of an intensity array，"></a>Randomly scale and shift the values of an intensity array，</h3><ul><li>can do z axis sections augmentation，first dim should be z</li><li>normalized array desired</li><li>set scale and shift’s biggest and smallest value</li></ul><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">np<span class="selector-class">.random</span><span class="selector-class">.uniform</span>(low=self<span class="selector-class">.shift_min</span>, high=self.shift_max))</span><br></pre></td></tr></table></figure><p>Return scale and shift value</p><ul><li>for each array: a.mean() + (a-a.mean())*scale + shift</li><li>np.clip(0,1)</li></ul><h3 id="similar-implementation"><a href="#similar-implementation" class="headerlink" title="similar implementation!"></a>similar implementation!</h3><ul><li>scale is similar to DataProvider/python/data_augmentation.py class GreyAugment(DataAugment)!</li><li>DataProvider/python/data_augmentation.py class GreyAugment(DataAugment)<strong>may be better</strong>：</li></ul><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="selector-tag">img</span> *= <span class="number">1</span> + (np<span class="selector-class">.random</span><span class="selector-class">.rand</span>() - <span class="number">0.5</span>)*self.CONTRAST_FACTOR</span><br><span class="line"><span class="selector-tag">img</span> += (np<span class="selector-class">.random</span><span class="selector-class">.rand</span>() - <span class="number">0.5</span>)*self.BRIGHTNESS_FACTOR</span><br><span class="line"><span class="selector-tag">img</span> = np.<span class="attribute">clip</span>(img, <span class="number">0</span>, <span class="number">1</span>)</span><br><span class="line"><span class="selector-tag">img</span> **= <span class="number">2.0</span>**(np<span class="selector-class">.random</span><span class="selector-class">.rand</span>()*<span class="number">2</span> - <span class="number">1</span>)</span><br></pre></td></tr></table></figure><ul><li>it borrows from (<a href="http://elektronn.org/).ELEKTRONN" target="_blank" rel="noopener">http://elektronn.org/).ELEKTRONN</a> is used for 2D/3D large scale image. Also used for segmentation</li></ul><h2 id="ElasticAugment"><a href="#ElasticAugment" class="headerlink" title="ElasticAugment"></a>ElasticAugment</h2><p>The author used ElasticAugment([4,40,40], [0,2,2], [0,math.pi/2.0], prob_slip=0.05,prob_shift=0.05,max_misalign=25)</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">class gunpowder.ElasticAugment(control_point_spacing, jitter_sigma, rotation_interval, <span class="attribute">prob_slip</span>=0, <span class="attribute">prob_shift</span>=0, <span class="attribute">max_misalign</span>=0, <span class="attribute">subsample</span>=1)</span><br></pre></td></tr></table></figure><ul><li>reshape array data into (channels,) + spatial dims</li><li>first  <strong>create_identity_transformation</strong><ul><li>create_identity_transformation from funkey another repo augment，create_identity_transformation, channel change to three , use np.meshgrid increase two channels</li></ul></li><li>if sum(jitter_sigma) &gt; 0: <strong>create_elastic_transformation</strong></li><li><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">augment.create_elastic_transformation(</span><br><span class="line">                    target_shape,</span><br><span class="line">                    self<span class="selector-class">.control_point_spacing</span>,</span><br><span class="line">                    self<span class="selector-class">.jitter_sigma</span>,</span><br><span class="line">                    subsample=self.subsample)</span><br></pre></td></tr></table></figure></li><li><p>first get control_point_offsets，the interpolation to upscale，generate 3 channel images</p></li><li><p>then by rotation_interval [0,math.pi/2.0], rotation_start = rotation_interval[0], rotation_max_amount = rotation_interval[1] - rotation_interval[0]</p><p>  rotation = random.random()*self.rotation_max_amount + self.rotation_start(less than 90)</p></li></ul><p>If rotation&gt;0,do <strong>create_rotation_transformation</strong></p><ul><li>then if prob_slip + prob_shift &gt; 0，做<strong>__misalign(transformation)</strong></li></ul><p>According to thres to do randomly shift by section</p><ul><li><p>at last if subsample &gt;1 before elastic augmentation do subsampling to speed up elasticaugment after augmentation then do upscale</p><p>  Instead of creating an elastic transformation on the full<br>  resolution, create one subsampled by the given factor, and linearly<br>  interpolate to obtain the full resolution transformation. This can<br>  significantly speed up this node, at the expense of having visible<br>  piecewise linear deformations for large factors. Usually, a factor<br>  of 4 can savely by used without noticable changes. However, the<br>  default is 1 (i.e., no subsampling).</p></li></ul><h2 id="DefectAugment"><a href="#DefectAugment" class="headerlink" title="DefectAugment"></a>DefectAugment</h2><p>class gunpowder.DefectAugment(intensities, prob_missing=0.05, prob_low_contrast=0.05, prob_artifact=0.0, prob_deform=0.0, contrast_scale=0.1, artifact_source=None, artifacts=None, artifacts_mask=None, deformation_strength=20, axis=0)[source]</p><p>Augment intensity arrays section-wise with artifacts like missing sections, low-contrast sections, by blending in artifacts drawn from a separate source, or by deforming a section.</p><ul><li>intensities (ArrayKey) – The key of the array of intensities to modify.</li><li>prob_missing (float) –</li><li>prob_low_contrast (float) –</li><li>prob_artifact (float) –</li><li>prob_deform (float) – Probabilities of having a missing section, low-contrast section, an artifact (see param artifact_source) or a deformed slice. The sum should not exceed 1. Values in missing sections will be set to 0.<br>contrast_scale (float, optional) – By how much to scale the intensities for a low-contrast section, used if prob_low_contrast &gt; 0.</li><li><p>(class (artifact_source) – BatchProvider, optional):A gunpowder batch provider that delivers intensities (via ArrayKey artifacts) and an alpha mask (via ArrayKey artifacts_mask), used if prob_artifact &gt; 0.</p></li><li><p>artifacts (ArrayKey, optional) – The key to query artifact_source for to get the intensities of the artifacts.</p></li><li>artifacts_mask (ArrayKey, optional) – The key to query artifact_source for to get the alpha mask of the artifacts to blend them with intensities.</li><li>deformation_strength (int, optional) – Strength of the slice deformation in voxels, used if prob_deform &gt; 0. The deformation models a fold by shifting the section contents towards a randomly oriented line in the section. The line itself will be drawn with a value of 0.</li><li>axis (int, optional) – Along which axis sections are cut.</li></ul><p>From a special artifect source read data and defectaugment</p><figure class="highlight lisp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">artifact_source = (</span><br><span class="line">        Hdf5Source(</span><br><span class="line">            'sample_ABC_padded_20160501.defects.hdf',</span><br><span class="line">            datasets = &#123;</span><br><span class="line">                ArrayKeys.RAW: 'defect_sections/raw',</span><br><span class="line">                ArrayKeys.ALPHA_MASK: 'defect_sections/mask',</span><br><span class="line">            &#125;</span><br><span class="line">        ) +</span><br><span class="line">        RandomLocation(<span class="name">min_masked=0</span>.<span class="number">05</span>, mask_array_key=ArrayKeys.ALPHA_MASK) +</span><br><span class="line">        Normalize() +</span><br><span class="line">        IntensityAugment(<span class="number">0.9</span>, <span class="number">1.1</span>, <span class="number">-0.1</span>, <span class="number">0.1</span>, z_section_wise=True) +</span><br><span class="line">        ElasticAugment([<span class="number">4</span>,<span class="number">40</span>,<span class="number">40</span>], [<span class="number">0</span>,<span class="number">2</span>,<span class="number">2</span>], [<span class="number">0</span>,math.pi/2.<span class="number">0</span>]) +</span><br><span class="line">        SimpleAugment(<span class="name">transpose_only_xy=True</span>)</span><br><span class="line">    )</span><br></pre></td></tr></table></figure><ul><li><p>threshold：</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">DefectAugment(</span><br><span class="line">            <span class="attribute">prob_missing</span>=0.03,</span><br><span class="line">            <span class="attribute">prob_low_contrast</span>=0.01,</span><br><span class="line">            <span class="attribute">prob_artifact</span>=0.03,</span><br><span class="line">            <span class="attribute">artifact_source</span>=artifact_source,</span><br><span class="line">            <span class="attribute">contrast_scale</span>=0.1)</span><br></pre></td></tr></table></figure></li><li><p>get missing，low_contrast, artifact, deform value    prob_missing_threshold = self.prob_missing   (0.03=0.03)<br>  prob_low_contrast_threshold = prob_missing_threshold + self.prob_low_contrast  (0.04=0.03+0.01)<br>  prob_artifact_threshold = prob_low_contrast_threshold + self.prob_artifact    (0.07=0.03+0.04)<br>  prob_deform_slice = prob_artifact_threshold + self.prob_deform   (0.07=0.07+0)</p></li><li>for each slice，generate 0-1 random value r，if：<ul><li>r &lt; prob_missing_threshold:  ‘zero_out’<ul><li>do nothing</li></ul></li><li>elif r &lt; prob_low_contrast_threshold:  ‘lower_contrast’<ul><li>mean = section.mean(), section -= mean, section *= self.contrast_scale, section += mean</li></ul></li><li>elif r &lt; prob_artifact_threshold:’artifact’<ul><li>raw.data[section_selector] = section<em>(1.0 - artifact_alpha) + artifact_raw</em>artifact_alpha</li><li>artifact_alpha, artifact_raw from artifact_source’s mask and raw， equals to a <strong>Alpha Blending</strong></li></ul></li><li>elif r &lt; prob_deform_slice: ‘deformed_slice’ if deformed slice, needs bigger upstream roi for deformed slice</li></ul></li></ul><figure class="highlight makefile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">section = raw.data[section_selector].squeeze()</span><br><span class="line">interpolation = 3 if self.spec[self.intensities].interpolatable <span class="keyword">else</span> 0</span><br><span class="line">flow_x, flow_y, line_mask = self.deform_slice_transformations[c]</span><br><span class="line">shape = section.shape</span><br><span class="line"><span class="comment">#做双线性插值</span></span><br><span class="line">section = map_coordinates(</span><br><span class="line">    section, (flow_y, flow_x), mode='constant', order=interpolation</span><br><span class="line">).reshape(shape)</span><br><span class="line">section = np.clip(section, 0., 1.)</span><br><span class="line">section[line_mask] = 0</span><br><span class="line">raw.data[section_selector] = section</span><br></pre></td></tr></table></figure><p><strong>cremi exapmle</strong><br><a href="https://github.com/funkey/gunpowder/tree/master/examples/cremi" target="_blank" rel="noopener">gunpowder/examples/cremi at master · funkey/gunpowder · GitHub</a></p><p>docker has problems (MALIS, pycaffe)<br>Install docker on server <a href="https://docs.docker.com/install/linux/docker-ce/ubuntu/#install-from-a-package" target="_blank" rel="noopener">Get Docker CE for Ubuntu | Docker Documentation</a></p><figure class="highlight applescript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">docker pull funkey/gunpowder</span><br><span class="line"></span><br><span class="line">sudo docker images</span><br><span class="line">sudo docker <span class="built_in">run</span> -i -t <span class="comment">--name=chen ubuntu:latest</span></span><br><span class="line"><span class="keyword">exit</span>  </span><br><span class="line">docker start <span class="built_in">id</span> / <span class="built_in">name</span></span><br><span class="line">docker attach <span class="built_in">id</span> /<span class="built_in">name</span></span><br></pre></td></tr></table></figure><p>Encounter problems with installing nvidia docker and docker run.</p><p>Seems we should try to rewrite some codes to incorporate directly without the bothering of docker, python2, caffe, malis and other settings.</p><h3 id="resources"><a href="#resources" class="headerlink" title="resources"></a>resources</h3><p><a href="https://github.com/donglaiw/em-data" target="_blank" rel="noopener">GitHub - donglaiw/EM-data: dataset loader</a><br><a href="https://github.com/torms3/DataProvider/tree/refactoring/python/dataprovider" target="_blank" rel="noopener">DataProvider/python/dataprovider at refactoring · torms3/DataProvider · GitHub</a><br><a href="https://arxiv.org/abs/1706.00120" target="_blank" rel="noopener">1706.00120 Superhuman Accuracy on the SNEMI3D Connectomics Challenge</a> Sebastian group</p><p>Then I incorporate the data augmentation methods into synapse prediction model for further use.<br><a href="https://github.com/james20141606/Summer_Intern/blob/master/synapse_prediction/jupyter/augment_cremi.ipynb" target="_blank" rel="noopener">augmentation on cremi</a></p><h3 id="Model-test-and-redesign"><a href="#Model-test-and-redesign" class="headerlink" title="Model test and redesign"></a>Model test and redesign</h3><p>I tried zudi’s synapse prediction model using 3D U-net, since it is based on pytorch, I spent some time to read the pytorch documentation. We have discussed about the model improvement issue and I have tried to use other architecture for better prediction results.<br><a href="https://github.com/james20141606/Summer_Intern/blob/master/synapse_prediction/jupyter/pytorch_synapse.ipynb" target="_blank" rel="noopener">synapse prediction model</a><br><a href="https://github.com/james20141606/Summer_Intern/blob/master/synapse_prediction/jupyter/syn_pytorch_add_dink.ipynb" target="_blank" rel="noopener">improvement on U net</a></p><p>I can try a lot fine tune and redesign about U-net and 3D U-net, maybe from kaggle top solutions and github. </p><hr><h1 id="Week-3"><a href="#Week-3" class="headerlink" title="Week 3"></a>Week 3</h1><p>In week 3, apart from NMJ project, I concentrate mainly on synapse prediction project and make many progress. Since zudi has come back, it is more efficient to discuss and collaborate, we have done a lot this week.</p><h2 id="short-term-plan"><a href="#short-term-plan" class="headerlink" title="short term plan"></a>short term plan</h2><p>Our short term plan is adding all the new tricks (dilation CNN block from Deepglobe’s Dlink-net, DICE_BCE combined loss, all the data augmentation methods) to the model. We train the whole model for three days on all A, B, C samples. And fine tune it later for one day. Then we submit the result to see if it is better.</p><p>In my consideration, the previous model <strong>has a not very little gap with the state-of-art one</strong>. About one fold error score. So it is a lot for us to do. <strong>I have used dilation CNN, combined Loss, and some very useful data augmentation methods</strong> which take me many days to accomplish(All admit that proper augmentation is one of the most import procedure to achieve better results.) So this week I have read many other people’s good paper, project summary and codes in similar tasks( <strong>the dilation CNN, DICE_BCE loss and data augmentation all originate from this.</strong>) I believe there are more to implement. The state-of-art model and pipeline looks really good, so I think I should use more strategy to improve our results.</p><h2 id="Model-improvement-and-training"><a href="#Model-improvement-and-training" class="headerlink" title="Model improvement and training"></a>Model improvement and training</h2><h3 id="settings-and-training"><a href="#settings-and-training" class="headerlink" title="settings and training"></a>settings and training</h3><p> Since I have done several deep learning and machine learning projects(Emaize, CardiacAI, 3D CT images, Deepshape), I am quite familiar with linux, python deep learning environment and all extra settings( jupyter, TensorboardX etc). I can be quite efficient in running the previous model designed by zudi.</p><h4 id="training-settings"><a href="#training-settings" class="headerlink" title="training settings"></a>training settings</h4><h5 id="create-envs"><a href="#create-envs" class="headerlink" title="create envs"></a>create envs</h5><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">conda env <span class="keyword">create</span> -f <span class="keyword">bin</span>/synapse_pytorch/envs/py3_pytorch.yml</span><br><span class="line"><span class="keyword">source</span> <span class="keyword">activate</span> py3_pytorch</span><br><span class="line"><span class="keyword">source</span> deactivate</span><br><span class="line"><span class="keyword">alias</span> act=<span class="string">'source activate py3_pytorch'</span></span><br><span class="line"><span class="keyword">alias</span> deact=<span class="string">'source deactivate'</span></span><br><span class="line">virtualenv venv</span><br></pre></td></tr></table></figure><h5 id="training-parameter-setting"><a href="#training-parameter-setting" class="headerlink" title="training parameter setting"></a>training parameter setting</h5><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">CUDA_VISIBLE_DEVICES=<span class="number">0</span>,<span class="number">1</span> python3 -u bin/synapse_pytorch/train<span class="selector-class">.py</span> -t data/cremi/ -dn images/im_A_v2_200.h5@images/im_B_v2_200.h5@images/im_C_v2_200<span class="selector-class">.h5</span> -ln gt-syn/syn_A_v2_200.h5@gt-syn/syn_B_v2_200.h5@gt-syn/syn_C_v2_200<span class="selector-class">.h5</span> -o outputs/cremi0719mixloss -lr <span class="number">0.001</span> --volume-total <span class="number">400000</span> --volume-save <span class="number">20000</span> -mi <span class="number">24</span>,<span class="number">256</span>,<span class="number">256</span> -g <span class="number">2</span> -c <span class="number">6</span> -<span class="selector-tag">b</span> <span class="number">2</span> -l mix</span><br><span class="line"><span class="selector-id">#b</span>:<span class="number">6</span>  try to keep gpu and batch size same</span><br></pre></td></tr></table></figure><h5 id="Tensorboard-monitor-setting"><a href="#Tensorboard-monitor-setting" class="headerlink" title="Tensorboard monitor setting"></a>Tensorboard monitor setting</h5><p>it is a real time monitoring</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#http://140.247.107.75:6006</span></span><br><span class="line"><span class="comment">#set locale to solve locale unsupported locale #setting problems</span></span><br><span class="line"><span class="builtin-name">export</span> <span class="attribute">LANGUAGE</span>=en_US.UTF-8</span><br><span class="line"><span class="builtin-name">export</span> <span class="attribute">LANG</span>=en_US.UTF-8</span><br><span class="line"><span class="builtin-name">export</span> <span class="attribute">LC_ALL</span>=en_US.UTF-8</span><br><span class="line"><span class="comment">#tensorboard monitor dir at: synapse/runs/outputs</span></span><br><span class="line">tensorboard --logdir cremi0719</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"><span class="built_in">kill</span> jupyter and release port</span></span><br><span class="line">ps aux | grep -i notebook</span><br></pre></td></tr></table></figure><h3 id="network-visualization"><a href="#network-visualization" class="headerlink" title="network visualization"></a>network visualization</h3><p>I visualize our current model, it is a very big one.</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">import torch</span><br><span class="line"><span class="keyword">from</span> torch.autograd import Variable</span><br><span class="line"><span class="keyword">from</span> torchviz import make_dot</span><br><span class="line">model = res_unet()</span><br><span class="line">model = model.<span class="keyword">to</span>(device)</span><br><span class="line">x = Variable(torch.randn(1,1,24, 256,256))#change 12 <span class="keyword">to</span> the channel number of<span class="built_in"> network </span>input</span><br><span class="line">y = model(x)</span><br><span class="line">g = make_dot(y)</span><br><span class="line">g.view()</span><br></pre></td></tr></table></figure><p><strong>Current model:</strong></p><p><img src="http://i2.tiimg.com/640680/aac9181df1496b48.png" alt="Markdown"></p><p>Scripts: <a href="https://github.com/james20141606/Summer_Intern/blob/master/synapse_prediction/jupyter/pytorch_synapse.ipynb" target="_blank" rel="noopener">Summer_Intern/pytorch_synapse.ipynb at master · james20141606/Summer_Intern · GitHub</a></p><h3 id="model-structure-and-loss-function-improvement"><a href="#model-structure-and-loss-function-improvement" class="headerlink" title="model structure and loss function improvement"></a>model structure and loss function improvement</h3><p>I read D-LinkNet: LinkNet with Pretrained Encoder and Dilated Convolution for High Resolution Satellite Imagery Road Extraction and other winners in some challenges and implement more model structure and loss function design.</p><h5 id="dilated-CNN-block"><a href="#dilated-CNN-block" class="headerlink" title="dilated CNN block"></a>dilated CNN block</h5><p>It can further enlarge the visual area of the model, which is helpful to learn more information efficiently. The dilation is adaptive according the the layer depth. The maximum is 8, we may change it smaller later.</p><h5 id="loss-function"><a href="#loss-function" class="headerlink" title="loss function"></a>loss function</h5><p>I read Deepglobe challenge’s solution and change the loss function to <strong>DICE + BCE</strong> like this:<br>P: predict result, GT: ground truth, N: batch size</p><script type="math/tex; mode=display">L = 1 - \frac{2 \times \sum_{i=1}^N  |P_i \cap GT_i  | }{\sum_{i=1}^N  (P_i + GT_i)} + \sum_{i=1}^N BCELoss(P_i,  GT_i)</script><p>It added a DICE part to previous BCE loss function, which is better since it is common to use DICE as loss function for U-net, the combined loss function is also used in some challenges winner.</p><p>loss function implementation:<br><a href="https://github.com/james20141606/Summer_Intern/blob/master/synapse_prediction/bin/loss.py" target="_blank" rel="noopener">Summer_Intern/loss.py at master · james20141606/Summer_Intern · GitHub</a><br><a href="https://github.com/james20141606/Summer_Intern/blob/master/synapse_prediction/jupyter/loss_dlink_net.ipynb" target="_blank" rel="noopener">Summer_Intern/loss_dlink_net.ipynb at master · james20141606/Summer_Intern · GitHub</a></p><figure class="highlight stata"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> dice_bce_loss(nn.Module):</span><br><span class="line">    def __init__(self, batch=True):</span><br><span class="line">        super(dice_bce_loss, self).__init__()</span><br><span class="line">        self.batch = batch</span><br><span class="line">        self.bce_loss = WeightedBCELoss()</span><br><span class="line"></span><br><span class="line">    def soft_dice_coeff(self, <span class="keyword">input</span>, target):</span><br><span class="line">        <span class="keyword">smooth</span> = 0.0  # may change</span><br><span class="line">        <span class="keyword">if</span> self.batch:</span><br><span class="line">            i = torch.<span class="built_in">sum</span>(target)</span><br><span class="line">            j = torch.<span class="built_in">sum</span>(<span class="keyword">input</span>)</span><br><span class="line">            intersection = torch.<span class="built_in">sum</span>(target * <span class="keyword">input</span>)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            i = target.<span class="built_in">sum</span>(1).<span class="built_in">sum</span>(1).<span class="built_in">sum</span>(1)</span><br><span class="line">            j = <span class="keyword">input</span>.<span class="built_in">sum</span>(1).<span class="built_in">sum</span>(1).<span class="built_in">sum</span>(1)</span><br><span class="line">            intersection = (target * <span class="keyword">input</span>).<span class="built_in">sum</span>(1).<span class="built_in">sum</span>(1).<span class="built_in">sum</span>(1)</span><br><span class="line">        <span class="keyword">score</span> = (2. * intersection + <span class="keyword">smooth</span>) / (i + j + <span class="keyword">smooth</span>)</span><br><span class="line">        #<span class="keyword">score</span> = (intersection + <span class="keyword">smooth</span>) / (i + j - intersection + <span class="keyword">smooth</span>)#iou</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">score</span>.<span class="keyword">mean</span>()</span><br><span class="line"></span><br><span class="line">    def soft_dice_loss(self, <span class="keyword">input</span>, target):</span><br><span class="line">        loss = 1 - self.soft_dice_coeff(<span class="keyword">input</span>, target)</span><br><span class="line">        <span class="keyword">return</span> loss</span><br><span class="line"></span><br><span class="line">    def __call__(self, <span class="keyword">input</span>, target, weight):</span><br><span class="line">        a = self.bce_loss(<span class="keyword">input</span>, target, weight)</span><br><span class="line">        b = self.soft_dice_loss(<span class="keyword">input</span>, target)</span><br><span class="line">        <span class="keyword">return</span> a + b</span><br></pre></td></tr></table></figure><p>I use TensorboardX to monitor the training procedure. I monitor the combined loss, DICE loss and BCE loss simultaneously. To my relief, the loss function decrease not only because BCE decrease, the DICE loss also decrease. Since 1- DICE reflect the overlap ratio of predicted and ground truth area, it means our model learns well.</p><p><strong>Train loss:</strong><br><img src="http://i4.fuimg.com/640680/03763aade56aca61.png" alt="Markdown"><br><strong>BCE loss:</strong><br><img src="http://i4.fuimg.com/640680/a3694f6e59e4a242.png" alt="Markdown"><br><strong>DICE loss:</strong><br><img src="http://i4.fuimg.com/640680/163718480760fffd.png" alt="Markdown"></p><p>We should do further improvement on loss function, BCE is divided by batch size, so we may do the same with DICE.</p><p>We think BCE punishes FN since we set a weight to punish it. And we think DICE is punishing FP since the remained are mainly FP, and we think it is good: our strategy is remove FN at first, then we can prune FP using some methods. So DICE loss may become one of our methods.</p><p><strong>We can also balance the FP, FN by adjusting DICE and BCE’s weight. It is essential for us to decrease FP since it is the main gap between the state of art model. We retain so many FP in predicting.</strong></p><h2 id="Data-augmentation"><a href="#Data-augmentation" class="headerlink" title="Data augmentation"></a>Data augmentation</h2><p><strong>I spent many time on this part because I believe it is the most important part besides the model, also this part may determine which pipeline is better between many good models.</strong></p><p>Last week we have discussed a lot about augmentation’s details. This week I rewrite several of them, and learned many different augmentation methods from many sources( mature pipeline, API, challenge winner and papers) I compare their results by rewrite their codes and visualize the results. Some of the codes  I read are hard to understand(like gunpowder) since it contain a whole pipeline’s complex manipulation. </p><p><strong>I have applied several of them and implement them in training and testing pipeline.</strong></p><p>The codes for final use is: <a href="https://github.com/james20141606/Summer_Intern/blob/master/synapse_prediction/bin/augmentation.py" target="_blank" rel="noopener">Summer_Intern/augmentation.py at master · james20141606/Summer_Intern · GitHub</a><br>And the implementation and visualization codes:<br><a href="https://github.com/james20141606/Summer_Intern/blob/master/synapse_prediction/jupyter/cremi_augmentation_implementation.ipynb" target="_blank" rel="noopener">Summer_Intern/cremi_augmentation_implementation.ipynb at master · james20141606/Summer_Intern · GitHub</a><br><a href="https://github.com/james20141606/Summer_Intern/blob/master/synapse_prediction/jupyter/augment_cremi.ipynb" target="_blank" rel="noopener">Summer_Intern/augment_cremi.ipynb at master · james20141606/Summer_Intern · GitHub</a></p><h3 id="training"><a href="#training" class="headerlink" title="training"></a>training</h3><p>I do transformation on image and mask at the same time. For simple augmentation it is simple, for elastic transformation, I use random seed to reproduce, and binarize mask data to 0, 1 again. For intensity augmentation  I don’t do any transformation on mask. For defect transformation, there are many to notice, including random seed to reproduce and binarize. </p><p>I do many visualization to benchmark every augmentation</p><h5 id="train"><a href="#train" class="headerlink" title="train"></a>train</h5><h4 id="simple-augmentation-1"><a href="#simple-augmentation-1" class="headerlink" title="simple augmentation"></a>simple augmentation</h4><p>Do X, Y, Z mirror and transpose, so in total there are 16 methods. Produce a list to generate four 0, 1 random int to decide do or not do these four methods. Do same transformation on image and mask.</p><p>Usage:  simpleaug_train_produce()<br><img src="http://i1.fuimg.com/640680/b230e7beb1ffb4fd.png" alt="Markdown"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># produce 16 binary arr</span></span><br><span class="line">binaryarr = np.zeros([<span class="number">16</span>, <span class="number">4</span>]).astype(<span class="string">'int'</span>)</span><br><span class="line"><span class="keyword">for</span> t <span class="keyword">in</span> range(<span class="number">16</span>):</span><br><span class="line">    binaryarr[t] = np.concatenate((np.repeat(<span class="number">0</span>, <span class="number">4</span>-len(bin(t)[<span class="number">2</span>:])), np.array(</span><br><span class="line">        [bin(t)[<span class="number">2</span>:][i] <span class="keyword">for</span> i <span class="keyword">in</span> range(len(bin(t)[<span class="number">2</span>:]))]).astype(<span class="string">'int'</span>)))</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">augmentsimple</span><span class="params">(data, rule)</span>:</span></span><br><span class="line">    <span class="keyword">assert</span> np.size(rule) == <span class="number">4</span> <span class="keyword">and</span> data.ndim == <span class="number">3</span></span><br><span class="line">    <span class="comment"># z reflection.</span></span><br><span class="line">    <span class="keyword">if</span> rule[<span class="number">0</span>]:</span><br><span class="line">        data = data[::<span class="number">-1</span>, :, :]</span><br><span class="line">    <span class="comment"># x reflection.</span></span><br><span class="line">    <span class="keyword">if</span> rule[<span class="number">1</span>]:</span><br><span class="line">        data = data[:, :, ::<span class="number">-1</span>]</span><br><span class="line">    <span class="comment"># y reflection.</span></span><br><span class="line">    <span class="keyword">if</span> rule[<span class="number">2</span>]:</span><br><span class="line">        data = data[:, ::<span class="number">-1</span>, :]</span><br><span class="line">    <span class="comment"># Transpose in xy.</span></span><br><span class="line">    <span class="keyword">if</span> rule[<span class="number">3</span>]:</span><br><span class="line">        data = data.transpose(<span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>)</span><br><span class="line">    <span class="keyword">return</span> data</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">produce_simple_16_sample</span><span class="params">(imgs, imgshape)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    imgs: 24*256*256 -&gt; 16*24*256*256</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="keyword">assert</span> imgs.ndim == <span class="number">3</span></span><br><span class="line">    augmentsimplearr = np.ndarray(</span><br><span class="line">        [<span class="number">16</span>, imgshape[<span class="number">0</span>], imgshape[<span class="number">1</span>], imgshape[<span class="number">2</span>]])</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">16</span>):</span><br><span class="line">        augmentsimplearr[i] = augmentsimple(imgs, binaryarr[i])</span><br><span class="line">    <span class="keyword">return</span> augmentsimplearr</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">produce_simple_train_sample</span><span class="params">(imgs, rule)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    imgs: 24*256*256 -&gt; 16*24*256*256</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="keyword">assert</span> imgs.ndim == <span class="number">3</span></span><br><span class="line">    <span class="keyword">return</span> augmentsimple(imgs, rule)</span><br></pre></td></tr></table></figure><h4 id="intensity-augmentation"><a href="#intensity-augmentation" class="headerlink" title="intensity augmentation"></a>intensity augmentation</h4><p>Usage: IntensityAugment()</p><p>Use <strong>mix</strong> mode for random choose 2D or 3D intensity augmentation. I do not transform mask because there is only pixel shift. </p><p><img src="http://i1.fuimg.com/640680/8f09339677f17d82.png" alt="Markdown"></p><figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">IntensityAugment</span>():</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(<span class="keyword">self</span>,  mode=<span class="string">'mix'</span>, skip_ratio=<span class="number">0</span>.<span class="number">3</span>, CONTRAST_FACTOR=<span class="number">0</span>.<span class="number">3</span>, BRIGHTNESS_FACTOR=<span class="number">0</span>.<span class="number">3</span>)</span></span><span class="symbol">:</span></span><br><span class="line">        <span class="string">""</span><span class="string">"</span></span><br><span class="line"><span class="string">        Initialize parameters.</span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            mode: 2D, 3D, or mix</span></span><br><span class="line"><span class="string">        "</span><span class="string">""</span></span><br><span class="line">        assert mode == <span class="string">'3D'</span> <span class="keyword">or</span> mode == <span class="string">'2D'</span> <span class="keyword">or</span> mode == <span class="string">'mix'</span></span><br><span class="line">        <span class="keyword">self</span>.mode = mode</span><br><span class="line">        <span class="keyword">self</span>.ratio = skip_ratio</span><br><span class="line">        <span class="keyword">self</span>.CONTRAST_FACTOR = CONTRAST_FACTOR</span><br><span class="line">        <span class="keyword">self</span>.BRIGHTNESS_FACTOR = BRIGHTNESS_FACTOR</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">augment</span><span class="params">(<span class="keyword">self</span>, imgs)</span></span><span class="symbol">:</span></span><br><span class="line">        skiprand = np.random.rand()</span><br><span class="line">        <span class="keyword">if</span> skiprand &gt; <span class="keyword">self</span>.<span class="symbol">ratio:</span></span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">self</span>.mode == <span class="string">'mix'</span><span class="symbol">:</span></span><br><span class="line">                threshold = <span class="number">1</span>-(<span class="number">1</span>-<span class="keyword">self</span>.ratio)/<span class="number">2</span></span><br><span class="line">                mode<span class="number">_</span> = <span class="string">'3D'</span> <span class="keyword">if</span> skiprand &gt; threshold <span class="keyword">else</span> <span class="string">'2D'</span> </span><br><span class="line">            <span class="symbol">else:</span></span><br><span class="line">                mode<span class="number">_</span> = <span class="keyword">self</span>.mode</span><br><span class="line">            <span class="keyword">if</span> mode<span class="number">_</span> == <span class="string">'2D'</span><span class="symbol">:</span></span><br><span class="line">                imgs = <span class="keyword">self</span>.augment2D(imgs)</span><br><span class="line">            elif mode<span class="number">_</span> == <span class="string">'3D'</span><span class="symbol">:</span></span><br><span class="line">                imgs = <span class="keyword">self</span>.augment3D(imgs)</span><br><span class="line">            <span class="keyword">return</span> imgs</span><br><span class="line">        <span class="symbol">else:</span></span><br><span class="line">            <span class="keyword">return</span> imgs</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">augment2D</span><span class="params">(<span class="keyword">self</span>, imgs)</span></span><span class="symbol">:</span></span><br><span class="line">        <span class="keyword">for</span> z <span class="keyword">in</span> range(imgs.shape[-<span class="number">3</span>])<span class="symbol">:</span></span><br><span class="line">            img = imgs[z, <span class="symbol">:</span>, <span class="symbol">:</span>]</span><br><span class="line">            img *= <span class="number">1</span> + (np.random.rand() - <span class="number">0</span>.<span class="number">5</span>)*<span class="keyword">self</span>.CONTRAST_FACTOR</span><br><span class="line">            img += (np.random.rand() - <span class="number">0</span>.<span class="number">5</span>)*<span class="keyword">self</span>.BRIGHTNESS_FACTOR</span><br><span class="line">            img = np.clip(img, <span class="number">0</span>, <span class="number">1</span>)</span><br><span class="line">            img **= <span class="number">2.0</span>**(np.random.rand()*<span class="number">2</span> - <span class="number">1</span>)</span><br><span class="line">            imgs[z, <span class="symbol">:</span>, <span class="symbol">:</span>] = img</span><br><span class="line">        <span class="keyword">return</span> imgs</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">augment3D</span><span class="params">(<span class="keyword">self</span>, imgs)</span></span><span class="symbol">:</span></span><br><span class="line">        imgs *= <span class="number">1</span> + (np.random.rand() - <span class="number">0</span>.<span class="number">5</span>)*<span class="keyword">self</span>.CONTRAST_FACTOR</span><br><span class="line">        imgs += (np.random.rand() - <span class="number">0</span>.<span class="number">5</span>)*<span class="keyword">self</span>.BRIGHTNESS_FACTOR</span><br><span class="line">        imgs = np.clip(imgs, <span class="number">0</span>, <span class="number">1</span>)</span><br><span class="line">        imgs **= <span class="number">2.0</span>**(np.random.rand()*<span class="number">2</span> - <span class="number">1</span>)</span><br><span class="line">        <span class="keyword">return</span> imgs</span><br></pre></td></tr></table></figure><h4 id="elastic-augmentation"><a href="#elastic-augmentation" class="headerlink" title="elastic augmentation"></a>elastic augmentation</h4><p>Usage: apply_elastic_transform(img, mask)<br>I compare gunpowder and this <a href="https://www.kaggle.com/bguberfain/elastic-transform-for-data-augmentation" target="_blank" rel="noopener">Elastic Transform for Data Augmentation | Kaggle</a> one, after comparison, it seems gunpowder version elastic has more functions, if we do not use rotate, it is fine.<br>The output range of create_transformation is 0-width, we can normalize the results. And I use random seed to reproduce on images and masks</p><p><img src="http://i1.fuimg.com/640680/b57e8e884c754591.png" alt="Markdown"></p><figure class="highlight maxima"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br></pre></td><td class="code"><pre><span class="line">def create_identity_transformation(shape, <span class="built_in">subsample</span>=<span class="number">1</span>):</span><br><span class="line">    dims = len(shape)</span><br><span class="line">    subsample_shape = tuple(<span class="built_in">max</span>(<span class="number">1</span>, int(s/<span class="built_in">subsample</span>)) <span class="keyword">for</span> s <span class="keyword">in</span> shape)</span><br><span class="line">    step_width = tuple(<span class="built_in">float</span>(shape[d]-<span class="number">1</span>)/(subsample_shape[d]-<span class="number">1</span>)</span><br><span class="line">                       <span class="keyword">if</span> subsample_shape[d] &gt; <span class="number">1</span> <span class="keyword">else</span> <span class="number">1</span> <span class="keyword">for</span> d <span class="keyword">in</span> <span class="built_in">range</span>(dims))</span><br><span class="line">    axis_ranges = (</span><br><span class="line">        <span class="built_in">np</span>.arange(subsample_shape[d], dtype=<span class="built_in">np</span>.float32)*step_width[d]</span><br><span class="line">        <span class="keyword">for</span> d <span class="keyword">in</span> <span class="built_in">range</span>(dims)</span><br><span class="line">    )</span><br><span class="line">    <span class="built_in">return</span> <span class="built_in">np</span>.<span class="built_in">array</span>(<span class="built_in">np</span>.meshgrid(*axis_ranges, indexing='ij'), dtype=<span class="built_in">np</span>.float32)</span><br><span class="line"></span><br><span class="line">def upscale_transformation(transformation, output_shape, interpolate_order=<span class="number">1</span>):</span><br><span class="line">    input_shape = transformation.shape[<span class="number">1</span>:]</span><br><span class="line">    dims = len(output_shape)</span><br><span class="line">    <span class="built_in">scale</span> = tuple(<span class="built_in">float</span>(s)/c <span class="keyword">for</span> s, c <span class="keyword">in</span> zip(output_shape, input_shape))</span><br><span class="line">    scaled = <span class="built_in">np</span>.zeros((dims,)+output_shape, dtype=<span class="built_in">np</span>.float32)</span><br><span class="line">    <span class="keyword">for</span> d <span class="keyword">in</span> <span class="built_in">range</span>(dims):</span><br><span class="line">        zoom(transformation[d], zoom=<span class="built_in">scale</span>,</span><br><span class="line">             output=scaled[d], order=interpolate_order)</span><br><span class="line">    <span class="built_in">return</span> scaled</span><br><span class="line"></span><br><span class="line">def create_elastic_transformation(shape, control_point_spacing=<span class="number">100</span>, jitter_sigma=<span class="number">10.0</span>, <span class="built_in">subsample</span>=<span class="number">1</span>):</span><br><span class="line">    dims = len(shape)</span><br><span class="line">    subsample_shape = tuple(<span class="built_in">max</span>(<span class="number">1</span>, int(s/<span class="built_in">subsample</span>)) <span class="keyword">for</span> s <span class="keyword">in</span> shape)</span><br><span class="line">    try:</span><br><span class="line">        spacing = tuple((d <span class="keyword">for</span> d <span class="keyword">in</span> control_point_spacing))</span><br><span class="line">    except:</span><br><span class="line">        spacing = (control_point_spacing,)*dims</span><br><span class="line">    try:</span><br><span class="line">        sigmas = [s <span class="keyword">for</span> s <span class="keyword">in</span> jitter_sigma]</span><br><span class="line">    except:</span><br><span class="line">        sigmas = [jitter_sigma]*dims</span><br><span class="line">    control_points = tuple(</span><br><span class="line">        <span class="built_in">max</span>(<span class="number">1</span>, int(<span class="built_in">round</span>(<span class="built_in">float</span>(shape[d])/spacing[d])))</span><br><span class="line">        <span class="keyword">for</span> d <span class="keyword">in</span> <span class="built_in">range</span>(len(shape))</span><br><span class="line">    )</span><br><span class="line">    control_point_offsets = <span class="built_in">np</span>.zeros(</span><br><span class="line">        (dims,) + control_points, dtype=<span class="built_in">np</span>.float32)</span><br><span class="line">    <span class="keyword">for</span> d <span class="keyword">in</span> <span class="built_in">range</span>(dims):</span><br><span class="line">        <span class="keyword">if</span> sigmas[d] &gt; <span class="number">0</span>:</span><br><span class="line">            control_point_offsets[d] = <span class="built_in">np</span>.<span class="built_in">random</span>.normal(</span><br><span class="line">                <span class="built_in">scale</span>=sigmas[d], size=control_points)</span><br><span class="line">    <span class="built_in">return</span> upscale_transformation(control_point_offsets, subsample_shape, interpolate_order=<span class="number">3</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def rotate(point, angle):</span><br><span class="line">    res = <span class="built_in">np</span>.<span class="built_in">array</span>(point)</span><br><span class="line">    res[<span class="number">0</span>] = math.<span class="built_in">sin</span>(angle)*point[<span class="number">1</span>] + math.<span class="built_in">cos</span>(angle)*point[<span class="number">0</span>]</span><br><span class="line">    res[<span class="number">1</span>] = -math.<span class="built_in">sin</span>(angle)*point[<span class="number">0</span>] + math.<span class="built_in">cos</span>(angle)*point[<span class="number">1</span>]</span><br><span class="line">    <span class="built_in">return</span> res</span><br><span class="line"></span><br><span class="line">def create_rotation_transformation(shape, angle, <span class="built_in">subsample</span>=<span class="number">1</span>):</span><br><span class="line">    dims = len(shape)</span><br><span class="line">    subsample_shape = tuple(<span class="built_in">max</span>(<span class="number">1</span>, int(s/<span class="built_in">subsample</span>)) <span class="keyword">for</span> s <span class="keyword">in</span> shape)</span><br><span class="line">    control_points = (<span class="number">2</span>,)*dims</span><br><span class="line">    control_point_scaling_factor = tuple(<span class="built_in">float</span>(s-<span class="number">1</span>) <span class="keyword">for</span> s <span class="keyword">in</span> shape)</span><br><span class="line">    <span class="built_in">center</span> = <span class="built_in">np</span>.<span class="built_in">array</span>([<span class="number">0.5</span>*(d-<span class="number">1</span>) <span class="keyword">for</span> d <span class="keyword">in</span> shape])</span><br><span class="line">    control_point_offsets = <span class="built_in">np</span>.zeros(</span><br><span class="line">        (dims,) + control_points, dtype=<span class="built_in">np</span>.float32)</span><br><span class="line">    <span class="keyword">for</span> control_point <span class="keyword">in</span> <span class="built_in">np</span>.ndindex(control_points):</span><br><span class="line"></span><br><span class="line">        point = <span class="built_in">np</span>.<span class="built_in">array</span>(control_point)*control_point_scaling_factor</span><br><span class="line">        center_offset = <span class="built_in">np</span>.<span class="built_in">array</span>(</span><br><span class="line">            [p-c <span class="keyword">for</span> c, p <span class="keyword">in</span> zip(<span class="built_in">center</span>, point)], dtype=<span class="built_in">np</span>.float32)</span><br><span class="line">        rotated_offset = <span class="built_in">np</span>.<span class="built_in">array</span>(center_offset)</span><br><span class="line">        rotated_offset[-<span class="number">2</span>:] = rotate(center_offset[-<span class="number">2</span>:], angle)</span><br><span class="line">        displacement = rotated_offset - center_offset</span><br><span class="line">        control_point_offsets[(slice(None),) + control_point] += displacement</span><br><span class="line"></span><br><span class="line">    <span class="built_in">return</span> upscale_transformation(control_point_offsets, subsample_shape)</span><br><span class="line"></span><br><span class="line">def random_offset(max_misalign):</span><br><span class="line">    <span class="built_in">return</span> Coordinate((<span class="number">0</span>,) + tuple(max_misalign - <span class="built_in">random</span>.randint(<span class="number">0</span>, <span class="number">2</span>*int(max_misalign)) <span class="keyword">for</span> d <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">2</span>)))</span><br><span class="line"></span><br><span class="line">def misalign(transformation, prob_slip, prob_shift, max_misalign):</span><br><span class="line">    num_sections = transformation[<span class="number">0</span>].shape[<span class="number">0</span>]</span><br><span class="line">    <span class="built_in">print</span> (num_sections)</span><br><span class="line">    shifts = [Coordinate((<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>))]*num_sections</span><br><span class="line">    <span class="keyword">for</span> z <span class="keyword">in</span> <span class="built_in">range</span>(num_sections):</span><br><span class="line">        r = <span class="built_in">random</span>.<span class="built_in">random</span>()</span><br><span class="line">        <span class="keyword">if</span> r &lt;= prob_slip:</span><br><span class="line">            shifts[z] = random_offset(max_misalign)</span><br><span class="line">        elif r &lt;= prob_slip + prob_shift:</span><br><span class="line">            offset = random_offset(max_misalign)</span><br><span class="line">            <span class="keyword">for</span> zp <span class="keyword">in</span> <span class="built_in">range</span>(z, num_sections):</span><br><span class="line">                shifts[zp] += offset</span><br><span class="line">                #<span class="built_in">print</span> ('shiftzp '+str(shifts[zp]))</span><br><span class="line">    <span class="keyword">for</span> z <span class="keyword">in</span> <span class="built_in">range</span>(num_sections):</span><br><span class="line">        transformation[<span class="number">1</span>][z, :, :] += shifts[z][<span class="number">1</span>]</span><br><span class="line">        transformation[<span class="number">2</span>][z, :, :] += shifts[z][<span class="number">2</span>]</span><br><span class="line">    <span class="built_in">return</span> transformation</span><br><span class="line">class ElasticAugment():</span><br><span class="line">    def __init__(</span><br><span class="line">            self,</span><br><span class="line">            control_point_spacing,</span><br><span class="line">            jitter_sigma,</span><br><span class="line">            rotation_interval,</span><br><span class="line">            prob_slip=<span class="number">0</span>,</span><br><span class="line">            prob_shift=<span class="number">0</span>,</span><br><span class="line">            max_misalign=<span class="number">0</span>,</span><br><span class="line">            <span class="built_in">subsample</span>=<span class="number">1</span>):</span><br><span class="line">        self.control_point_spacing = control_point_spacing</span><br><span class="line">        self.jitter_sigma = jitter_sigma</span><br><span class="line">        self.rotation_start = rotation_interval[<span class="number">0</span>]</span><br><span class="line">        self.rotation_max_amount = rotation_interval[<span class="number">1</span>] - rotation_interval[<span class="number">0</span>]</span><br><span class="line">        self.prob_slip = prob_slip</span><br><span class="line">        self.prob_shift = prob_shift</span><br><span class="line">        self.max_misalign = max_misalign</span><br><span class="line">        self.<span class="built_in">subsample</span> = <span class="built_in">subsample</span></span><br><span class="line">    def create_transformation(self, target_shape):</span><br><span class="line">        transformation = create_identity_transformation(</span><br><span class="line">            target_shape,</span><br><span class="line">            <span class="built_in">subsample</span>=self.<span class="built_in">subsample</span>)</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">sum</span>(self.jitter_sigma) &gt; <span class="number">0</span>:</span><br><span class="line">            transformation += create_elastic_transformation(</span><br><span class="line">                target_shape,</span><br><span class="line">                self.control_point_spacing,</span><br><span class="line">                self.jitter_sigma,</span><br><span class="line">                <span class="built_in">subsample</span>=self.<span class="built_in">subsample</span>)</span><br><span class="line">        rotation = <span class="built_in">random</span>.<span class="built_in">random</span>()*self.rotation_max_amount + self.rotation_start</span><br><span class="line">        <span class="keyword">if</span> rotation != <span class="number">0</span>:</span><br><span class="line">            transformation += create_rotation_transformation(</span><br><span class="line">                target_shape,</span><br><span class="line">                rotation,</span><br><span class="line">                <span class="built_in">subsample</span>=self.<span class="built_in">subsample</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> self.<span class="built_in">subsample</span> &gt; <span class="number">1</span>:</span><br><span class="line">            transformation = upscale_transformation(</span><br><span class="line">                transformation,</span><br><span class="line">                target_shape)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> self.prob_slip + self.prob_shift &gt; <span class="number">0</span>:</span><br><span class="line">            misalign(transformation, self.prob_slip,</span><br><span class="line">                     self.prob_shift, self.max_misalign)</span><br><span class="line">        <span class="built_in">return</span> transformation</span><br><span class="line">def apply_transformation(<span class="built_in">image</span>, transformation, interpolate=True, outside_value=<span class="number">0</span>, output=None):</span><br><span class="line"></span><br><span class="line">    # <span class="built_in">print</span>(<span class="string">"Applying transformation..."</span>)</span><br><span class="line">    order = <span class="number">1</span> <span class="keyword">if</span> interpolate == True <span class="keyword">else</span> <span class="number">0</span></span><br><span class="line">    output = <span class="built_in">image</span>.dtype <span class="keyword">if</span> output <span class="built_in">is</span> None <span class="keyword">else</span> output</span><br><span class="line">    <span class="built_in">return</span> map_coordinates(<span class="built_in">image</span>, transformation, output=output, order=order, mode='<span class="built_in">constant</span>', cval=outside_value)</span><br><span class="line"></span><br><span class="line">def apply_elastic_transform(img, mask):</span><br><span class="line">    assert img.shape[<span class="number">1</span>] == img.shape[<span class="number">2</span>]</span><br><span class="line">    img *= img.shape[<span class="number">1</span>]</span><br><span class="line">    <span class="built_in">transform</span> = ElasticAugment([<span class="number">4</span>, <span class="number">40</span>, <span class="number">40</span>], [<span class="number">0</span>, <span class="number">2</span>, <span class="number">2</span>], [<span class="number">0</span>, <span class="number">0</span>], prob_slip=<span class="number">0.05</span>,</span><br><span class="line">                               prob_shift=<span class="number">0.05</span>, max_misalign=<span class="number">25</span>).create_transformation([img.shape[<span class="number">0</span>], img.shape[<span class="number">1</span>], img.shape[<span class="number">2</span>]])</span><br><span class="line">    img_transform = apply_transformation(img,</span><br><span class="line">                                         <span class="built_in">transform</span>,</span><br><span class="line">                                         interpolate=False,</span><br><span class="line">                                         outside_value=img.dtype.type(-<span class="number">1</span>),</span><br><span class="line">                                         output=<span class="built_in">np</span>.zeros(img.shape, dtype=<span class="built_in">np</span>.float32))</span><br><span class="line">    seg_transform = apply_transformation(mask,</span><br><span class="line">                                         <span class="built_in">transform</span>,</span><br><span class="line">                                         interpolate=False,</span><br><span class="line">                                         outside_value=mask.dtype.type(-<span class="number">1</span>),</span><br><span class="line">                                         output=<span class="built_in">np</span>.zeros(mask.shape, dtype=<span class="built_in">np</span>.float32))(<span class="built_in">np</span>.<span class="built_in">unique</span>(seg_transform.ravel()),<span class="built_in">np</span>.<span class="built_in">unique</span>(<span class="built_in">transform</span>.ravel()))</span><br><span class="line">    seg_transform[seg_transform &gt; <span class="number">0</span>] = <span class="number">1</span></span><br><span class="line">    seg_transform[seg_transform != <span class="number">1</span>] = <span class="number">0</span></span><br><span class="line">    <span class="built_in">return</span> img_transform/img_transform.shape[<span class="number">1</span>], seg_transform</span><br></pre></td></tr></table></figure><h4 id="defect-augmentation"><a href="#defect-augmentation" class="headerlink" title="defect augmentation"></a>defect augmentation</h4><p>Usage: transformedimgs, transformedmasks = apply_deform(testdatraw/256.,testdatseg,0,20,0.08)</p><p><img src="http://i1.fuimg.com/640680/3dfea030bf017168.png" alt="Markdown"></p><p>I use gunpowder’s defect, the block region used to be zero, but I change it to zero for better batch normalization results. I use random seed to reproduce images and masks. The block size and missing section’s ratio can change. </p><figure class="highlight maxima"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br></pre></td><td class="code"><pre><span class="line">def prepare_deform_slice(slice_shape, deformation_strength, <span class="built_in">iterations</span>, randomseed):</span><br><span class="line">    <span class="built_in">np</span>.<span class="built_in">random</span>.seed(randomseed)</span><br><span class="line">    grow_by = <span class="number">2</span> * deformation_strength</span><br><span class="line">    shape = (slice_shape[<span class="number">0</span>] + grow_by, slice_shape[<span class="number">1</span>] + grow_by)</span><br><span class="line">    fixed_x = <span class="built_in">np</span>.<span class="built_in">random</span>.<span class="built_in">random</span>() &lt; .<span class="number">5</span></span><br><span class="line">    <span class="keyword">if</span> fixed_x:</span><br><span class="line">        x0, y0 = <span class="number">0</span>, <span class="built_in">np</span>.<span class="built_in">random</span>.randint(<span class="number">1</span>, shape[<span class="number">1</span>] - <span class="number">2</span>)</span><br><span class="line">        x1, y1 = shape[<span class="number">0</span>] - <span class="number">1</span>, <span class="built_in">np</span>.<span class="built_in">random</span>.randint(<span class="number">1</span>, shape[<span class="number">1</span>] - <span class="number">2</span>)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        x0, y0 = <span class="built_in">np</span>.<span class="built_in">random</span>.randint(<span class="number">1</span>, shape[<span class="number">0</span>] - <span class="number">2</span>), <span class="number">0</span></span><br><span class="line">        x1, y1 = <span class="built_in">np</span>.<span class="built_in">random</span>.randint(<span class="number">1</span>, shape[<span class="number">0</span>] - <span class="number">2</span>), shape[<span class="number">1</span>] - <span class="number">1</span></span><br><span class="line">    line_mask = <span class="built_in">np</span>.zeros(shape, dtype='bool')</span><br><span class="line">    rr, cc = line(x0, y0, x1, y1)</span><br><span class="line">    line_mask[rr, cc] = <span class="number">1</span></span><br><span class="line">    line_vector = <span class="built_in">np</span>.<span class="built_in">array</span>([x1 - x0, y1 - y0], dtype='float32')</span><br><span class="line">    line_vector /= <span class="built_in">np</span>.linalg.norm(line_vector)</span><br><span class="line">    normal_vector = <span class="built_in">np</span>.zeros_like(line_vector)</span><br><span class="line">    normal_vector[<span class="number">0</span>] = - line_vector[<span class="number">1</span>]</span><br><span class="line">    normal_vector[<span class="number">1</span>] = line_vector[<span class="number">0</span>]</span><br><span class="line">    x, y = <span class="built_in">np</span>.meshgrid(<span class="built_in">np</span>.arange(shape[<span class="number">1</span>]), <span class="built_in">np</span>.arange(shape[<span class="number">0</span>]))</span><br><span class="line">    flow_x, flow_y = <span class="built_in">np</span>.zeros(shape), <span class="built_in">np</span>.zeros(shape)</span><br><span class="line">    <span class="built_in">components</span>, n_components = <span class="built_in">label</span>(<span class="built_in">np</span>.logical_not(line_mask).<span class="built_in">view</span>('uint8'))</span><br><span class="line">    assert n_components == <span class="number">2</span>, <span class="string">"%i"</span> <span class="symbol">%</span> n_components</span><br><span class="line">    neg_val = <span class="built_in">components</span>[<span class="number">0</span>, <span class="number">0</span>] <span class="keyword">if</span> fixed_x <span class="keyword">else</span> <span class="built_in">components</span>[-<span class="number">1</span>, -<span class="number">1</span>]</span><br><span class="line">    pos_val = <span class="built_in">components</span>[-<span class="number">1</span>, -<span class="number">1</span>] <span class="keyword">if</span> fixed_x <span class="keyword">else</span> <span class="built_in">components</span>[<span class="number">0</span>, <span class="number">0</span>]</span><br><span class="line">    flow_x[<span class="built_in">components</span> == pos_val] = deformation_strength * normal_vector[<span class="number">1</span>]</span><br><span class="line">    flow_y[<span class="built_in">components</span> == pos_val] = deformation_strength * normal_vector[<span class="number">0</span>]</span><br><span class="line">    flow_x[<span class="built_in">components</span> == neg_val] = - deformation_strength * normal_vector[<span class="number">1</span>]</span><br><span class="line">    flow_y[<span class="built_in">components</span> == neg_val] = - deformation_strength * normal_vector[<span class="number">0</span>]</span><br><span class="line">    flow_x, flow_y = (x + flow_x).reshape(-<span class="number">1</span>, <span class="number">1</span>), (y + flow_y).reshape(-<span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">    line_mask = binary_dilation(line_mask, <span class="built_in">iterations</span>=<span class="built_in">iterations</span>)  # default=<span class="number">10</span></span><br><span class="line">    <span class="built_in">return</span> flow_x, flow_y, line_mask</span><br><span class="line"></span><br><span class="line">def deform_2d(image2d, deformation_strength, <span class="built_in">iterations</span>, randomseed):</span><br><span class="line">    flow_x, flow_y, line_mask = prepare_deform_slice(</span><br><span class="line">        image2d.shape, deformation_strength, <span class="built_in">iterations</span>, randomseed)</span><br><span class="line">    section = image2d.squeeze()</span><br><span class="line">    <span class="built_in">mean</span> = section.<span class="built_in">mean</span>()</span><br><span class="line">    shape = section.shape</span><br><span class="line">    #interpolation=<span class="number">3</span></span><br><span class="line">    section = map_coordinates(section, (flow_y, flow_x), mode='<span class="built_in">constant</span>', order=<span class="number">3</span>).reshape(int(flow_x.shape[<span class="number">0</span>]**<span class="number">0.5</span>), int(flow_x.shape[<span class="number">0</span>]**<span class="number">0.5</span>))</span><br><span class="line">    section = <span class="built_in">np</span>.clip(section, <span class="number">0</span>., <span class="number">1</span>.)</span><br><span class="line">    section[line_mask] = <span class="built_in">mean</span></span><br><span class="line">    <span class="built_in">return</span> section</span><br><span class="line"></span><br><span class="line">def apply_deform(imgs, masks, deformation_strength=<span class="number">0</span>, <span class="built_in">iterations</span>=<span class="number">20</span>, deform_ratio=<span class="number">0.08</span>):</span><br><span class="line">    transformedimgs, transformedmasks = &#123;&#125;, &#123;&#125;</span><br><span class="line">    sectionsnum = imgs.shape[<span class="number">0</span>]</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(sectionsnum):</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">random</span>.<span class="built_in">random</span>() &lt;= deform_ratio:</span><br><span class="line">            randomseed = <span class="built_in">np</span>.<span class="built_in">random</span>.randint(<span class="number">1000000</span>)</span><br><span class="line">            transformedimgs[i] = deform_2d(</span><br><span class="line">                imgs[i], deformation_strength, <span class="built_in">iterations</span>, randomseed)</span><br><span class="line">            transformedmasks[i] = deform_2d(</span><br><span class="line">                masks[i], deformation_strength, <span class="built_in">iterations</span>, randomseed)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            transformedimgs[i] = imgs[i]</span><br><span class="line">            transformedmasks[i] = masks[i]</span><br><span class="line">    <span class="built_in">return</span> transformedimgs, transformedmasks</span><br></pre></td></tr></table></figure><h3 id="test"><a href="#test" class="headerlink" title="test"></a>test</h3><p>For each sample in test, I will produce 16 transformed images and reverse the predicted masks later.</p><p>Usage: simpleaug_test_produce()</p><p>Usage: simpleaug_test_reverse()</p><p>In test, it is common to only use 16 kinds <strong>simple augmentation</strong></p><p>For X Y Z axis mirro and xy transpose, in all $2^4 = 16$ images produced.</p><figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">simpleaug_test_produce</span>():</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(<span class="keyword">self</span>, model_io_size=[<span class="number">24</span>, <span class="number">256</span>, <span class="number">256</span>])</span></span><span class="symbol">:</span></span><br><span class="line">        <span class="keyword">self</span>.model_io_size = model_io_size</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__call__</span><span class="params">(<span class="keyword">self</span>, imgs)</span></span><span class="symbol">:</span></span><br><span class="line">        <span class="keyword">return</span> produce_simple_16_sample(imgs, <span class="keyword">self</span>.model_io_size)</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">simpleaug_train_produce</span>():</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(<span class="keyword">self</span>, model_io_size=[<span class="number">24</span>, <span class="number">256</span>, <span class="number">256</span>])</span></span><span class="symbol">:</span></span><br><span class="line">        <span class="keyword">self</span>.rule = np.random.randint(<span class="number">2</span>, size=<span class="number">4</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__call__</span><span class="params">(<span class="keyword">self</span>, imgs, mask)</span></span><span class="symbol">:</span></span><br><span class="line">        <span class="comment">#print (self.rule)</span></span><br><span class="line">        imgs_aug = produce_simple_train_sample(imgs, <span class="keyword">self</span>.rule)</span><br><span class="line">        mask_aug = produce_simple_train_sample(mask, <span class="keyword">self</span>.rule)</span><br><span class="line">        <span class="keyword">return</span> imgs_aug, mask_aug</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">simpleaug_test_reverse</span>():</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(<span class="keyword">self</span>, model_io_size=[<span class="number">24</span>, <span class="number">256</span>, <span class="number">256</span>])</span></span><span class="symbol">:</span></span><br><span class="line">        <span class="keyword">self</span>.model_io_size = model_io_size</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">augmentsimplereverse</span><span class="params">(<span class="keyword">self</span>,data, rule)</span></span><span class="symbol">:</span></span><br><span class="line">        assert np.size(rule) == <span class="number">4</span> <span class="keyword">and</span> data.ndim == <span class="number">3</span></span><br><span class="line">        <span class="comment"># z reflection.</span></span><br><span class="line">        <span class="keyword">if</span> rule[<span class="number">3</span>]<span class="symbol">:</span></span><br><span class="line">            data = data.transpose(<span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>)</span><br><span class="line">        <span class="keyword">if</span> rule[<span class="number">2</span>]<span class="symbol">:</span></span><br><span class="line">            data = data[<span class="symbol">:</span>, <span class="symbol">:</span><span class="symbol">:-</span><span class="number">1</span>, <span class="symbol">:</span>]</span><br><span class="line">        <span class="keyword">if</span> rule[<span class="number">1</span>]<span class="symbol">:</span></span><br><span class="line">            data = data[<span class="symbol">:</span>, <span class="symbol">:</span>, <span class="symbol">:</span><span class="symbol">:-</span><span class="number">1</span>]</span><br><span class="line">        <span class="keyword">if</span> rule[<span class="number">0</span>]<span class="symbol">:</span></span><br><span class="line">            data = data[<span class="symbol">:</span><span class="symbol">:-</span><span class="number">1</span>, <span class="symbol">:</span>, <span class="symbol">:</span>]</span><br><span class="line">        <span class="keyword">return</span> data</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">reverse_and_mean</span><span class="params">(<span class="keyword">self</span>,imgs, imgshape)</span></span><span class="symbol">:</span></span><br><span class="line">        <span class="string">''</span><span class="string">'</span></span><br><span class="line"><span class="string">        imgs: 16*24*256*256 -&gt;24*256*256</span></span><br><span class="line"><span class="string">        '</span><span class="string">''</span></span><br><span class="line">        assert imgs.ndim == <span class="number">4</span></span><br><span class="line">        reversedsimplearr = np.ndarray(</span><br><span class="line">            [<span class="number">16</span>, imgshape[<span class="number">0</span>], imgshape[<span class="number">1</span>], imgshape[<span class="number">2</span>]])</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">16</span>)<span class="symbol">:</span></span><br><span class="line">            reversedsimplearr[i] = <span class="keyword">self</span>.augmentsimplereverse(imgs[i], binaryarr[i])</span><br><span class="line">        <span class="keyword">return</span> np.mean(reversedsimplearr, axis=<span class="number">0</span>)</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__call__</span><span class="params">(<span class="keyword">self</span>, imgs)</span></span><span class="symbol">:</span></span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">self</span>.reverse_and_mean(imgs, <span class="keyword">self</span>.model_io_size)</span><br></pre></td></tr></table></figure><h3 id="further-useful-augmentation"><a href="#further-useful-augmentation" class="headerlink" title="further useful augmentation"></a>further useful augmentation</h3><p>I have found many other augmentations in many sources. Since our main aim now is to get a better result in a short time. I will see if the augmentation strategy is enough. If not, I will implement them later. If the augmentation is good enough, we can do more on task 3 later. Since the model is really hard to train and see the results(may take a week for the feedback), time is really precious, and we can’t test one method at one time.</p><p>I have tried some here <a href="https://github.com/james20141606/Summer_Intern/blob/master/synapse_prediction/jupyter/further_aug_imgaug.ipynb" target="_blank" rel="noopener">Summer_Intern/further_aug_imgaug.ipynb at master · james20141606/Summer_Intern · GitHub</a>, but I will decide what to do first next week.</p><ul><li>2018 annual datascience bowl top1: segment cells<br><a href="https://www.leiphone.com/news/201804/qfus8zALhZLoA8Ai.html" target="_blank" rel="noopener">https://www.leiphone.com/news/201804/qfus8zALhZLoA8Ai.html</a><br>There are many data augmentation methods:<br><a href="https://github.com/selimsef/dsb2018_topcoders/blob/7a87c07e1fb8e090186a3914a1443469f5107962/albu/src/augmentations/transforms.py" target="_blank" rel="noopener">https://github.com/selimsef/dsb2018_topcoders/blob/7a87c07e1fb8e090186a3914a1443469f5107962/albu/src/augmentations/transforms.py</a><br>It seems clear and they used an augmentation API imgaug<br><a href="http://imgaug.readthedocs.io/en/latest/" target="_blank" rel="noopener">imgaug</a><br>Apart from gunpowder’s method, there are a lot to use!</li><li>random zoom, rotate, flip</li><li>contrast and brightness</li><li>heavy geometric transform:  Elastic Transform, Perspective Transform, Piecewise Affine transforms, Pincushion Distortion</li><li>contrast limited adaptive histogram equalization (CLAHE) ，Sharpen，Emboss</li><li>Gaussian noise</li><li>Blur、Median Blur、Motion Blur</li><li>HSV</li><li>rearrange channel</li><li>repeat of cell nuclear</li></ul><h2 id="future-work"><a href="#future-work" class="headerlink" title="future work"></a>future work</h2><h3 id="compare-with-other-work-and-thoughs"><a href="#compare-with-other-work-and-thoughs" class="headerlink" title="compare with other work and thoughs"></a>compare with other work and thoughs</h3><ul><li>For randomly chosen sample, we use pixel ratio to determine if one sample is used to train, Funke use probability,  we can think which is better.</li><li>Funke use regression whereas  we use binary classification, it is simple to change to regression, but we should compare our loss function and strategy at first. For example, Funke’s STED loss may not be good.</li><li>Funke use auxiliary loss for better prune, but we have visualize the result and find the proposed synapse matchh the membrane well, so it may not be necessary. </li></ul><h3 id="future-plan"><a href="#future-plan" class="headerlink" title="future plan"></a>future plan</h3><p>As mentioned above, I believe there are more to implement. The state-of-art model and pipeline looks really good, so I think I should use more strategy to improve our results.</p><ul><li>read and consider V-net’s structure. V-Net: Fully Convolutional Neural Networks for Volumetric Medical Image Segmentation<br><a href="http://mattmacy.io/vnet.pytorch/" target="_blank" rel="noopener">vnet.pytorch</a><br><a href="https://github.com/mattmacy/vnet.pytorch" target="_blank" rel="noopener">GitHub - mattmacy/vnet.pytorch: A PyTorch implementation for V-Net: Fully Convolutional Neural Networks for Volumetric Medical Image Segmentation</a><br><a href="https://github.com/mattmacy/torchbiomed" target="_blank" rel="noopener">GitHub - mattmacy/torchbiomed: Datasets, Transforms and Utilities specific to Biomedical Imaging</a></li><li>if submitted result has a big difference with Funke’s, I may try to reproduce their work.</li><li>read task 3 complete codes apart from  <a href="https://github.com/paragt/EMSynConn" target="_blank" rel="noopener">GitHub - paragt/EMSynConn: One algorithm to detect synaptic location AND connectivity, both dyadic and polyadic, in Electron Microscopy volume.</a></li><li>how to fine tune, apart from learning rate adjustment, we should consider some prune work. For example, the paper <strong>Stacked U-Nets with Multi-Output for Road Extraction</strong>:<blockquote><p>postprocessing: Various post-processing techniques for road extraction have been proposed in the literature, e.g., centerline extraction using structured SVM or Markov random ﬁeld, handling noisy data using a special CNN, recovering lines by sampling junction-points, and bridging road gaps by heuristic search. Here we develope a novel post processing technique by linking broken roads through shortest path search with decreasing conﬁdence thresholds. More speciﬁcally, we ﬁrst convert the raster road prediction image to vector format so we can bridge gaps and trim spurious roads, then we render a raster image from road vectors and merge it with the original prediction because the challenge needs raster images for IoU calculation.</p></blockquote></li><li>examine results carefully to understand the difference between FP and TP, gain biological intuition from it for better model design. For example the  density of membrane and vesicle</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;It is part of my computational task during my summer intern in Lichtman lab&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;It is part of the big synapse project. Also the challenge 2 of &lt;a href=&quot;https://cremi.org&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;CREMI&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The codes related are here: &lt;a href=&quot;https://github.com/james20141606/Summer_Intern/tree/master/synapse_prediction&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Summer_Intern/synaptic_partner at master · james20141606/Summer_Intern · GitHub&lt;/a&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="summer intern" scheme="https://www.cmwonderland.com/categories/summer-intern/"/>
    
    
      <category term="deep learning" scheme="https://www.cmwonderland.com/tags/deep-learning/"/>
    
      <category term="project" scheme="https://www.cmwonderland.com/tags/project/"/>
    
      <category term="summer intern" scheme="https://www.cmwonderland.com/tags/summer-intern/"/>
    
      <category term="neural science" scheme="https://www.cmwonderland.com/tags/neural-science/"/>
    
      <category term="Jeff Lichtman" scheme="https://www.cmwonderland.com/tags/Jeff-Lichtman/"/>
    
  </entry>
  
  <entry>
    <title>NMJ Project</title>
    <link href="https://www.cmwonderland.com/2018/07/14/summerintern_NMJ/"/>
    <id>https://www.cmwonderland.com/2018/07/14/summerintern_NMJ/</id>
    <published>2018-07-15T00:35:19.000Z</published>
    <updated>2018-07-30T01:44:31.951Z</updated>
    
    <content type="html"><![CDATA[<p>It is my main task during my summer intern in Lichtman lab.<br>The codes related are here: <a href="https://github.com/james20141606/Summer_Intern/tree/master/NMJ" target="_blank" rel="noopener">NMJ codes</a></p><a id="more"></a><hr><h1 id="First-two-weeks"><a href="#First-two-weeks" class="headerlink" title="First two weeks"></a>First two weeks</h1><p>Since the new data is still to be processed, I spent several days doing dense segmentation work both for study and future training. </p><p>I have done 25 sections dense segmentation in W12-W14 for 4 days(7.5-7.8), it includes dense segmentation of Axons, Schwann cell, and Schwann cell nucleus. </p><p>I have written codes to use python to visualize animation of the 25 segments <img src="http://i2.tiimg.com/640680/674ef868297b66d0.gif" alt="Markdown">. Since the computational  work use more python codes know, I also shared my animating code with others.<br><a href="https://github.com/james20141606/Summer_Intern/blob/master/NMJ/jupyter/plot_segment.ipynb" target="_blank" rel="noopener">plot segment script</a></p><p>Core codes to plot animation in python</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line">defdef  transform_rgbtransfor (img):</span><br><span class="line">    num = np.unique(img.reshape(<span class="number">-1</span>,<span class="number">3</span>),axis=<span class="number">0</span>).shape[<span class="number">0</span>]</span><br><span class="line">    <span class="comment">#print (num)</span></span><br><span class="line">    <span class="comment">#rgbarr = np.ndarray([num*3])</span></span><br><span class="line">    <span class="comment">#for i in range(num*3):</span></span><br><span class="line">      <span class="comment">#  rgbarr[i] = np.random.uniform(0,1)</span></span><br><span class="line">    <span class="comment">#rgbarr = rgbarr.reshape(-1,3)</span></span><br><span class="line">    image = np.zeros([img.shape[<span class="number">0</span>]*img.shape[<span class="number">1</span>],<span class="number">3</span>])</span><br><span class="line">    sumimg = np.sum(img.reshape(<span class="number">-1</span>,<span class="number">3</span>),axis=<span class="number">1</span>)</span><br><span class="line">    uniqueind = np.unique(img.reshape(<span class="number">-1</span>,<span class="number">3</span>),axis=<span class="number">0</span>)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> np.arange(<span class="number">0</span>,num<span class="number">-1</span>):</span><br><span class="line">        image[sumimg==<span class="number">3</span>*(uniqueind[i][<span class="number">0</span>]+<span class="number">1</span>)] = colorsgallery[i]</span><br><span class="line">    <span class="comment">#print (sumimg.shape)</span></span><br><span class="line">    image[sumimg==<span class="number">0</span>] = [<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>]</span><br><span class="line">    <span class="keyword">return</span> image.reshape(img.shape[<span class="number">0</span>],img.shape[<span class="number">1</span>],<span class="number">3</span>)</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> matplotlib.animation <span class="keyword">as</span> animation</span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> rc</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">animations</span><span class="params">(opt=<span class="string">'show'</span>,type=<span class="string">'gif'</span>)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    opt: show/save</span></span><br><span class="line"><span class="string">    type:gif/mp4</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    imagelist = [transform_rgb(imagedata[i][<span class="number">100</span>:<span class="number">900</span>,<span class="number">100</span>:<span class="number">900</span>]) <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">26</span>)]</span><br><span class="line">    fig,ax=plt.subplots(<span class="number">1</span>,figsize=(<span class="number">16</span>,<span class="number">12</span>)) </span><br><span class="line">    im =ax.imshow(imagelist[<span class="number">0</span>])</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">updatefig</span><span class="params">(j)</span>:</span></span><br><span class="line">        im.set_array(imagelist[j])</span><br><span class="line">        <span class="keyword">return</span> [im]</span><br><span class="line">    anim = animation.FuncAnimation(fig, updatefig, frames=range(<span class="number">26</span>), </span><br><span class="line">                                  interval=<span class="number">100</span>, blit=<span class="keyword">True</span>)</span><br><span class="line">    <span class="keyword">if</span> opt==<span class="string">'show'</span>:</span><br><span class="line">        <span class="keyword">return</span> anim</span><br><span class="line">    <span class="keyword">elif</span> opt==<span class="string">'save'</span>:</span><br><span class="line">        <span class="keyword">if</span> type==<span class="string">'mp4'</span>:</span><br><span class="line">            Writer = animation.writers[<span class="string">'ffmpeg'</span>]</span><br><span class="line">            writer1 = Writer(fps=<span class="number">10</span>)</span><br><span class="line">            anim.save(<span class="string">'animation.'</span>+type, writer=writer1,dpi=<span class="number">1000</span>)</span><br><span class="line">        <span class="keyword">elif</span> type==<span class="string">'gif'</span>:</span><br><span class="line">            <span class="comment">#Writer = animation.writers['imagemagick']</span></span><br><span class="line">            anim.save(<span class="string">'animation.'</span>+type, writer=<span class="string">'imagemagick'</span>, fps=<span class="number">10</span>)</span><br></pre></td></tr></table></figure><p>I also read several articles Marco and Yaron recommened, including previous NMJ work, some segmentation and Connectome processing pipeline papers.</p><h2 id="Future-work"><a href="#Future-work" class="headerlink" title="Future work"></a>Future work</h2><p>We also discuss a lot about the future plan of the project. Since it is more challenging than other tasks, it seems we are a little slow in progress. We have worked with Marco to find a way to label the ROI and use a script to extract coordinates of the bounding box. We have labeled one mask, later we will test Adi’s align results and generate more.</p><ul><li>Manually create ROI region for bundles and NMJ for alignment</li><li>Do segmentation and \textbf{statistical analysis} work on some NMJs(concerning our limited staying time, it seems there isn’t enough time to wait for all NMJs’ alignment and segmentation results to analyze)</li></ul><hr><h1 id="Week-3"><a href="#Week-3" class="headerlink" title="Week 3"></a>Week 3</h1><h2 id="mask-and-seeding"><a href="#mask-and-seeding" class="headerlink" title="mask and seeding"></a>mask and seeding</h2><p>This week we have worked out a plan on alignment and seeding.</p><p>We use a mind map to record tree’s nodes to visualize our progress. The mask was sent to Adi for aligning. The alignment results seem very good.</p><p>I have seeded three masks Adi sent back, <strong>for about 900 sections mainly in bundle area.</strong></p><p>I also wrote python script <a href="https://github.com/james20141606/Summer_Intern/blob/master/NMJ/jupyter/plot_segment.ipynb" target="_blank" rel="noopener">Summer_Intern/plot_segment.ipynb at master · james20141606/Summer_Intern · GitHub</a> for further analysis. Since I am proficient in using python for visualization, statistical analysis and machine learning, I wrote some python scripts to read seeding result, visualize them and plot them in 3D and animation. It will be better to have more statistical analysis when I collect more seeding data.<br><img src="http://i1.fuimg.com/640680/45e727fc31a9b0dc.png" alt="Markdown"></p><h2 id="discussion-on-mask"><a href="#discussion-on-mask" class="headerlink" title="discussion on mask"></a>discussion on mask</h2><p>When we put masks on ROI, we found many branches even in bundle area, two branches from one stem may encounter and form a closed loop. We are worried if it will be a problem when we merge all masks together. After discussing with Adi and Daniel, we understand that the spatial structure’s change isn’t a big problem.</p><hr><p>Week 4</p><h2 id="My-thought-about-how-the-whole-project"><a href="#My-thought-about-how-the-whole-project" class="headerlink" title="My thought about how the whole project"></a>My thought about how the whole project</h2><p>This week I continue to seed on Mask3, and then I do a lot of exploration on how to do the NMJ project automatically.</p><p>I have understood how big and challenging this project is, it requires so many manually labeling work than we can’t finish all the masking and seeding and segmentation and reconstruction work in two months. We know that it took KK and Marco several months to finish part of the bundle parts. But the remaining parts are more complex to seed, segment and it contains maybe 200 masks with approximately 50,000 sections. It is hard to estimate how long it will take to finish the whole project</p><p>However, as I am getting more familiar with this project, I am trying to build a more automatically pipeline for seeding, predicting membrane and segmentation. If it works, the project may move faster when we are here and after we leave:)</p><h3 id="Seeding-on-Mask3"><a href="#Seeding-on-Mask3" class="headerlink" title="Seeding on Mask3"></a>Seeding on Mask3</h3><p>I felt that seeding on mask3 is much more complex than previous bundle seeding, the axon travels very fast and I should look up and down to look for one axon, it takes much more time to trace the branch than the main bundle.</p><h2 id="Automatic-pipeline"><a href="#Automatic-pipeline" class="headerlink" title="Automatic pipeline"></a>Automatic pipeline</h2><p><strong>We would like to build up a more automatic pipeline before we leave and test the whole pipeline on several masks to see if they can be merged and reconstructed.</strong></p><p>We would like to build up the whole pipeline, prepare all the codes and model for prediction and processing and write down the protocol.</p><p>The complete pipeline should contain:<br><strong>Generating Masks —&gt; Seeding —&gt; Predict Membrane —&gt; Expand Seeds —&gt; Merge different Masks</strong></p><p>Previously we do seeding manually and then predict membrane, but the remaining masks have so many sections, I would like to do the seeding work more automatically too.</p><h3 id="Predict-Membrane"><a href="#Predict-Membrane" class="headerlink" title="Predict Membrane"></a>Predict Membrane</h3><p>The automatically prediction parts must include membrane prediction, because it is “easier” to predict since the raw image already have the membrane.</p><h3 id="Automatically-seeding"><a href="#Automatically-seeding" class="headerlink" title="Automatically seeding"></a>Automatically seeding</h3><p>The traditional way is to manually put seeds on each axon, but we have approximately 50,000 sections if all masks are generated, it is so time-consuming to manually put seeds. I will <strong>generate seeds by distance transformation from membrane</strong></p><p>Then the seeds must be indexed to track each seed is from which axon, so we will manually put seeds  per 100 sections, then do <strong>Hungarian matching.</strong></p><h3 id="segmentation"><a href="#segmentation" class="headerlink" title="segmentation"></a>segmentation</h3><p>Expand the seed to generate segments</p><h3 id="Merge-masks"><a href="#Merge-masks" class="headerlink" title="Merge masks"></a>Merge masks</h3><p>We are thinking about linear interpolation to merge anchor sections for loop problems. We will discuss it more with Daniel and Yaron after the segmentation</p><h2 id="Algorithm"><a href="#Algorithm" class="headerlink" title="Algorithm"></a>Algorithm</h2><p>The related codes are here:<br><a href="https://github.com/james20141606/membrane_prediction" target="_blank" rel="noopener">GitHub - james20141606/membrane_prediction: Use 3D U-net to predict membrane predition</a></p><h3 id="Predict-Membrane-1"><a href="#Predict-Membrane-1" class="headerlink" title="Predict Membrane"></a>Predict Membrane</h3><p>I will use a 3D U-net model to use contours extracted from dense segmentation sections. Use 50 sections for training, then predict more, proofread predicted sections to generate more training samples. <strong>The iterative training and predicting method will make the model more precise.</strong></p><p>The model’s weight is adaptive to the pixels ratio, I can do fine tune on the model iteratively. So the model will be more precise and requires fewer proofreading. Last week I do many augmentation works, it is also useful to generate more training images since I only have 50 sections for training now.</p><p><strong>How to fine tune:</strong><br>If we want better result, we can manually label several sections on each mask and retrain the model on each mask.</p><p>For membrane prediction, since we do not consider affinity, we can also consider 2D U-net, it contains much less parameters and easier to train.</p><h3 id="Automatically-seeding-1"><a href="#Automatically-seeding-1" class="headerlink" title="Automatically seeding"></a>Automatically seeding</h3><ul><li><strong>Distance transformation</strong> to generate seeds from membrane</li><li><strong>Hungarian matching</strong> to label each seeds for different axons. Manually label one section’ s seed and do Hungarian matching for the next 100 sections.</li></ul><h3 id="Watershed"><a href="#Watershed" class="headerlink" title="Watershed"></a>Watershed</h3><p>Use watershed to expand seeds and generate segments</p><h3 id="Useful-resources"><a href="#Useful-resources" class="headerlink" title="Useful resources"></a>Useful resources</h3><p><a href="https://github.com/tdedecko/hungarian-algorithm/blob/master/hungarian.py#L6" target="_blank" rel="noopener">hungarian-algorithm/hungarian.py at master · tdedecko/hungarian-algorithm · GitHub</a><br><a href="https://github.com/hrldcpr/hungarian" target="_blank" rel="noopener">GitHub - hrldcpr/hungarian: Hungarian / Munkres’ algorithm for the linear assignment problem, in Python</a></p><p><a href="https://github.com/microns-ariadne/pipeline_engine/tree/cf100202997d3c848a21de441e15deb9f975042d/ariadne_microns_pipeline/tasks" target="_blank" rel="noopener">pipeline_engine/ariadne_microns_pipeline/tasks at cf100202997d3c848a21de441e15deb9f975042d · microns-ariadne/pipeline_engine · GitHub</a></p><p>Other possible algorithm:</p><ul><li>seeding<br>Use EM data to predict seeds, train seeding prediction network, using affinity because the axon travels fast. Sebastian’s group has some work. But I think it is imprecise compared to membrane prediction—distance transformation algorithm</li></ul><blockquote><p>Convolutional Networks Can Learn to Generate Affinity<br>Graphs for Image Segmentation<br>Maximin affinity learning of image segmentation</p></blockquote><p><a href="https://github.com/jiwoon-ahn/psa" target="_blank" rel="noopener">GitHub - jiwoon-ahn/psa: Learning Pixel-level Semantic Affinity with Image-level Supervision for Weakly Supervised Semantic Segmentation, CVPR 2018</a></p><h2 id="Work-on-membrane-prediction"><a href="#Work-on-membrane-prediction" class="headerlink" title="Work on membrane prediction"></a>Work on membrane prediction</h2><h3 id="Prepare-ground-truth-training-set"><a href="#Prepare-ground-truth-training-set" class="headerlink" title="Prepare ground truth training set"></a>Prepare ground truth training set</h3><p>I have started on membrane prediction pipeline after discussion with zudi, yaron and others. I would like to use previously label siyan and I have done in first two weeks to save time. We have done dense segmentation on 51 sections, I wrote a python script</p><p>Codes: <a href="https://github.com/james20141606/Summer_Intern/blob/master/NMJ/jupyter/extract_membrane_gt.ipynb" target="_blank" rel="noopener">Summer_Intern/extract_membrane_gt.ipynb at master · james20141606/Summer_Intern · GitHub</a></p><p> to extract the needed EM image and contours of the membrane in the following steps:</p><ul><li>export segmentation and EM ROI from VAST</li><li>read in python, converting id array to RGB array for visualization</li></ul><p><img src="http://i2.tiimg.com/640680/8b18f43f830a2eb7.png" alt="Markdown"></p><ul><li>find bounding box of each segmentation and EM image</li></ul><figure class="highlight maxima"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">def find_bounding(data):</span><br><span class="line">    xmin = <span class="built_in">np</span>.<span class="built_in">sort</span>(<span class="built_in">np</span>.where(data[:,:,<span class="number">0</span>]!=<span class="number">0</span>)[<span class="number">0</span>])[<span class="number">0</span>]</span><br><span class="line">    xmax = <span class="built_in">np</span>.<span class="built_in">sort</span>(<span class="built_in">np</span>.where(data[:,:,<span class="number">0</span>]!=<span class="number">0</span>)[<span class="number">0</span>])[-<span class="number">1</span>]</span><br><span class="line">    ymin = <span class="built_in">np</span>.<span class="built_in">sort</span>(<span class="built_in">np</span>.where(data[:,:,<span class="number">0</span>]!=<span class="number">0</span>)[<span class="number">1</span>])[<span class="number">0</span>]</span><br><span class="line">    ymax = <span class="built_in">np</span>.<span class="built_in">sort</span>(<span class="built_in">np</span>.where(data[:,:,<span class="number">0</span>]!=<span class="number">0</span>)[<span class="number">1</span>])[-<span class="number">1</span>]</span><br><span class="line">    <span class="built_in">return</span> xmin, xmax, ymin, ymax</span><br><span class="line"><span class="built_in">row</span> = <span class="number">26</span></span><br><span class="line">fig,ax=plt.subplots(<span class="built_in">row</span>,<span class="number">2</span>,figsize=(<span class="number">16</span>,<span class="number">6</span>*<span class="built_in">row</span>))</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">row</span>):</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">2</span>):</span><br><span class="line">        xmin, xmax, ymin, ymax = find_bounding(segdata[i*<span class="number">2</span>+j])</span><br><span class="line">        ax[i,j].imshow(transform_rgb(segdata[i*<span class="number">2</span>+j][(xmin-<span class="number">10</span>): (xmax+<span class="number">10</span>), (ymin-<span class="number">10</span>): (ymax+<span class="number">10</span>)]))</span><br></pre></td></tr></table></figure><ul><li>remove Schwann cell to concentrate on axons</li></ul><p>First it has some problems</p><p><img src="http://i2.tiimg.com/640680/2a5303ed7c054769.png" alt="Markdown"></p><p>Then I separately plot and find the black wrong region is 25 and 38</p><p>After correction:</p><p><img src="http://i2.tiimg.com/640680/96c7c89bc409d6ba.png" alt="Markdown"></p><ul><li>convert the segment array to binary mask</li></ul><p><img src="http://i2.tiimg.com/640680/070eef826c0612ae.png" alt="Markdown"></p><ul><li>Make sure each mask and EM data are in same bounding box</li></ul><p><img src="http://i2.tiimg.com/640680/2b1b68b735d93e29.png" alt="Markdown"></p><ul><li>Generate contours as training set label<br>opencv’s findcontour function is not suitable for same grayscale image, so I use erode and dilation<figure class="highlight makefile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">open = cv2.erode(grayimg, None, iterations = 4)</span><br><span class="line">open1 = cv2.dilate(open, None, iterations = 3)</span><br><span class="line">imshow(open1-open)</span><br></pre></td></tr></table></figure></li></ul><p><img src="http://i2.tiimg.com/640680/c9d99655bdaf9cac.png" alt="Markdown"></p><ul><li>Padding for same image size<br>Do reflection padding on each image to generate images with same size.</li></ul><p><img src="http://i2.tiimg.com/640680/06d9ead45f4caa9a.png" alt="Markdown"></p><p>The margin is the reflection of the original image<br>Store in HDF5</p><ul><li>Save image and label as HDF5<br>EM data as training set’s image and contour as label</li></ul><p>Save as uint8   (51, 530, 835)</p><h3 id="Train-membrane-prediction-model"><a href="#Train-membrane-prediction-model" class="headerlink" title="Train membrane prediction model"></a>Train membrane prediction model</h3><p>Input image and label are the 51 bundle sections.</p><p><strong>Train model args:</strong><br>Run on two machines: one with one gpu and another with 4 gpus</p><figure class="highlight lsl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">CUDA_VISIBLE_DEVICES=<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span> python3 -u bin/train.py -t data/train_set/ -dn em_51 -ln mask_51 -o outputs/nmj0729mixloss -lr <span class="number">0.001</span> --volume-total <span class="number">40000</span> --volume-save <span class="number">2000</span> -mi <span class="number">4</span>,<span class="number">256</span>,<span class="number">256</span> -g <span class="number">1</span> -c <span class="number">12</span> -b <span class="number">4</span> -lt <span class="number">4</span> -ac <span class="number">2</span></span><br><span class="line"></span><br><span class="line">CUDA_VISIBLE_DEVICES=<span class="number">0</span> python3 -u bin/train.py -t data/train_set/ -dn em_51 -ln mask_51 -o outputs/nmj0729mixloss -lr <span class="number">0.001</span> --volume-total <span class="number">40000</span> --volume-save <span class="number">2000</span> -mi <span class="number">4</span>,<span class="number">256</span>,<span class="number">256</span> -g <span class="number">1</span> -c <span class="number">6</span> -b <span class="number">1</span> -lt <span class="number">4</span> -ac <span class="number">2</span></span><br><span class="line"></span><br><span class="line">#-lt <span class="number">4</span> focal and dice loss</span><br></pre></td></tr></table></figure><p>References:</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">CUDA_VISIBLE_DEVICES=<span class="number">0</span>,<span class="number">1</span> python3 -u bin/synapse_pytorch/train<span class="selector-class">.py</span> -t data/cremi/ -dn images/im_A_v2_200.h5@images/im_B_v2_200.h5@images/im_C_v2_200<span class="selector-class">.h5</span> -ln gt-syn/syn_A_v2_200.h5@gt-syn/syn_B_v2_200.h5@gt-syn/syn_C_v2_200<span class="selector-class">.h5</span> -o outputs/cremi0719mixloss -lr <span class="number">0.001</span> --volume-total <span class="number">400000</span> --volume-save <span class="number">20000</span> -mi <span class="number">24</span>,<span class="number">256</span>,<span class="number">256</span> -g <span class="number">2</span> -c <span class="number">6</span> -<span class="selector-tag">b</span> <span class="number">2</span> -l mix</span><br><span class="line"><span class="selector-id">#b</span>:<span class="number">6</span>  try to keep gpu and batch size same</span><br></pre></td></tr></table></figure><p>The hp003 memory is small, also train on hpc</p><h4 id="Check-loss"><a href="#Check-loss" class="headerlink" title="Check loss"></a>Check loss</h4><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tensorboard <span class="attribute">--logdir</span>=outputs/nmj0729mixloss</span><br></pre></td></tr></table></figure><p>Monitor loss and test on new EM image. If is good, train it longer with more GPUs</p><p>Train loss( DICE + Focal loss)</p><p><img src="http://i2.tiimg.com/640680/68a5d5858d2eca5f.png" alt="Markdown"></p><p>Focal loss</p><p><img src="http://i2.tiimg.com/640680/14e81611ee917fa6.png" alt="Markdown"></p><p>Dice loss</p><p><img src="http://i2.tiimg.com/640680/20dac5fc8da5ae1f.png" alt="Markdown"></p><p>It seems that the combined loss and both focal and dice loss decrease well.</p><h4 id="real-time-monitoring-predicted-result"><a href="#real-time-monitoring-predicted-result" class="headerlink" title="real time monitoring predicted result"></a>real time monitoring predicted result</h4><p>Use TensorboardX to monitor predicted results:</p><figure class="highlight dockerfile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">if i % <span class="number">20</span> == <span class="number">0</span>:</span><br><span class="line">            <span class="comment">#draw image every 20 batches</span></span><br><span class="line">            writer.add_image(<span class="string">'EM image '</span>+str(i),</span><br><span class="line">                             torchvision.utils.make_grid(<span class="keyword">volume</span><span class="bash">), i)</span></span><br><span class="line"><span class="bash">            writer.add_image(<span class="string">'GT image '</span>+str(i), torchvision.utils.make_grid(label), i)</span></span><br><span class="line"><span class="bash">            writer.add_image(<span class="string">'predict image '</span>+str(i), torchvision.utils.make_grid(output), i)</span></span><br></pre></td></tr></table></figure><p>This will allow me to see the improvement of model’s performance more clearly.</p><p>EM in 3680th batches</p><p><img src="http://i2.tiimg.com/640680/6ba0407959138c82.png" alt="Markdown"></p><p>Ground truth in 3680th batches</p><p><img src="http://i2.tiimg.com/640680/83ebcf388707f0a6.png" alt="Markdown"></p><p>Predicted in 3680th batches</p><p><img src="http://i2.tiimg.com/640680/f618886ef775eb3c.png" alt="Markdown"></p><p>Then I will predict new EM image which is preprocessed by the previous steps. Then do proofreading on the predicted membrane. Then do distance transformation to generate seeds.</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;It is my main task during my summer intern in Lichtman lab.&lt;br&gt;The codes related are here: &lt;a href=&quot;https://github.com/james20141606/Summer_Intern/tree/master/NMJ&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;NMJ codes&lt;/a&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="summer intern" scheme="https://www.cmwonderland.com/categories/summer-intern/"/>
    
    
      <category term="deep learning" scheme="https://www.cmwonderland.com/tags/deep-learning/"/>
    
      <category term="project" scheme="https://www.cmwonderland.com/tags/project/"/>
    
      <category term="summer intern" scheme="https://www.cmwonderland.com/tags/summer-intern/"/>
    
      <category term="neural science" scheme="https://www.cmwonderland.com/tags/neural-science/"/>
    
      <category term="Jeff Lichtman" scheme="https://www.cmwonderland.com/tags/Jeff-Lichtman/"/>
    
  </entry>
  
  <entry>
    <title>Wittgenstein’s love and philosophy</title>
    <link href="https://www.cmwonderland.com/2018/06/20/wittgenstein_bio/"/>
    <id>https://www.cmwonderland.com/2018/06/20/wittgenstein_bio/</id>
    <published>2018-06-21T03:19:27.000Z</published>
    <updated>2018-07-13T13:40:36.925Z</updated>
    
    <content type="html"><![CDATA[<p>When I prepared my presentation about Wittgenstein, especially chapter RELUCTANT PROFESSOR, I write down some of my thoughts towards Wittgenstein, some of them is biased, because I read something which reveals the weakness and defect about him:</p><a id="more"></a><blockquote><p>To be honest, I first found Wittgenstein very mysterious and great (part of the reason is the translation of his german name sounds mysterious). But now I feel little disappointed. I see more a crazy and strange man rather than a genius. He may be a genius, but if it is because his madness and defect (I know it is wrong to say like this, but I can always have such feelings that some man is “great” just because he is strange or even mad), I don’t think it is a great thing. Also I am always confused about philosopher, they always think themselves have the right to judge everything, it is really ridiculous. So I am not happy about many of his thoughts concerning language, science and maths.</p></blockquote><p>The content which gives me the deepest impression is about the death of Francis.</p><p>By the time the second world war broke out, Skinner’s period as an apprentice had come to an end and he returned to Cambridge and he seems to have made an attempt to return to theoretical work. But<br>he knew that he was losing Wittgenstein’s love. After his return to Cambridge, he and Wittgenstein lived separately</p><p>In 1941 Francis had been taken seriously ill with polio and had been admitted into hospital. On 11 October 1941, Francis died.<br>Wittgenstein’s initial reaction was <strong>one of delicate restraint</strong>. In letters to friends telling them of Francis’s death, he managed a <strong>tone of quiet dignity.</strong> :<br>~He died without any pain or struggle entirely peacefully. I was with him. I think he has had one of the happiest lives I’ve known anyone to have, &amp; also the most peaceful death.~ </p><p>By the time of the funeral  his restraint had gone. He behaves like a <strong>‘frightened wild animal’</strong> at the ceremony, after the ceremony he refused to go to the house but  walked around town.</p><p>The reaction is very pure Wittgenstein’s style. But I don’t think Wittgenstein’s guilt over Francis was because his influence on Francis. It had to do with more about Wittgenstein himself.</p><p>I think Wittgenstein is cruel towards Francis, as he admitted:<br>~In the last 2 years of his life very often loveless and, in my heart, unfaithful to him. If he had not been so boundlessly gentle and true, I would have become totally loveless towards him.~ </p><p>I think it is a little like the normal lovers in daily life. The one who do more and contribute more always have less paid back. The other one always  do not value this sacrifice, and later regret about his/her wrongdoing. In this aspect, Wittgenstein is also a very normal person, or even more stubborn and make more mistakes concerning emotions. He is genius in thinking,  making metaphors and find out the tiniest details of world, but his genius make him innocent and vulnerable in emotion. He is not an all-rounder, like Keynes and Russell. His stubbornness and childish in emotions and love made him more vulnerable, and more sensitive to the world.</p><p>There is a very important concept about Wittgenstein’s love and philosophical ideas:  <strong>solipsism</strong></p><p>Compared with other people Wittgenstein love but not get reward, we can see the characteristics of his love: <strong>a certain indifference to the feelings of the other person.</strong> Neither Pinsent nor Marguerite nor Kirk  were in love with him seemed not to affect his love for them. Indeed, it perhaps made <strong>his love easier to give</strong>, for the relationship could be <strong>safe</strong>, in the <strong>splendid isolation of his own feelings</strong>.  But Francis acts opposite, he gave too many love to Wittgenstein, it seems that the <strong>”overlove”</strong> made Wittgenstein feel unsafe. He is a man which loves to contribute something to others but afraid of <strong>“having too many reaction and reward”</strong> It is very strange to say “I understand him”, but I can really understand part of his philosophy in love. Sometimes I also felt it is better to contribute and make others happy. But the idea to have others treat you well too. You may want the reaction, but you are more afraid and frightened about the “disappointment”, the afraid overcome your desire for intimacy. I know several friends who are great at their study and work but holds the similar ideas about love, the <strong>ambivalent</strong> about love made them vulnerable. Actually I think love is really difficult for almost everyone, it is particularly difficult for someone like Wittgenstein.</p><p>Wittgenstein’s ideas towards love is the reflect of his core value towards life and his work. We can say that most of his later work is to against the philosophical solipsism which once attracted him very much. He characterized his later work as <strong>an attempt to show the fly the way out of the fly-bottle.</strong> Here I don’t think it is a very high comment on his contribution to other people, but about himself. He is a isolated man, he is always interested in so many things, like he loves more than one person, even more than one gender. But he is also afraid if one thing or one person hurt him or let him down because he is so into them. He always reflects on himself, he is always sensitive to the environment, be careful not to be controlled by other thing and people. This kind of character made him great at criticizing the problem of previous thought about world and give him the ability to do some very creative work. But this kind of character also made him struggle at most of the time.</p><p>Its parallel is the emotional solipsism. Together with Francis the isolation was threatened, and, in the face of that threat, Wittgenstein had withdrawn. I think it is the major reason he behaves really badly towards Francis in his last few years and made him so guilty.</p><p>It shows us what a complicated man Wittgenstein was, most importantly, the author gives us the parallel comparison about his love and his work. We can have a better understanding how one man’s characteristics can deeply shape him,  especially for such a genius.</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;When I prepared my presentation about Wittgenstein, especially chapter RELUCTANT PROFESSOR, I write down some of my thoughts towards Wittgenstein, some of them is biased, because I read something which reveals the weakness and defect about him:&lt;/p&gt;
    
    </summary>
    
      <category term="thoughts" scheme="https://www.cmwonderland.com/categories/thoughts/"/>
    
    
      <category term="philosophy" scheme="https://www.cmwonderland.com/tags/philosophy/"/>
    
      <category term="Wittgenstein" scheme="https://www.cmwonderland.com/tags/Wittgenstein/"/>
    
      <category term="thoughts" scheme="https://www.cmwonderland.com/tags/thoughts/"/>
    
  </entry>
  
  <entry>
    <title>记和爸爸的点点滴滴</title>
    <link href="https://www.cmwonderland.com/2018/06/16/dad/"/>
    <id>https://www.cmwonderland.com/2018/06/16/dad/</id>
    <published>2018-06-17T03:58:06.000Z</published>
    <updated>2018-07-13T13:51:57.110Z</updated>
    
    <content type="html"><![CDATA[<p>今天终于考完试了，也好久没有更新网站了，正好是父亲节，很想写一篇文章回忆一些记忆中细碎但是印象深刻的事。</p><p>我是个很在意各种细节的人，父亲也用一个个细节深深地影响了我，回顾我长大的每一步，每一步都有爸爸带着我向前走，我们一起经历了很多很多的事情，人们都说一个人二十岁之前的经历基本决定了他是一个什么样的人，我要感谢爸爸和妈妈让我经历的成长，让我成长为一个没有让人失望的人。</p><p>我记下了一些一下子就能想到的事情，还有太多太多细节都回荡在脑海里，如果有时间的话，应该仔细地写一写这些宝贵的回忆。</p><p><img src="http://i4.fuimg.com/640680/c81907e7cb9988e7.jpg" alt="Markdown"></p><a id="more"></a><ul><li>幼儿园</li></ul><p>我记得那时候冬天裹得像个大粽子，妈妈给我裹上一层有一层，我面对着爸爸坐在摩托上，爸爸送我去幼儿园，等着老师开门<br>小学。虽然就家离幼儿园很远，但是纵使能到的特别早，坐在第一排，后来想想这应该是我意识里要做到最好的源头，在我连记忆都保存不下来的岁月里，是爸爸妈妈帮我塑造起了这样的意识。</p><p>我记得直到后来上了小学，赶不上公交了，爸爸就骑着摩托带我一路追上，我最喜欢的就是爸爸说算了直接给你送到吧，我就反而可以提前到校门口，早点到学校总能给我一种很好的感觉，这是父母给我的良好的习惯，永远要早一点，永远不要往后拖。</p><p>我在路上总是喜欢问爸爸无穷无尽的问题，那时候应付一个小孩子多简单呀，爸爸还会教我认各种各样的车，我还记得有一次说到计算图形的面积，爸爸说，还有一个东西叫微积分，还可以计算曲线的面积，比如皮鞋鞋面的面积，我记得很清楚，那是爸爸载着我在买菜的时候说的，我惊讶极了，还有这么高级的东西，我给爸爸说，能不能暑假教会我。现在想想自己当初好天真，但是这种让我每天都对知识充满新鲜与好奇感的生活真的很美妙。</p><ul><li>小学</li></ul><p>我记得爸爸在我一年级的时候就买来奥数书和我一起钻研，这是一件让我感激至今的事情，这个行为带给我的深远影响可能远远地被低估了。我记得那些题，很难，也很有意思，比如用直线分割平面，数列找规律等等问题，有的题还要爸爸研究了答案讲给我。初一的时候郑老师给我们说，要有中招意识，这个思维方式已经很先进了，但是我觉得爸爸的思维方式更先进，让我时刻先人一步地学习，我记得我小学第一学期期末考试就考了双百分，现在想想，得益于父母的眼光，我的每一步都走的很早又很准确，这是多么幸运的事情。</p><p>我记得有一段时间我和爸爸迷上了九宫格和十六宫格，我们比赛谁能先填出来，有一天爸爸告诉我说他半夜起来还填了一会儿，我当时觉得特别有趣，和爸爸比赛做数学题在我看来是一个小男孩能够经历了最佳童年运动之一。我会经常回忆起这个片段，并且会暗下决心，设计更多有趣的挑战，亲子游戏，和我以后的孩子一起经历。虽然时代进步，可以利用的方法和工具越来越多，但是父母的亲身参与陪伴才是这些事的内核所在，如果父亲只是扔下一本奥数书，或者说，你把这十个数独题做出来，我想年幼的我会索然无味，可能就变得和我学电子琴一样了，但是爸爸通过亲身的陪伴把事情变得非常有趣，在我很小的时候就揭示出了“世界上有很多有挑战并且有趣的事情”的道理。我想如果世界上的爸爸们都能这么配着孩子做这些有趣的事情，很多孩子的人生可能会变得很不一样了。</p><p>我还特别感谢爸爸在我小学的时候带着我打篮球和羽毛球，从我使劲扔都仍不到篮筐开始，到我可以一放学就抱着球满场飞奔。我一直觉得小学的时候是我身体最好的时候，也为我中学时候储备了不错的身体，可惜这种锻炼的精神没能被自己坚持下来。那时候总是被爸爸叫着去运动，打羽毛球的时候爸爸总是会想办法调动我，我可以打的越来越远了，投的越来越远了，很可惜到了郑州之后打球的机会就非常少了，风也打，时间也少，再也没有了在老房子门前打羽毛球等着妈妈叫吃饭，还有在篮球场打球打得汗流浃背被爸爸妈妈叫回家的美好记忆了。</p><p>我还记得小学的时候有一次老师让每个人写一张速算卡，以后课间拿着背，别的同学很多是家长手抄，我自己写好公式之后，爸爸就用word敲好公式，然后彩色打印，粘在硬纸板上，我拿着比其他小朋友更精致的速算板，让我无比的自豪，其实我不需要速算卡来帮助我学速算，我每天都做速算题，做很多数学题，六年级还拿了速算比赛的第一名，但是爸爸对细节的重视，利用更好的工具做的更好的行为让我印象深刻。现在我也总是会想办法找到最好的工具，把东西做的尽量完美，这种对待事物的态度对每个人，尤其是对于小孩子的影响是巨大的。</p><p>可以说，爸爸妈妈对教育的重视和精细是我童年收到的最宝贵的礼物，</p><ul><li>小学毕业</li></ul><p>得到去郑外考试的消息，爸爸就立马决定带我去郑州考试，记得当时我还在妈妈办公室玩电脑，正在畅想六年级的暑假，文具盒都已经找不到了。我跟着爸爸做大巴到郑州站，做公交到了偏僻荒凉的西开发区，在那片有艺术气息但是荒废的独栋转车，然后找到河南工业大学，经历了一场偷偷摸摸的安排在晚上的考试，然后彻底改变了自己人生的轨迹。我在想如果是我的话，我会不会这么果决地带着自己的孩子去考试呢，很有可能我会错过一次改变我的孩子和家庭命运的机会，而父母总是在我还年幼无知，不知不觉间，帮我做了一次次无比正确的，让我感激不尽的选择。</p><p>我还记得考试一段时间之后的故事，郑外的考试数学满分一百份，英语五十，语文三十，我当时和爸爸在外面等我认识的一个家长的孩子交流做题的感觉，他应该是考遍了郑州能考的学校，我能感觉到自己信息的闭塞，但是当时题做的确实很不错，交流了一遍之后感觉自己应该都做对了，但是英语很难，觉得自己做的一般，和在南阳的考试完全不是一个难度。回到南阳之后过了很久，都一直没有消息。有一天爸爸送我去睿源上课外班，说起来还没有消息的事情，我大概觉得有点没面子，就说，反正就算考上了也不一定去上，这是大家一直说的，我太小，就算考上了也不一定要去。我记得正好到睿源的楼下了，爸爸停车看着我说，虽然是这么说，但是我很想接到打来的电话。这句话我记得非常非常深，一直到今天我都会常常想起。后来终于接到了电话，我的第一反应就是，一定要去，这是我人生的挑战和转折的起点，父亲的一句话给我带来了多少影响，我说不清楚，但是那种深藏的强烈的期望感在很多年里都在催促着我，在我悠闲的小学时光之后，一下子体会到我应该做些什么。父母的无数的行动和点滴的言语，让我在远离父母的时候从毫无想法的小学生迅速地有了很多意识和想法，这种意识比家长和老师的灌输要有用的多。</p><ul><li>初中的长路漫漫</li></ul><p>还记得初中求学的艰辛和有趣，炎热的夏天，我和爸爸从家里出发，心中充满了不舍，坐上去郑州的大巴，为了早点走，不想等下一班，我们就坐在大巴车偷偷加的小椅子上坐了一路，一路颠簸到郑州。我记得我写的对家人的想念的文章还因为感情细腻被郑老师朗读了。我一学期也就回家一两次，而爸爸妈妈却要轮流来看我，实在是非常辛苦，尤其是妈妈，要坐一天的车，晚上还要买菜做饭，周末为我洗衣服，带我继续在市里奔波上课，为了让我接受好一倍的教育，父母付出了多了好几倍的辛劳，这份付出一直是我内心深处一份非常重要的力量。</p><p>还记得好几次回家的经历，有父亲在国庆节接我到火车站坐大巴，因为买不到票了，一直等到晚上八点多，火车站上空还升起来了孔明灯，回到家已经凌晨了，妈妈还是做好了饭等着。第一个国庆我们去快到高速口的地方坐大巴，也是车非常少，我们最后坐上了一辆一路晃晃悠悠跑的相当慢的车，还在省道上走了一段。还有一年相当有趣，实在等不到车的我们突然在高速口看到一辆从郑州回南阳想挣点外快的救护车，于是我们大概七八个人都挤进去，坐着闪着等的救护车回了南阳。高中之后都是大家一起包车了，也没有太多记忆深刻的回家的记忆，但是初中的每次经历真的让我记忆格外的深刻，每次终于到了南阳的收费站，淡黄色的灯光一下子出现在眼前，那种家的气息就出现了，妈妈总是做好了可口的饭菜在等着，直到大学，已经有了很多的回家的方式，但是让我印象最深刻的依然是初中的求学经历。</p><p>高中的保送生考试，依然是和爸爸一起到了北京，然后记得后学校等成绩的那天，正好发烧了，然后爸爸那天到了郑州，然后就接到消息，被清华录取啦，这几年的艰辛的路都没有白走。记得去大学报到还是和爸爸一起，住在清华南边的宾馆，提前一天晚上逛校园，在法学院门前合照，以及因为转系的风波爸爸来北京，和学校协商，最终顺利地转了专业，每一步能走的顺心如意，真的要感谢父亲。</p><p>我记得初中有一年父亲节，我忘了是初一还是初二，我期末考试考了第一名，非常高兴，打电话给爸爸，正好是父亲节，我就说，我送你的礼物就是我考了第一名，那个时候我很骄傲，不过那个时候我能给的就只是考试的成绩，很多年了这一直是我能做的为数不多的事情。我很感谢父母给了我那么好的机会，让我一直追求我想追求的事情，让我少受到了很多束缚，让我不用想世俗的很多事情，让我做我想做的事情。</p><p>谢谢爸爸让我成长为一个更好的人，我想，等我以后当父亲了，我也要做爸爸这样的父亲。</p><p>煽情的话也不多说啦，<strong>祝爸爸父亲节快乐~</strong></p><p><img src="http://i4.fuimg.com/640680/c99b6d3389f52a7a.jpg" alt="Markdown"></p><center><font color="blue" ,font="" size="5">如果我懂得了爱，那一定是因为你</font></center><center><font color="pink" ,font="" size="5">If I Know What Love is, It is Because of You</font></center>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;今天终于考完试了，也好久没有更新网站了，正好是父亲节，很想写一篇文章回忆一些记忆中细碎但是印象深刻的事。&lt;/p&gt;
&lt;p&gt;我是个很在意各种细节的人，父亲也用一个个细节深深地影响了我，回顾我长大的每一步，每一步都有爸爸带着我向前走，我们一起经历了很多很多的事情，人们都说一个人二十岁之前的经历基本决定了他是一个什么样的人，我要感谢爸爸和妈妈让我经历的成长，让我成长为一个没有让人失望的人。&lt;/p&gt;
&lt;p&gt;我记下了一些一下子就能想到的事情，还有太多太多细节都回荡在脑海里，如果有时间的话，应该仔细地写一写这些宝贵的回忆。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://i4.fuimg.com/640680/c81907e7cb9988e7.jpg&quot; alt=&quot;Markdown&quot;&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="life" scheme="https://www.cmwonderland.com/categories/life/"/>
    
    
      <category term="life" scheme="https://www.cmwonderland.com/tags/life/"/>
    
  </entry>
  
  <entry>
    <title>Artificial Intelligence and Wittgenstein&#39;s use theory of meaning</title>
    <link href="https://www.cmwonderland.com/2018/06/15/Artificial%20Intelligence%20and%20Wittgenstein&#39;s%20use%20theory%20of%20meaning/"/>
    <id>https://www.cmwonderland.com/2018/06/15/Artificial Intelligence and Wittgenstein&#39;s use theory of meaning/</id>
    <published>2018-06-16T02:30:17.000Z</published>
    <updated>2018-07-13T13:50:43.325Z</updated>
    
    <content type="html"><![CDATA[<h3 id="AI’s-view"><a href="#AI’s-view" class="headerlink" title="AI’s view:"></a>AI’s view:</h3><h4 id="what-is-AI"><a href="#what-is-AI" class="headerlink" title="what is AI"></a>what is AI</h4><p>Artificial intelligence means using program to accomplish tasks human can do by simulating human’s brain. The purpose of artificial intelligence is to bypass the brain and body to achieve a full understanding of human intelligence. The idea can be traced back to <strong>Turing machine</strong> (we can already see some interesting coincidence here~ Turing has attended Wittgenstein’s class in Cambridge, though Turing holds different opinions, Wittgenstein values his ideas very much. Maybe Turing also get some inspiration during the idea conflict.)</p><a id="more"></a><p>In 1956, H. Simon and others designed the first AI program called “logical theoretician”. It can prove 38 of the first 52 theorems in “Principles of Mathematics.”</p><p>Today, AI has thrived in research, application, and education field in computer science and other disciplines. It specializes in issues such as procedural languages, methods of reasoning and problem solving, visual identification, and expert systems. The most powerful use of nowadays AI is in computer vision. </p><p>AI tries to help humans understand the human intelligence by means of symbolic operations, so it proposes under what conditions we have reason to attribute the state of mind to a purely physical system. It has made a great contribution to the development of cognitive science and the philosophy of the mind. There is a distinction between “strong AI theory” and “weak AI theory.” The weak AI theory aargues that computer programs help to understand the state of mind. This point has been widely accepted. The strong point of view claimed that the “heart” of the computer exemplifies the human psychological process. This argument is controversial. Searle argues that, unlike people, the syntactic manipulation of symbols by machines is not accompanied by a semantic understanding of the meaning of symbols.</p><h4 id="Why-Wittgenstein’s-theory-should-be-related-to-AI"><a href="#Why-Wittgenstein’s-theory-should-be-related-to-AI" class="headerlink" title="Why Wittgenstein’s theory should be related to AI"></a>Why Wittgenstein’s theory should be related to AI</h4><p>The AI discipline was formally established until the Dartmouth Conference in 1956, at that time Wittgenstein had already passed away. However, this does not mean that, from his discussion of other philosophical issues, we cannot find its philosophical inspiration for artificial intelligence, and on this basis reconstruct an artificial intelligence using Wittgenstein’s philosophy.</p><p>Artificial intelligence does not have a certain level of unity as mature disciplines such as physics and chemistry. In other words, the fundamental issues related to the basic definition of disciplines, such as what is “intelligence” “what is artificial intelligence,” and “what is the basic means of artificial intelligence”, the divergence among different AI schools is almost impossible to reconcile (until today It is far from being able to reach a consensus on how artificial intelligence can be realized. No one has a effective plan to achieve it.</p><p>One of the hopes of the public in philosophical research is that philosophy should be able to provide some kind of “integrated” solution to conflicts between discourse systems of various natural sciences (even between science and common sense). For example, philosophy of physics should provide some reconciliation solutions for conflicts between quantum mechanics and relativity; and cognitive science philosophy must provide some reconciliation solutions for conflicts between common-sense psychology and neuroscience; as for metaphysics It is necessary to provide a reconciliation program on the topic of “what is there?” and the conflict between common sense insights and scientific perspectives in the most general sense. From this point of view, facing the chaotic situation of within artificial intelligence discipline, people naturally expect philosophers to do something for them.</p><p>I think Wittgenstein’s philosophy may has closest relation to artificial intelligence and it can also give people inspiration on AI.</p><h4 id="The-pursuit-of-Unity-in-Wittgenstein’s-Philosophy"><a href="#The-pursuit-of-Unity-in-Wittgenstein’s-Philosophy" class="headerlink" title="The pursuit of Unity in Wittgenstein’s Philosophy"></a>The pursuit of Unity in Wittgenstein’s Philosophy</h4><p>Wittgenstein’s philosophy seems to have an inherent “integrated” quality, that is, its philosophy is always trying to provide a unified solution to all problems in the world. This academic style is scarce for the present situation of scattered research on AI. (actually, due to the rising of probability and statistics methods in AI, we focus less on the ideas and essence of AI, but only on techniques of math.)</p><p>This philosophical temperament was embodied in Wittgenstein’s Early work(TLP). In the “preface” of TLP, he wrote a bit arrogantly:</p><blockquote><p>“……On the other hand the truth of the thoughts that are here communicated seems to me unassailable and deﬁnitive. I therefore believe myself to have found, on all essential points, the ﬁnal solution of the problems. And if I am not mistaken in this belief, then the second thing in which the value of this work consists is that it shows how little is achieved when these problems are solved”</p></blockquote><p>The broad coverage of this book undoubtedly confirms his rhetoric. Within just over 20,000 words, Wittgenstein actually involved three topics: the metaphysical construction of the world and the metaphysical world. Linguistic representations, and the “right of being silence” for matters that cannot be characterized. These three topics basically correspond to the three steps of the “<strong>knowledge representation</strong>” task of artificial intelligence science: </p><ul><li>First, the metaphysical understanding of the object being represented; </li><li>Second, the technical means (especially the logical technical means) of knowledge representation </li><li>Third, the problem of delineation of the possibility boundary of the scope of knowledge representation (under the premise of selecting a specific representation method).</li></ul><p>From the perspective of the history of AI development, Wittgenstein ’ thought roughly corresponds to the “primary physics plan” of Hayes, who once attempted to use predicate calculus as a technical means to compile human daily physics knowledge (in contrast to scientific physics)  into axioms and set certain inference rules so that the system can automatically infer the required conclusions in the relevant context. </p><p>In other words, early Wittgenstein, like Hayes, tried to use the “once-for-all” approach to complete the task of comprehensive representation of daily knowledge on the premise of formal logic. They also assumed that in this process of knowledge representation, any <strong>particular agent</strong> that uses this knowledge is either negligible (as in Hayes) or cannot be characterized (in Wittgenstein, the “subject” merely “shows” rather than “characterizes” the boundaries of the linguistic representation). Although Wittgenstein eventually gave up the TLP system, and Hayes’s “simple physics” plan has now become part of  AI history, they share the same kind of theoretical courage to try to integrate all aspects of daily knowledge. And ambition, is what today’s AI engineers lack. Today’s AI theoreticians and engineers often abandon the interpretability of the model but pursue the increase in the accuracy of the model. This is an unavoidable but pitiable solution to solve complex problem in daily life. Nowadays AI already loses its interpretability under simple tasks. Could it become interpretable under hard tasks? Or maybe we can only solve the hard problem and produce real AI when we integrate everything again.</p><p>This kind of advocating comprehensive temperament has not disappeared in Wittgenstein’s later work Philosophical Investigations. Judging by appearance, this book is only constituted by a few scattered reviews of “philosophical sentiments”, but the “sloppyness” does not obscure its substantial comprehensiveness. In the preface of the book, he recognizes:</p><blockquote><p>“The thoughts that I publish in what follows are the precipitate of philosophical investigations which have occupied me for the last sixteen years. They concern many subjects: the concepts of meaning, of understanding, of a proposition and sentence, of logic, the foundations of mathematics, states of consciousness, and other things.……Originally it was my intention to bring all this together in a book whose form I thought of differently at different times.……After several unsuccessful attempts to weld my results together into such a whole, I realized that I should never succeed.”</p></blockquote><h4 id="Wittgenstein-uses-language-games-to-settle-everything"><a href="#Wittgenstein-uses-language-games-to-settle-everything" class="headerlink" title="Wittgenstein uses language games to settle everything"></a>Wittgenstein uses language games to settle everything</h4><p>The topics involved in the “Philosophical Investigations” are essentially related to each other, although the author has to observe the same or almost the same entity from different perspectives, and draw new sketches. </p><p>If we summarize the subject of “Philosophical Investigations” in the most succinct way with AI colors, then we might say that this book discusses  <strong>agents</strong>: Under what kind of normative constraints, in a dynamic  diachronic environment, using relevant characterization tools (especially daily language)  to perform causal interactions with the environment and other agents, and ultimately accomplish certain tasks. </p><p>Obviously, because the interactive process between the agent and the environment (using language) is very complicated, Wittgenstein had to carefully philosophize the normative conditions of the various links involved in the process. For example, his reflection on the concept of “understanding” and the concept of “consciousness” is about the agent itself; the investigation of concepts such as “logic”, “proposition”, and “meaning” is about representation of mediation; and discussion about “life “Formal”  involves the portrayal of an external environment grasped by an agent. </p><p>Wittgenstein put all these discussions together in the general framework of language games and settled them one by one. By constantly exploring new varieties of language games, he eventually turned his entire post-philosophy into an unusually open system: this system has no clear boundaries and it can  extend its jurisdiction to new virgin lands at any time.</p><p>It is really hard to put all these things together, but it seems that it is the best way to achieve real artificial intelligence. Nowadays many “naive” researchers are immersed in tuning there networks to do better at image classification、object recognition or related “simple/single task”. But it is far from the real intelligence. Recently some excellent scientists concer more about how to produce a real agent, but it is hard to use current state-of-art deep learning architecture to design a real agent. So deep learning cannot play hard games( game like GO looks hard to human, but it is more of a computational work to computer, so it can’t reflect the real intelligence level of computer).</p><h4 id="Real-time-information-processing-when-using-it"><a href="#Real-time-information-processing-when-using-it" class="headerlink" title="Real-time information processing when using it"></a>Real-time information processing when using it</h4><p>From the AI’s point of view, Philosophical Investigations surpass TLP because it no longer regards the static knowledge system as a focal point of philosophical theories, but shifts the focus to the <strong>actions of agents</strong>. Moved to the real-time processing of information. In other words, the issue of subject portrayal in the marginal position of its early philosophy has now become one of the focuses of the study. Considering the diversity of different real-time information processing tasks that different agents face, the complexity involved in Philosophical Investigations naturally will be far better than the TLP - which also partly explains why the late Wittgenstein’s philosophy is so complicated. In AI textbooks, the problem solving task also correspond to completely different problem solving techniques!  (For example, you can use Bayesian networks for reasoning, use genetic algorithms to do optimization of solution selection, use neuron networks for pattern recognition, use symbolic AI techniques for expert systems, etc.). Wittgenstein used a “language game” cover to try to put some of its most elementary neutrality on the various information processing processes. In the fragmented AI kingdom, we have seen this kind integration.</p><h4 id="Wittgenstein’s-“Factualism”-and-use-theory"><a href="#Wittgenstein’s-“Factualism”-and-use-theory" class="headerlink" title="Wittgenstein’s “Factualism” and use theory"></a>Wittgenstein’s “Factualism” and use theory</h4><p>From this point of view, Wittgenstein is indeed a different kind of alternative (or, he is an anti-analytic philosopher in analytic philosophy, and an analytical philosopher in anti-analytic philosophers). This special philosophical character mainly reflects in the atmosphere of some kind of “Factualism” by his writings.</p><p>“Factualism is referring to a philosophical style in which philosophers are to faithfully describe the known areas of humanity. In other words, they should not be as ignorant of the linguistic phenomena of everyday language as the traditional rationalist philosophers. In other words, “Factualism” must balance the “respection of  people’s existing experience” and “rethinking of the existing human experience”, and thus gradually tease out the deep formal framework from the complex human experience.</p><p>From the author’s point of view, the following paragraphs written by Vickers in Philosophical Studies fully demonstrate this practical essence of Vickers’ later philosophy—and it is in a way that makes AI experts happy:</p><blockquote><p>Think of the tools in a toolbox: there is a hammer, pliers, a saw, a screwdriver, a rule, a glue-pot, glue, nails and screws. a The functions of words are as diverse as the functions of these objects. (And in both cases there are similarities.)</p><p>Of course, what confuses us is the uniform appearance of words when we hear them in speech, or see them written or in print. For their use is not that obvious. Especially when we are doing philosophy!</p></blockquote><h5 id="This-corresponds-to-the-programming-design-level-of-programming"><a href="#This-corresponds-to-the-programming-design-level-of-programming" class="headerlink" title="This corresponds to the programming design level of programming"></a>This corresponds to the programming design level of programming</h5><p>In the language of AI, Wittgenstein actually distinguishes between the <strong>two interfaces of language operation</strong>: </p><ul><li>the usage of the terms between the words “user-friendly” is not obvious, and the traditional philosophers are also confused by its similar surface. </li><li>In a deeper “programming level”, the differences between the functional structures of the words are presented - in which each functional structure corresponds to a different input-output relationship and different input-oriented operations. </li></ul><p>Obviously, in Wittgenstein’s opinion, the real task of philosophy is to find the differences between these functional uses as marked by words—or to find real programs that allow our natural language mechanisms to flourish. This is the true ground that philosophers want to hug, or the “pragmatic facts” faced by the “Factualer”.</p><p>This position of Wittgenstein’s inspiration for today’s AI is: If an AI engineer wants to faithfully reproduce intelligent activities on the symbolic representation level, his working sample can neither be too close to the surface form of human natural language, nor too far away. </p><p>Not too close is because the surface form of natural language does not show the real mechanism of the use of words; not too far away is because the somewhat confusing “user-friendly” interface, after all is the starting point to explore the real mechanism behind it. </p><p>The recommended working procedure should be: The researcher must first observe the different functional roles played by the similar “user interface” <strong>in different use environments</strong> (ie, the input-output relationship), and then re-introduce the most likely programs. Then find the most appropriate engineering simulation method in the existing “arsenal library” of computer science. </p><p>This means that the engineering reproduction of the language operation mechanism is essentially a “top-down” construction process that use the experience of human natural language mechanisms. </p><p>At the initial stage of this construction process, that is, at the stage of qualitatively characterizing the operating mechanism of the characterization system, the reference significance of Wittgenstein’s philosophy will be most clearly demonstrated.</p><h2 id="Additional-Basic-data-mining-of-Philosophical-Investigations"><a href="#Additional-Basic-data-mining-of-Philosophical-Investigations" class="headerlink" title="Additional: Basic data mining of Philosophical Investigations"></a>Additional: Basic data mining of Philosophical Investigations</h2><p>I also spent some time analyzing the context of Philosophical Investigations using python, since it is more casual and less organized, it seems it’s hard to learn many information from it.</p><h3 id="prepare-data"><a href="#prepare-data" class="headerlink" title="prepare data"></a>prepare data</h3><figure class="highlight xl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> jieba</span><br><span class="line"><span class="keyword">import</span> os, codecs  </span><br><span class="line">from collections <span class="keyword">import</span> Counter  </span><br><span class="line"><span class="keyword">import</span> matplotlib.pylab <span class="keyword">as</span> plt</span><br><span class="line">plt.style.use(<span class="string">'ggplot'</span>)</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> matplotlib</span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"><span class="keyword">with</span> codecs.open(<span class="string">'text.txt'</span>, <span class="string">'r'</span>, <span class="string">'utf8'</span>) <span class="keyword">as</span> f:  </span><br><span class="line">    txt = f.read()</span><br><span class="line">def get_words(txt):  </span><br><span class="line">    seg_list = jieba.cut(txt)  </span><br><span class="line">    c = Counter()  </span><br><span class="line">    <span class="keyword">for</span> x <span class="built_in">in</span> seg_list:  </span><br><span class="line">        <span class="keyword">if</span> len(x)&gt;<span class="number">1</span> <span class="built_in">and</span> x != <span class="string">'\r\n'</span>:  </span><br><span class="line">            c[x] += <span class="number">1</span>  </span><br><span class="line">    return c  # c.most_common(<span class="number">100</span>)</span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:  </span><br><span class="line">    <span class="keyword">with</span> codecs.open(<span class="string">'text.txt'</span>, <span class="string">'r'</span>, <span class="string">'utf8'</span>) <span class="keyword">as</span> f:  </span><br><span class="line">        txt = f.read()  </span><br><span class="line">    a = get_words(txt)</span><br><span class="line">dat = a.most_common(<span class="number">1000</span>)</span><br><span class="line">countlist = np.ndarray([<span class="number">1000</span>,<span class="number">2</span>]).astype(<span class="string">'str'</span>)</span><br><span class="line"><span class="keyword">for</span> i <span class="built_in">in</span> range(<span class="number">1000</span>):</span><br><span class="line">    countlist[i][<span class="number">0</span>] = dat[i][<span class="number">0</span>]</span><br><span class="line">    countlist[i][<span class="number">1</span>] = dat[i][<span class="number">1</span>]</span><br><span class="line">wordlist = np.array(re.split(<span class="string">' |; |, |\*|\n'</span>,txt))</span><br><span class="line">select_ind = np.<span class="keyword">where</span>(np.isin(countlist[:,<span class="number">0</span>],np.array(selectwords)))[<span class="number">0</span>]</span><br><span class="line">countlist_ = []</span><br><span class="line"><span class="keyword">for</span> i <span class="built_in">in</span> range(<span class="number">1000</span>):</span><br><span class="line">    countlist_.append(countlist[i,<span class="number">0</span>]+<span class="string">': '</span>+countlist[i,<span class="number">1</span>])</span><br><span class="line">pd.DataFrame(np.array(countlist_)[:<span class="number">200</span>].reshape(<span class="number">20</span>,<span class="number">10</span>))</span><br></pre></td></tr></table></figure><h3 id="words-counts-most-frequent"><a href="#words-counts-most-frequent" class="headerlink" title="words counts, most frequent"></a>words counts, most frequent</h3><p><img src="http://i2.tiimg.com/640680/8c3c1a7d9cbd30c5.png" alt="Markdown"></p><figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">count</span> = <span class="number">27</span></span><br><span class="line">fig,ax=plt.subplots(<span class="number">1</span>,figsize=(<span class="number">20</span>,<span class="number">10</span>))</span><br><span class="line">ax.bar(<span class="built_in">range</span>(<span class="built_in">count</span>),countlist[:<span class="built_in">count</span>,<span class="number">1</span>].astype(<span class="string">'int'</span>))</span><br><span class="line">ax.set_xticks(<span class="built_in">range</span>(<span class="built_in">count</span>))</span><br><span class="line">ax.set_xticklabels(countlist[:<span class="built_in">count</span>,<span class="number">0</span>])</span><br><span class="line">ax.set_title(<span class="string">'most frequent words'</span>)</span><br></pre></td></tr></table></figure><p><img src="http://i2.tiimg.com/640680/0b08dd44adeb8df9.png" alt="Markdown"></p><h3 id="most-frequent-words-with-specific-meaning"><a href="#most-frequent-words-with-specific-meaning" class="headerlink" title="most frequent words with specific meaning"></a>most frequent words with specific meaning</h3><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">count = <span class="number">50</span></span><br><span class="line">index = index_54[:count]</span><br><span class="line">fig,ax=plt.subplots(<span class="number">1</span>,figsize=(<span class="number">20</span>,<span class="number">10</span>))</span><br><span class="line">barlist = plt.bar(range(count),countlist[index,<span class="number">1</span>].astype(<span class="string">'int'</span>))</span><br><span class="line"><span class="keyword">for</span> <span class="selector-tag">i</span> <span class="keyword">in</span> np.arange(<span class="number">0</span>,<span class="number">2</span>):</span><br><span class="line">    barlist[i].set_color(<span class="string">'r'</span>)</span><br><span class="line"><span class="keyword">for</span> <span class="selector-tag">i</span> <span class="keyword">in</span> np.arange(<span class="number">2</span>,<span class="number">5</span>):</span><br><span class="line">    barlist[i].set_color(<span class="string">'b'</span>)</span><br><span class="line"><span class="keyword">for</span> <span class="selector-tag">i</span> <span class="keyword">in</span> np.arange(<span class="number">5</span>,<span class="number">12</span>):</span><br><span class="line">    barlist[i].set_color(<span class="string">'g'</span>)</span><br><span class="line"><span class="keyword">for</span> <span class="selector-tag">i</span> <span class="keyword">in</span> np.arange(<span class="number">12</span>,<span class="number">26</span>):</span><br><span class="line">    barlist[i].set_color(<span class="string">'m'</span>)</span><br><span class="line"><span class="keyword">for</span> <span class="selector-tag">i</span> <span class="keyword">in</span> np.arange(<span class="number">26</span>,count):</span><br><span class="line">    barlist[i].set_color(<span class="string">'y'</span>)</span><br><span class="line">ax.set_xticks(range(count))</span><br><span class="line">ax.set_xticklabels(countlist[index,<span class="number">0</span>],fontsize=<span class="number">15</span>)</span><br><span class="line">ax.set_title(<span class="string">'Most Frequent Words in Philosophical Investigations'</span>,fontsize=<span class="number">30</span>)</span><br><span class="line">plt.setp(ax.get_xticklabels(), rotation=<span class="number">30</span>, horizontalalignment=<span class="string">'right'</span>)</span><br><span class="line">fig.tight_layout()</span><br></pre></td></tr></table></figure><p><img src="http://i2.tiimg.com/640680/1b87903c55d89ec6.png" alt="Markdown"></p><h3 id="words-frequency-by-chapter"><a href="#words-frequency-by-chapter" class="headerlink" title="words frequency by chapter"></a>words frequency by chapter</h3><figure class="highlight prolog"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">chapterind = [<span class="number">1491</span>,<span class="number">82898</span>]</span><br><span class="line">namelist = countlist[index,<span class="number">0</span>]</span><br><span class="line">def count_frequent(chap,count):</span><br><span class="line">    freqlist =[]</span><br><span class="line">    if chap &lt;<span class="number">1</span>:</span><br><span class="line">        for i in range(count):</span><br><span class="line">            freqlist.append(np.where(wordlist[chapterind[chap]:chapterind[chap+<span class="number">1</span>]] ==namelist[i])[<span class="number">0</span>].shape[<span class="number">0</span>])</span><br><span class="line">    else:</span><br><span class="line">        for i in range(count):</span><br><span class="line">            freqlist.append(np.where(wordlist[chapterind[chap]:] ==namelist[i])[<span class="number">0</span>].shape[<span class="number">0</span>])</span><br><span class="line">    return np.array(freqlist)</span><br><span class="line">freq_var = np.ndarray([<span class="number">2</span>,<span class="number">50</span>])</span><br><span class="line">for i in range(<span class="number">2</span>):</span><br><span class="line">    freq_var[i] = count_frequent(i,<span class="number">50</span>)</span><br><span class="line">freq_var[<span class="number">0</span>,:] =freq_var[<span class="number">0</span>,:]/len1</span><br><span class="line">freq_var[<span class="number">1</span>,:] =freq_var[<span class="number">1</span>,:]/len2</span><br><span class="line">fig,ax=plt.subplots(<span class="number">1</span>,figsize =(<span class="number">10</span>,<span class="number">10</span>))</span><br><span class="line">ax.matshow(np.repeat(freq_var.<span class="symbol">T</span>,<span class="number">4</span>).reshape(<span class="number">50</span>,<span class="number">-1</span>) ,cmap =<span class="string">'jet'</span>)</span><br><span class="line">ax.set_title(<span class="string">'50 Key Words Fluctuation in 2 Parts'</span>)</span><br><span class="line">ax.set_xticks(np.arange(<span class="number">0</span>,<span class="number">12</span>,<span class="number">6</span>))</span><br><span class="line">ax.set_xticklabels([<span class="string">'Part I'</span>,<span class="string">'Part II'</span>])</span><br><span class="line">ax.set_yticks(range(<span class="number">50</span>))</span><br><span class="line">ax.set_yticklabels(namelist)</span><br></pre></td></tr></table></figure><p><strong>You can see the words variation in different chapters</strong>.  Unfortunately, unlike TLP, Wittgenstein only divide his book into two parts, so we can learn very little by feature fluctuation.</p><p><img src="http://i2.tiimg.com/640680/7b056710733efb93.png" alt="Markdown"></p><h3 id="Calculate-different-words’-distances"><a href="#Calculate-different-words’-distances" class="headerlink" title="Calculate different words’ distances"></a>Calculate different words’ distances</h3><p>I also think about how to depict different words’ relationship. One easiest way I think of is calculating two words minimum distance in the book whenever they appears. <strong>For example:</strong><br><figure class="highlight nix"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">def calculate_distance(ind1,ind2):</span><br><span class="line">    <span class="attr">pos1</span> = np.where(<span class="attr">wordlist==namelist[ind1])[0]</span></span><br><span class="line">    <span class="attr">pos2</span> = np.where(<span class="attr">wordlist==namelist[ind2])[0]</span></span><br><span class="line">    num1 ,<span class="attr">num2</span> = pos1.shape[<span class="number">0</span>],pos2.shape[<span class="number">0</span>]</span><br><span class="line">    <span class="keyword">if</span> num1&gt;num2:</span><br><span class="line">        <span class="attr">small</span> = num2</span><br><span class="line">        <span class="attr">large</span> = num1</span><br><span class="line">        <span class="attr">lararr</span> = pos1</span><br><span class="line">        <span class="attr">smarr</span> = pos2</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="attr">small</span> = num1</span><br><span class="line">        <span class="attr">large</span> = num2</span><br><span class="line">        <span class="attr">lararr</span> = pos2</span><br><span class="line">        <span class="attr">smarr</span> = pos1</span><br><span class="line">    <span class="attr">disarr</span> = np.ndarray([small,large])  <span class="comment">#each line calculate the small set's ith word's and large set's every words distance</span></span><br><span class="line">    <span class="attr">arr1=</span> np.repeat(smarr,large).reshape(-<span class="number">1</span>,large)</span><br><span class="line">    <span class="attr">arr2=</span> np.repeat(lararr,small).reshape(-<span class="number">1</span>,small).T</span><br><span class="line">    <span class="attr">mindis</span> = np.min(np.abs(arr2-arr1),<span class="attr">axis=1)</span></span><br><span class="line">    return mindis</span><br><span class="line">def draw_dist_count(ind1,ind2):</span><br><span class="line">    fig,<span class="attr">ax=plt.subplots(1,figsize=(20,10))</span></span><br><span class="line">    ax.bar(range(calculate_distance(<span class="number">0</span>,<span class="number">1</span>).shape[<span class="number">0</span>]),calculate_distance(<span class="number">0</span>,<span class="number">1</span>),<span class="attr">color='g')</span></span><br><span class="line">    ax.set_title('Minimum Distance of '+namelist[ind1]+<span class="string">" and "</span>+namelist[ind2])</span><br><span class="line">draw_dist_count(<span class="number">0</span>,<span class="number">1</span>)</span><br></pre></td></tr></table></figure></p><p>It is the minimum distance of two words: <strong>picture and game</strong>, we can see at the first half of the book, the two words have closer distance, which means they have higher chance appearing together or nearby. But in the last half, the author didn’t arrange them appearing together very close.</p><p><img src="http://i4.fuimg.com/640680/dd3d1fca192d56eb.png" alt="Markdown"></p><p>We can also see one word’s distance with many other words: <strong>game and other words</strong><br><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">fig,ax=plt.subplots(<span class="number">4</span>,<span class="number">2</span>,figsize=(<span class="number">20</span>,<span class="number">20</span>))</span><br><span class="line"><span class="keyword">for</span> <span class="selector-tag">i</span> <span class="keyword">in</span> range(<span class="number">4</span>):</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">2</span>):</span><br><span class="line">        ax[<span class="selector-tag">i</span>,j].bar(range(calculate_distance(<span class="number">2</span>*i+j,<span class="number">8</span>)<span class="selector-class">.shape</span>[<span class="number">0</span>]),calculate_distance(<span class="number">2</span>*i+j,<span class="number">8</span>))</span><br><span class="line">        ax[<span class="selector-tag">i</span>,j].set_title(<span class="string">'Minimum Distance of '</span>+namelist[<span class="number">9</span>]+<span class="string">" and "</span>+namelist[<span class="number">2</span>*i+j])</span><br></pre></td></tr></table></figure></p><p><img src="http://i2.tiimg.com/640680/d444d7717b1a13d8.png" alt="Markdown"></p><p>As we can see, the word game’s appearance with other words have different patterns, for example, it is closer to word use than word picture.</p><p>We can also overview one word’s distance(relationship) with others using hist or boxplot<br><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">fig,ax=plt.subplots(<span class="number">4</span>,<span class="number">2</span>,figsize=(<span class="number">20</span>,<span class="number">20</span>))</span><br><span class="line"><span class="keyword">for</span> <span class="selector-tag">i</span> <span class="keyword">in</span> range(<span class="number">4</span>):</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">2</span>):</span><br><span class="line">        ax[<span class="selector-tag">i</span>,j].hist(calculate_distance(<span class="number">0</span>,<span class="number">1</span>+<span class="number">2</span>*i+j),bins =<span class="number">50</span>,<span class="attribute">color</span>=<span class="string">'b'</span>,alpha=<span class="number">0.4</span>)</span><br><span class="line">        ax[<span class="selector-tag">i</span>,j].set_title(<span class="string">'Minimum Distance of '</span>+namelist[<span class="number">0</span>]+<span class="string">" and "</span>+namelist[<span class="number">1</span>+<span class="number">2</span>*i+j])</span><br></pre></td></tr></table></figure></p><p><img src="http://i2.tiimg.com/640680/20c8d426b5181b6b.png" alt="Markdown"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">dist_data = &#123;&#125;</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> np.arange(<span class="number">1</span>,<span class="number">20</span>):</span><br><span class="line">    dist_data[i] = calculate_distance(<span class="number">0</span>,i)</span><br><span class="line">dataframe_dxp = pd.concat((pd.DataFrame(&#123;namelist[i]:dist_data[i]&#125;) <span class="keyword">for</span> i <span class="keyword">in</span> np.arange(<span class="number">1</span>,<span class="number">20</span>)),axis=<span class="number">1</span>)</span><br><span class="line"><span class="keyword">import</span> seaborn <span class="keyword">as</span> sns</span><br><span class="line">fig, ax = plt.subplots(figsize=(<span class="number">100</span>,<span class="number">20</span>))</span><br><span class="line">sns.boxplot(data =dataframe_dxp,ax=ax,boxprops=dict(alpha=<span class="number">.5</span>),color=<span class="string">'g'</span>)</span><br><span class="line">ax.set_title(<span class="string">u'Proposition and others'</span>,fontsize=<span class="number">120</span>)</span><br><span class="line">ax.set_xticks(range(<span class="number">19</span>))</span><br><span class="line">ax.set_xticklabels(namelist[<span class="number">1</span>:<span class="number">20</span>],fontsize=<span class="number">80</span>)</span><br><span class="line">plt.setp(ax.get_xticklabels(), rotation=<span class="number">30</span>, horizontalalignment=<span class="string">'right'</span>)</span><br><span class="line">fig.tight_layout()</span><br></pre></td></tr></table></figure><p><img src="http://i2.tiimg.com/640680/93eed5dd19fdd212.png" alt="Markdown"></p><p>That’s all about my basic data mining and analysis of Philosophical Investigations. It took me a some time writing codes to count and plot. I believe in the future we can use data mining, natural language processing to do analysis more automatically. What’s more, it may do some really exciting and serious study instead of my basic plotting. We may use the powerful so called “artificial intelligence” tool to mine thousands of books and materials to analyze the hidden ideas which is omitted by human due to our limitation of memorizing. Thus we can understand more about the author and the ideas behind the book. For some book as “random and complex” as this one, it may be a good tool to help us understand more by using statistics and visualization.</p>]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;AI’s-view&quot;&gt;&lt;a href=&quot;#AI’s-view&quot; class=&quot;headerlink&quot; title=&quot;AI’s view:&quot;&gt;&lt;/a&gt;AI’s view:&lt;/h3&gt;&lt;h4 id=&quot;what-is-AI&quot;&gt;&lt;a href=&quot;#what-is-AI&quot; class=&quot;headerlink&quot; title=&quot;what is AI&quot;&gt;&lt;/a&gt;what is AI&lt;/h4&gt;&lt;p&gt;Artificial intelligence means using program to accomplish tasks human can do by simulating human’s brain. The purpose of artificial intelligence is to bypass the brain and body to achieve a full understanding of human intelligence. The idea can be traced back to &lt;strong&gt;Turing machine&lt;/strong&gt; (we can already see some interesting coincidence here~ Turing has attended Wittgenstein’s class in Cambridge, though Turing holds different opinions, Wittgenstein values his ideas very much. Maybe Turing also get some inspiration during the idea conflict.)&lt;/p&gt;
    
    </summary>
    
      <category term="thoughts" scheme="https://www.cmwonderland.com/categories/thoughts/"/>
    
    
      <category term="philosophy" scheme="https://www.cmwonderland.com/tags/philosophy/"/>
    
      <category term="Wittgenstein" scheme="https://www.cmwonderland.com/tags/Wittgenstein/"/>
    
      <category term="thoughts" scheme="https://www.cmwonderland.com/tags/thoughts/"/>
    
  </entry>
  
  <entry>
    <title>Linear Regression Assignment 13</title>
    <link href="https://www.cmwonderland.com/2018/05/28/line7/"/>
    <id>https://www.cmwonderland.com/2018/05/28/line7/</id>
    <published>2018-05-28T20:01:19.000Z</published>
    <updated>2018-05-28T08:45:29.327Z</updated>
    
    <content type="html"><![CDATA[<p>The thirteenth assignment of Linear Regression. The assignment is written in Rmarkdown, a smart syntax supported by RStudio helping with formula, plot visualization and plugin codes running.</p><a id="more"></a><blockquote><p><strong>most recommend:</strong> <a href="https://www.cmwonderland.com/linearweek13.html"><strong>click here</strong></a> for <strong>html version</strong> of assignment, you can see codes as well as plots.</p></blockquote><p>You may also find the <a href="https://github.com/james20141606/somethingmore/tree/master/linear_regression_pdf" target="_blank" rel="noopener"><strong>PDF Version</strong></a> of this assignment from github. <strong>Or if you can cross the fire wall, just see below</strong>:<br><div class="row"><iframe src="https://drive.google.com/file/d/1WibEAysXOszktnD3d1BK8bDNcS-vN6wN/preview" style="width:100%; height:550px"></iframe></div></p><h1 id="1"><a href="#1" class="headerlink" title="1"></a>1</h1><h2 id="a"><a href="#a" class="headerlink" title="a"></a>a</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">dat&lt;-read.table(&quot;CH19PR10.txt&quot;)</span><br><span class="line">names(dat)&lt;-c(&apos;y&apos;,&apos;age&apos;,&apos;gender&apos;)</span><br><span class="line">dat$age&lt;-factor(dat$age)</span><br><span class="line">dat$gender&lt;-factor(dat$gender)</span><br><span class="line">fit&lt;-aov(data=dat,y~age*gender)</span><br><span class="line">summary(fit)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">library(&quot;ggpubr&quot;)</span><br><span class="line">ggboxplot(dat, x = &quot;age&quot;, y = &quot;y&quot;, color = &quot;gender&quot;,</span><br><span class="line">          palette = c(&quot;#00AFBB&quot;, &quot;#E7B800&quot;))</span><br></pre></td></tr></table></figure><h2 id="b"><a href="#b" class="headerlink" title="b"></a>b</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">for (i in c(1,2,3,4,5,6))&#123;</span><br><span class="line">  print (sum(fit$residuals[(6*(i-1)+1):(6*i)]))</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>Yes, they sum to zero for each treatment.</p><h2 id="c"><a href="#c" class="headerlink" title="c"></a>c</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">par(mfrow=c(1,2))</span><br><span class="line">stripchart(split(resid(fit), dat$gender), method = &quot;stack&quot;,  pch = 19)</span><br><span class="line">abline(h = seq(2, 4)-0.1)</span><br><span class="line">title(&quot;Aligned Residual Dot Plot gender&quot;)</span><br><span class="line">stripchart(split(resid(fit), dat$age), method = &quot;stack&quot;,  pch = 19)</span><br><span class="line">abline(h = seq(2, 4)-0.1)</span><br><span class="line">title(&quot;Aligned Residual Dot Plot age&quot;)</span><br></pre></td></tr></table></figure><h2 id="d"><a href="#d" class="headerlink" title="d"></a>d</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">rq&lt;-c()</span><br><span class="line">for (i in c(1:36)) &#123;</span><br><span class="line">  qq&lt;-qnorm((i-3/8)/(36+1/4))</span><br><span class="line">  rq&lt;-c(rq,qq)</span><br><span class="line">&#125;</span><br><span class="line">plot(rq,sort(fit$residuals))</span><br><span class="line">abline(0,1)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cor(rq,sort(fit$residuals))**2</span><br></pre></td></tr></table></figure><p>when n = 36 and significance value = 0.05, the Critical Values for Coefficient of Correlation between Ordered Residuals and Expected Values under Normality is 0.97. And the correlation calculated is 0.9720399. So it appears reasonable.</p><h2 id="e"><a href="#e" class="headerlink" title="e"></a>e</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">par(mfrow=c(1,2))</span><br><span class="line">arrofres = matrix(nrow = 6,ncol = 6)</span><br><span class="line">for (i in c(1,2,3,4,5,6))&#123;</span><br><span class="line">  arrofres[i,]&lt;-fit$residuals[(6*(i-1)+1):(6*i)]</span><br><span class="line">&#125;</span><br><span class="line">matplot(arrofres)</span><br><span class="line">plot(fit$residuals,type = &apos;b&apos;)</span><br></pre></td></tr></table></figure><p>residuals in each treatment’s sum is equal to zero, and it seems that the residuals has no relation with treatments.</p><h1 id="2"><a href="#2" class="headerlink" title="2"></a>2</h1><h2 id="a-1"><a href="#a-1" class="headerlink" title="a"></a>a</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">interaction.plot(dat$age,dat$gender,dat$y,type=&quot;b&quot;,col=c(&quot;red&quot;,&quot;blue&quot;),pch=c(16,18))</span><br></pre></td></tr></table></figure><p>age has larger effect and gender has small effect, since they are nearly parrallel, they have little interaction.</p><h2 id="b-1"><a href="#b-1" class="headerlink" title="b"></a>b</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">fit&lt;-aov(data=dat,y~age*gender)</span><br><span class="line">anova(fit)</span><br></pre></td></tr></table></figure><p>age, it has the largest SSR.</p><h2 id="c-1"><a href="#c-1" class="headerlink" title="c"></a>c</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">fit&lt;-aov(data=dat,y~age*gender)</span><br><span class="line">anova(fit)</span><br><span class="line">qf(0.95,2,30)</span><br><span class="line">1-pf(1.0581,2,30)</span><br></pre></td></tr></table></figure><ul><li>Hypothesis: $H<em>0 : \sigma^2</em>{\alpha\beta}&gt;0, i =1,2,3,j=1,2 $</li><li>Decision rule: Reject $H<em>0$ if $F* &gt;F</em>{0.95,2,30}$</li><li>Conclusion: Since $F∗ = \frac{MSAB}{MSE} = 1.0581$ , we do not reject $H_0$ and conclude that there<br>are no interaction effects with p = 0.3597133</li></ul><h2 id="d-1"><a href="#d-1" class="headerlink" title="d"></a>d</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">qf(0.95,2,2)</span><br><span class="line">qf(0.95,1,2)</span><br><span class="line">1-pf(66.2907,2,2)</span><br><span class="line">1-pf(2.2791,1,2)</span><br></pre></td></tr></table></figure><h3 id="i-Factor-A-age-main-effect"><a href="#i-Factor-A-age-main-effect" class="headerlink" title="i. Factor A (age) main effect"></a>i. Factor A (age) main effect</h3><ul><li>Hypothesis: $H_0 : \alpha_i = 0 vs H_a$ : at least one $\alpha_i$ is not 0, i = 1, 2, 3</li><li>Decision rule: Reject $H<em>0$ if $F∗ = MSA/MSE &gt; F</em>{0.95,2,2} = 19$</li><li>Conclusion: Since F∗ = 158.361/2.528 =62.6428 , we reject $H_0$ and conclude that there is<br>Factor A main effect for the number of coats with p =0.01486089</li></ul><h3 id="ii-Factor-B-gender-main-effect"><a href="#ii-Factor-B-gender-main-effect" class="headerlink" title="ii. Factor B (gender) main effect"></a>ii. Factor B (gender) main effect</h3><ul><li>Hypothesis: $H_0 : \beta_i = 0 vs H_a$ : at least one $\beta_i$ is not 0, i = 1, 2</li><li>Decision rule: Reject $H<em>0$ if $F∗ = MSB/MSE &gt; F</em>{0.95,1,2} = 18.51282$</li><li>Conclusion: Since F∗ = 2.2791 , we do not reject $H_0$ and conclude that there is no<br>Factor B main effect for the number of coats with p =0.2701973</li></ul><h2 id="f"><a href="#f" class="headerlink" title="f"></a>f</h2><p>Yes, age has large effect and gender has little effect, and there are no apparent interactions</p><h2 id="g"><a href="#g" class="headerlink" title="g"></a>g</h2><ul><li>single factor: age 316.7, Residuals   82.2 </li><li>two factor: age 316.7, gender    5.44, age:gender   5.06, Residuals   71.67<br>factor A age has same sum of squares, and residuals’ sum of squares in single factor ANOVA equals to all other sum of squares in two factor ANOVA except age. Yes, the degree holds the same relation.</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;The thirteenth assignment of Linear Regression. The assignment is written in Rmarkdown, a smart syntax supported by RStudio helping with formula, plot visualization and plugin codes running.&lt;/p&gt;
    
    </summary>
    
      <category term="school work" scheme="https://www.cmwonderland.com/categories/school-work/"/>
    
    
      <category term="codes" scheme="https://www.cmwonderland.com/tags/codes/"/>
    
      <category term="R" scheme="https://www.cmwonderland.com/tags/R/"/>
    
      <category term="assignment" scheme="https://www.cmwonderland.com/tags/assignment/"/>
    
      <category term="statistics" scheme="https://www.cmwonderland.com/tags/statistics/"/>
    
      <category term="linear regression" scheme="https://www.cmwonderland.com/tags/linear-regression/"/>
    
  </entry>
  
  <entry>
    <title>系统临界态对智能的启发</title>
    <link href="https://www.cmwonderland.com/2018/05/23/aboutbrain&amp;dl/"/>
    <id>https://www.cmwonderland.com/2018/05/23/aboutbrain&amp;dl/</id>
    <published>2018-05-23T20:49:39.000Z</published>
    <updated>2018-05-23T11:25:31.484Z</updated>
    
    <content type="html"><![CDATA[<p>本文写于2016年末，作为《脑科学与人工智能》一课的作业</p><h3 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h3><p>人工智能领域自惊艳出世以来已有几十年的历史，经过长久的蛰伏和起伏，在这两年徐徐展开渐至高潮的宏伟篇章。对人工智能的探讨是一个复杂的深奥的话题，我想选择人工智能与生物接壤的一个小角度，谈谈我对智能的理解。</p><a id="more"></a><p>脑科学与人工智能有着天然的交叉，虽然很多做技术的人并不懂得古老又崭新的神经学知识。我们的大脑确实格外神秘，我们的大脑有上千亿的神经元，还要再多上几个数量级的突触。这个数量虽然远远不如广袤的宇宙中行星的个数，但也足够赋予渺小的人类畅想的资质和勇气。思考本身就是个值得思考的有趣的话题，然而这类话题往往陷入人择原理的尴尬处境。人脑是如此复杂精细的结构，以至于我们用还原论的方法研究至今还进展尚浅，甚至也弄不明白人工智能和大脑结构究竟哪个能更快地弄明白原理。因此我想稍微降低一些难度，也是受到宋老师的启发，从智能的角度思考，试图像数学家和物理学家那样，思考得更基本一些。</p><h3 id="简单与复杂"><a href="#简单与复杂" class="headerlink" title="简单与复杂"></a>简单与复杂</h3><p>智能、生命的定义是什么，是一个不止由生物学家专有的宏大的问题。薛定谔就曾经写过令生物学家折服的小册子《生命是什么》，其负熵的概念颇有新意，只是那个时候意识和智能并没有被摆到很高的位置，而如今身体其他部位问题的解惑愈发加重了人们对于智能和意识的兴趣，我甚至觉得意识和智能就是万物之源，自然真理乃至永生之匙。</p><p>一门自称为科学的学科总是试图用尽可能少的通行的原理构建体系。物理学家苦苦寻觅统一基本作用力的终极理论，生物学家却不得不一开始就面对真实世界的复杂的模型。Paul Nurse曾经评论道：要真正理解细胞系统的复杂网络，可能不得不进入一个更抽象的陌生领域，这个领域更适于进行数学分析。</p><p>自从洛伦兹在分析天气问题时搞出来一套混沌科学的想法，这套神奇又诱人的理论就格外深入人心，人们可以把简单至三体问题复杂至生物系统、经济社会等各种问题代入其中并且摊手摇头。过去我们习惯于研究生物化学中的平衡状态，但是平衡态意味着寂静和死亡，生物系统中存在着大量的非平衡态，正是在非平衡态中得以产生秩序与结构。</p><p>大脑是无可置疑的复杂系统，神经元的立体连接，不同化学物质的复杂作用让整个系统复杂得让人心碎。随着连接组学等学科的深入发展，有更多的理论工作者开始努力阐释系统的原理。我们希望系统是尽可能简洁的，纵然结果并不简洁，但是希望源头是简洁的。就像四种基本作用力和精确的宇宙常数带来的宏大的宇宙，机械决定论畅想过拉普拉斯妖这样的预言家。现在我们知道仅从物理角度是不够拼凑出真理的全部的，那么我们还需要多少尽量简单的东西呢？</p><p>简单可以产生让人炫目的复杂，比如冯诺依曼提出的元胞自动机。就是一种简单的对真实世界的模型化建构。通过简单的非线性函数的耦合，元胞自动机可以产生极端复杂的结果。比如规则为184的自动机，可以描述交通流、模拟晶体生长。规则30可以产生混沌，用来做伪随机数发生器。最强大的110，被证明具有图灵完备性，具有通用的计算能力。110也许是目前最简单的通用计算机了，可以计算自然界能够计算的复杂性的上限。Wolfram大胆地提出了新的自然定律，指出自然系统不可能产生不可计算的行为。自然界中各种过程实现的计算在复杂程度上都是本质等价的，没有比通用计算机所能进行的计算更复杂的了。</p><p>元胞自动机的规则如此简单，却能产生如此复杂的强大的结果让人振奋。如果，人脑也是这么简单就好了，或者，稍微复杂一些，但是逃不过一些简单的规则的组合，只是这些规则的非线性乃至量子性像一团薄雾蒙住了我们的眼睛，这样的迷雾我们是不怕的，人类对简洁的天然的追求总会为我们拨云见日，揭示复杂背后的简单。</p><h3 id="从简单到复杂：描述智能的临界态"><a href="#从简单到复杂：描述智能的临界态" class="headerlink" title="从简单到复杂：描述智能的临界态"></a>从简单到复杂：描述智能的临界态</h3><p>人工智能已经在很多领域取得了成就，语音、视觉、自动驾驶、语言、博弈等等领域。</p><p>但是遗憾的是每个机器学习的系统只能解决特定问题，并不像人类大脑经过简单学习学会新的东西，通用的智能似乎是计算机很难实现的。</p><p>通用的智能是否有方法进行描述？人工智能解决某个具体问题，我们总能找到一个标准去判断其好坏，比如下围棋，视觉、语音识别准确率。那么通用性的评价标准是什么呢？</p><p>在《临界：智能设计的原则》一书中有这样有趣的观点：</p><blockquote><p>把智能/人工智能定义为在任意环境中找到最优解的能力。</p></blockquote><p>这种神奇的能力，在我看来受到一些更基本的原理的影响：比如对称性，最小作用量原理和熵。比如很简单的例子：一个粒子在盒子里自由运动，一旦撞到壁游戏就结束，那么最安全的，选择最多的位置就是正中央；当单摆恰好直立时，有两种不同的摆动方向，这位它自身提供了最大的未来的可能性，虽然它本身会处于一种不稳定平衡的状态。而当系统变得复杂的时候，这种不稳定的最大化可能就很容易维持了。</p><p>比如在鸟群或者昆虫群的集体运动中，要让未来有更多的可能性，鸟群的可能性包括更多的食物和对捕食者的应对，当未来有无穷的可能性时它们仍然有办法做出响应。一个天然的想法是无限可能性=每个粒子完全随机，其实这个想法是不对的，完全随机意味着可能性的急剧降低，系统的统计学指标会趋于完全确定，而完全的有序当然也不是可能性最多的选择。只有在处于某种临界态上，群体可以维持完整的同时每个粒子个体可以感受到未来刺激，并且在整个群体中产生长程的效应，微小的扰动就可以在群体中产生剧烈的响应。这样的性质是临界态的重要特点，这样的临界态才是对应于未来可能性最大的状态，也就是智能的状态。这个例子很好理解，看一下鸟群和海洋中的鱼群是如何在空间中做出令人炫目的群体运动就可以有最直观的感受了。完全的无序和完全的有序都不是最大化未来可能的选择，真正的选择应该是自组织临界态。</p><p>未来选择最多的状态应该是处于对称状态，可以发生对称性破缺；从动力学角度看，应该停留在分叉点或者中间转变态的点。某个对称性消失，系统就可能产生转变。在临界态附近系统会表现出既不完全随机也不完全有序的状态。两个晶体的温度差距可以非常小，但是图案样式可以有非常大的变化。临界态的图像的构象上有自相似的特性，图案上有分形的结构，这是临界现象的特点。</p><h3 id="沙堆模型与森林大火"><a href="#沙堆模型与森林大火" class="headerlink" title="沙堆模型与森林大火"></a>沙堆模型与森林大火</h3><p>桌子上放着一些沙子，沙子还比较少，不断增加沙子之后沙堆的斜坡的斜率会增加，但是不会无限增加，我们从来没有见过九十度的沙堆。当沙堆的斜坡斜率小于某个值，不断加沙子就会增加斜率，而达到某个值之后，不管怎么加沙子，尽管斜率会发生一些微小的变化，但是会维持在一个临界的斜率，它不需要我们用某种智能的方法维持就会自发维持，这就是沙堆模型，这就是能让系统自动保持在临界态附近的最简单模型。</p><p>我对沙堆模型感兴趣的原因之一是我的实验室老板(2016年)汤超教授也是该模型的发现者之一。这个模型有非常非常深刻又广泛的内涵。当我们去研究简单的东西（比如沙堆）时却能发现深刻的规律的时候，我们就会不禁去想，这背后应该有什么非常有趣的规律。</p><p>沙堆模型是非平衡物理中非常简单又深刻的模型，沙堆不断流入的沙粒可以看做系统对外界能量和信息的吸收，从而使自己维持在自组织临界的状态，这个模型是具有普适的意义的。前面从简单到复杂的模型已经让我们意识到，自组织临界的维持是需要条件的，沙堆模型向我们很好的演示了这一点，而这样的思想与生命拥有的智能不谋而合：我们正是依靠摄入能量和信息来抵御熵增，避免生命走向彻底的无序。</p><p>一粒沙子也可以改变整个系统的临界态。有可能造成巨大的崩塌的，也有可能没有什么影响。接下来再放的沙子又有可能恢复系统的临界态，也就是说沙子会造成系统的崩塌，但是系统的临界态还可以保持在很小的范围内。如果沙堆还没有到达临界态上，放上一粒沙子我们就可以很确定地说沙堆不会发生崩塌，而达到临界态之后沙堆有可能发生任意规模的崩塌，这也就是我们前面说的最大未来可能性。至于发生多大规模的崩塌，几位专家很容易地统计出来，沙堆崩塌规模的大小与概率符合幂律分布。</p><p>如果在临界状态下，外界不做干预的情况下沙堆的未来是无法预知的，而非临界态时的未来是可知的，这也是我们前面说的最大化未来可能。地质学家很快发现，地震震级与发生概率也符合幂律分布，地壳的活动可能也处于自组织临界的状态，这也是地震等灾害难以预测的原因。沙堆模型也是一种特殊的元胞自动机模型，有很多相关的研究。</p><p>不但许多自然现象里蕴含着沙堆模型，更贴近生命与智能的例子里也蕴含着这样的思想。鱼群的游动受到的扰动与一条鱼的速度分布符合幂律分布，人群的合作也可以与沙堆模型类比。一个沙子扰动附近的沙子的分叉行为就符合分形产生的规则，如果有初始状态的确定，这个结构还可以不断增长，比如生物发育中三个胚层的发育。</p><p><img src="http://i1.fuimg.com/640680/e707a6c6f4ea10a4.png" alt="Markdown"></p><p>生物/智能“自发选择”的符合自组织临界的特性让人欣喜，我们可以考虑把研究生物智能问题和研究系统自组织问题融入一体，从简单的动力学和统计学角度去思考智能的特性，这无疑是很好的结果了。</p><h3 id="森林大火与大脑的临界态"><a href="#森林大火与大脑的临界态" class="headerlink" title="森林大火与大脑的临界态"></a>森林大火与大脑的临界态</h3><p>森林大火模型与沙堆模型以一些相似性。一棵树的燃烧可能造成周围格点，也就是其他树的燃烧，这与沙子掉落导致斜率超过临界值带来的崩塌是类似的。不同之处在于：沙堆可以继续放沙子，而森林大火刚刚燃烧过的地方是不能再燃烧的，这种本来就是自然的特性的东西显然与生物模型更符合！稍微考虑一下就会想到，这可以与生物的不应期对应，这在大脑的功能中是重要的。因此研究森林大火的统计性质，从某种程度上也是在研究大脑神经活动的统计性质。</p><p>何帆先生曾经写过一篇政治杂文，用森林大火的模型研究政治现象（可见这种朴素普适的想法是很容易被人接受的），他提出如果不断地掉落一些小的火种，让森林部分燃烧，这样森林虽然燃烧一部分，但是火灾规模都不大，如果平时不燃烧，那么系统有小概率产生大的火灾的时候就很糟糕了，这里面想讲的政治道理其实已经比较明显了。</p><p>森林可能不是一年四季都处于临界状态的，何帆先生先放一把火的做法就让森林不那么容易发生火灾了，这就是让系统偏离平衡态的方法。雨水很多的地方不易发生火灾，可以被称为亚临界状态，如果非常干燥极易发生火灾，可以被称为超临界状态。</p><p>超临界状态也是系统和生命常见的状态。如果崩塌中有正反馈，一个崩塌就越容易造成崩塌，就像马太效应一样。正反馈的直接效果会带来要不然不崩塌要不然剧烈崩塌的双稳态。<br>其实这个现象是非常容易发生的，在非理想情况的沙堆实验中，沙子落下时会带有一定冲击力，沙子掉下时就会造成本来不至于崩塌的地方也崩塌，也就是说沙堆会处于某种超临界的状态，这种正反馈的效果就是真实实验观察时发现了很多巨大的崩塌。</p><p>换句话说，在沙堆实验中只要给系统一些正反馈，就会使系统处于一种超临界的状态下，使系统要不不崩塌要不产生巨大崩塌，这种状态就是双稳态。因为沙堆的崩塌是连带性的，因此一个崩塌区域是连城一片的，所以真实的沙堆模型可能就没有抽象的理论中那么复杂的分形结构。</p><p>下面是针对大脑神经活动的思考。图灵曾经有过一段深受时代影响的对机器思想的论述。他从原子弹的原理受到启发，提出人类的大脑输入的想法就像反应堆外界轰击的中子，这些中子会造成我们大脑的反应，这些反应可能会逐步消失，但是过量的轰击甚至会导致反应堆解体。图灵进一步提出了不具有真正思想的动物的大脑可能的情况：动物的大脑处于亚临界状态，输入想法则回到静息状态，而人类的一部分思想处于超临界状态，给一个想法可以创造出无穷无尽的想法。图灵很自然地就提出，想让机器拥有智能，就要让机器也拥有超临界的状态。<br>然而图灵的想法有两个重大的失误，这种设想并不直接对应于神经的活动，超临界状态也并不是最好的选择。超临界状态只有两种结果：大的崩塌或者小的崩塌。如果大脑真的出现超临界状态，比如大脑大范围的神经放电，这种现象就对应于癫痫。（当然超临界状态也许会给人一些因祸得福的好处，比如近乎“疯狂”的创造力）到了八九十年代，提出沙堆模型的bak在一个神经科学会议上提出，人类的大脑应该是临界态的，虽然当时没有办法进行实验验证，但是如今我们已经有非常多的实验证明大脑确实处于临界状态。</p><p>从小尺度上，我们可以观察到与沙堆模型相似的神经雪崩，一个神经的兴奋造成周围神经的兴奋。大尺度上，类似于鸟群一样，大脑皮层在时间空间上产生长程关联，虽然是小概率但是可能发生在行为层次上，产生类似于临界慢化的现象，类似于疾病的潜伏期。</p><p>虽然实验说明在大脑一小块儿区域有临界态，切片与完整的大脑还是不一样的，如今通过对整个大脑的测量，比如功能核磁共振FMRI，来做大脑神经活动研究，如果要说明大脑整体处于临界状态，就要看大脑的神经关联是否足够长。FMRI实验证明了大脑皮层的神经活动存在长程关联，从而说明了大脑在整体上也处于临界态。如今已经有理论说明了大脑的临界性与大脑的网络结构有关。</p><p>有一些人因为看一些画面闪烁过快的电视节目发生癫痫：频率过高的输入信号就像沙堆模型中大规模的沙粒下落会导致大规模沙崩导致癫痫。而正常人在看到这些画面不会癫痫的保护机制包括输入信号频率增加后突触的活动会降低，就像发生小火变多之后森林大火的可能性减小，也就是说我们的大脑存在机制保持在临界态上不会产生崩溃。这个例子也让人联想到最近一项治疗阿尔兹海默症的突破性研究，在大量的药物治疗尝试后，科学家试着用一定频率的闪光治疗阿尔兹海默症并且取得了奇效。这让人浮想联翩，除了对某些化学物质的刺激，一定频率（40Hz）的光还对大脑产生了什么影响。现在人们已经知道大脑的临界态对大脑的意义包括信息传输、存储、计算能力的提升。</p><h3 id="大脑中的幂律分布"><a href="#大脑中的幂律分布" class="headerlink" title="大脑中的幂律分布"></a>大脑中的幂律分布</h3><p>大脑中也是存在幂律分布的，新近的研究更是进一步阐释了这一点。该研究的其中一个实验是以蒙着眼睛的小鼠作为实验对象，实验结果表明在生物大脑中可能存在一种通用的计算原理。在小鼠大脑发育的关键阶段，蒙着眼睛的小鼠的大脑中与视觉相关的大脑区域会被重新分配其它心智任务。这似乎能够证明人脑和老鼠的大脑相似，都具有可塑性，可以以一种通用计算机器（universal computing machine）的方式进行重新编程。</p><p>钱卓教授花费了十多年的时间研究连接理论（Theory of Connectivity）中，试图发现人脑灰质（grey matter）中存在的一种占据主导地位的统一计算原理</p><p>很多人都一直在思考，在智能的起源和大脑的进化上一定有一个基本的设计原理，就像是 DNA 的双螺旋结构，每一个生物都有普遍存在的遗传密码。钱卓给出了证据说明人脑可能是按照一种极其简单的数理逻辑来运作的。连接理论（Theory of Connectivity）认为一个叫做基于二次幂的置换（power-of-two-based permutation）的简单算法可以被用来解释大脑的回路，其可表示为$N = 2^i -1$</p><p>研究观察说明大脑的基本计算算法确实是通过基于二次幂的置换逻辑进行组织的。这个简单的数学逻辑可以用来解释整个进化谱（evolutionary spectrum）中的大脑计算，范围涵盖最简单的神经网络到最复杂的神经网络。这种工作是令人震撼和激动的。想一想，一项结合了神经科学，自然世界与人工神经网络的研究，把一种普适的规律就这样发现了：自然界的幂律分布，大脑的简洁又普适的运算机制，又能给深度学习很多启发，这样的发现是对神经科学的重大的冲击，对人工智能的极大地促进，对自己的更加深刻的认识。这个公式的简洁让人舒适又开心，自然界就应该是这样的：复杂又简单，简单源自于思想与原理的统一，只要原理是统一的，我们就只需要处理数学和具体条件的复杂，这样的复杂性就可以被聪明的人类掌握于手中。这就是自然、科学、数理与思想的奇妙和联系！</p><p>进一步把临界态的特征适用于大脑中，可以看到大脑的一些特点，比如长程性：大脑存在着长程的时间与空间相关，让大脑有了更高的整合特性，这也是大家关注的大脑与AI的重要区别。比如均衡性，保持整体与保持敏感的平衡，而且在做出反应后依然可以保持敏感，而不是接受一波信息之后就放松了。这种状态是稳定性与随机性的最佳的平衡。人的大脑还在有序无序之间表达出很强的适应性，一个很随机的系统很灵活但是不稳定，比如今天做这个明天做这个但是记不住，没法应用和进步；一个很稳定的大脑可以记住很多东西但是并不能互相联系（比如AI），因此一个适应性很强的大脑应该在稳定性与随机性之间。</p><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>我相信世界应该是简洁的，我们总是希望用各种方法逼近一些尽可能简洁美好的基本真理。现在工业界的深度学习进展迅速，这是对人类发展有益的好事，但是对于科学界来说，更关键的不是盲目的冒进，而是静下心来发现原理性的东西，计算科学家希望从数学和计算的角度研究，神经科学家希望从大脑结构和功能的角度研究，我想我们应该加入更多的系统生物学和物理学、统计学的想法，把所有的这些东西都融合起来，追本溯源，也许就可拨云见日，目见真理。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文写于2016年末，作为《脑科学与人工智能》一课的作业&lt;/p&gt;
&lt;h3 id=&quot;引言&quot;&gt;&lt;a href=&quot;#引言&quot; class=&quot;headerlink&quot; title=&quot;引言&quot;&gt;&lt;/a&gt;引言&lt;/h3&gt;&lt;p&gt;人工智能领域自惊艳出世以来已有几十年的历史，经过长久的蛰伏和起伏，在这两年徐徐展开渐至高潮的宏伟篇章。对人工智能的探讨是一个复杂的深奥的话题，我想选择人工智能与生物接壤的一个小角度，谈谈我对智能的理解。&lt;/p&gt;
    
    </summary>
    
      <category term="thoughts" scheme="https://www.cmwonderland.com/categories/thoughts/"/>
    
    
      <category term="thoughts" scheme="https://www.cmwonderland.com/tags/thoughts/"/>
    
      <category term="deep learning" scheme="https://www.cmwonderland.com/tags/deep-learning/"/>
    
      <category term="brain" scheme="https://www.cmwonderland.com/tags/brain/"/>
    
  </entry>
  
  <entry>
    <title>Multivariate Statistics Assignment 5</title>
    <link href="https://www.cmwonderland.com/2018/05/23/multi2/"/>
    <id>https://www.cmwonderland.com/2018/05/23/multi2/</id>
    <published>2018-05-23T16:59:19.000Z</published>
    <updated>2018-05-23T07:19:15.683Z</updated>
    
    <content type="html"><![CDATA[<p>The fifth assignment of Multivariate Statistics. The assignment is written in Rmarkdown, a smart syntax supported by RStudio helping with formula, plot visualization and plugin codes running.</p><a id="more"></a><blockquote><p><strong>most recommend:</strong> <a href="https://www.cmwonderland.com/multihw5.html"><strong>click here</strong></a> for <strong>html version</strong> of assignment, you can see codes as well as plots.</p></blockquote><p>You may also find the <a href="https://github.com/james20141606/somethingmore/tree/master/multi-variable_pdf" target="_blank" rel="noopener"><strong>PDF Version</strong></a> of this assignment from github. <strong>Or if you can cross the fire wall, just see below</strong>:<br><div class="row"><iframe src="https://drive.google.com/file/d/1AXRjDQXwYn_LiF3O0zzfgwiWLfeiThHp/preview" style="width:100%; height:550px"></iframe></div></p><h1 id="1"><a href="#1" class="headerlink" title="1"></a>1</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">mp &lt;- matrix(c(5,2,2,2),2,2,byrow=T) </span><br><span class="line">eigen(mp)</span><br></pre></td></tr></table></figure><script type="math/tex; mode=display">\text{The eigenvalue-eigenvector pairs are } \lambda_1 =6, e_1 =  [\frac{2}{\sqrt5},\frac{1}{\sqrt5}]; \lambda_2 =1, e_2 =  [-\frac{1}{\sqrt5},\frac{2}{\sqrt5}].\\\text{Therefore, the principle componenets become: }\\Y_1 = e_1^T X = \frac{2}{\sqrt5}X_1 +\frac{1}{\sqrt5}X_2\\Y_1 = e_2^T X = -\frac{1}{\sqrt5}X_1 +\frac{2}{\sqrt5}X_2\\\text{The total population variance explained by first principal component is:}\\\frac{var(Y_1)}{var(Y_1)+var(Y_2)} = \frac{\lambda_1}{\lambda_1+\lambda_2} = \frac{6}{1+6} \approx 85.71\%</script><h1 id="2"><a href="#2" class="headerlink" title="2"></a>2</h1><h2 id="a"><a href="#a" class="headerlink" title="a"></a>a</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cov2cor(mp)</span><br></pre></td></tr></table></figure><script type="math/tex; mode=display">\text{The correlation matrix } \rho = \left[\begin{matrix}1 & \sqrt\frac{2}{5} \\\sqrt\frac{2}{5}&1\end{matrix}\right]\\\text{The eigenvalue-eigenvector pairs are }\\\lambda_1= \frac{5+\sqrt {10}}{5}, \ e_1=\left[\begin{matrix}\frac{1}{\sqrt 2} \\\frac{1}{\sqrt 2}\end{matrix}\right]\\\lambda_1= \frac{5-\sqrt {10}}{5}, \ e_1=\left[\begin{matrix}-\frac{1}{\sqrt 2} \\\frac{1}{\sqrt 2}\end{matrix}\right]</script><script type="math/tex; mode=display">Let \ \mathbf{Z_i} = \frac{\mathbf{X_i}-\mu_i}{\sqrt {\sigma_{ii}}}, i =1,...,p. \text{The principal components become:}\\Y_1 = e_1^T Z = \frac{1}{\sqrt2}Z_1 +\frac{1}{\sqrt2}Z_2\\Y_1 = e_2^T Z = -\frac{1}{\sqrt2}Z_1 +\frac{1}{\sqrt2}Z_2\\\text{The total population variance explained by first principal component is:}\\\frac{var(Y_1)}{var(Y_1)+var(Y_2)} = \frac{\lambda_1}{\lambda_1+\lambda_2} = \frac{5+\sqrt{10}}{10} \approx 81.6\%</script><h2 id="b"><a href="#b" class="headerlink" title="b"></a>b</h2><p>The principal components of Z obtained from the eigenvectors of the correlation<br>matrix ρ of X is different from those calculated from covariance matrix $\Sigma$. Because<br>the eigen pairs derived from $\Sigma$, in general not the same as the ones derived from $\rho$</p><h2 id="c"><a href="#c" class="headerlink" title="c"></a>c</h2><p>THe correlations between $Y_j$ and $Z_i$ are:</p><script type="math/tex; mode=display">\rho_{Y_1,Z_1} = e_{11}\sqrt {\lambda_1} = \frac{1}{\sqrt2}\sqrt \frac{5+\sqrt {10}}{5} \approx0.903 \\\rho_{Y_1,Z_2} = e_{12}\sqrt {\lambda_1} = \frac{1}{\sqrt2}\sqrt \frac{5+\sqrt {10}}{5} \approx0.903 \\\rho_{Y_2,Z_1} = e_{21}\sqrt {\lambda_2} = - \frac{1}{\sqrt2}\sqrt \frac{5-\sqrt {10}}{5} \approx -0.429</script><h1 id="3"><a href="#3" class="headerlink" title="3"></a>3</h1><h2 id="a-1"><a href="#a-1" class="headerlink" title="a"></a>a</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"># read the data</span><br><span class="line">setwd(&apos;~/Desktop/三春/3多元统计分析/作业/作业4/&apos;)</span><br><span class="line">dat&lt;-read.csv(&quot;table8.4.csv&quot;)</span><br><span class="line">X1&lt;-dat$x1</span><br><span class="line">X2&lt;-dat$x2</span><br><span class="line">X3&lt;-dat$x3</span><br><span class="line">X4&lt;-dat$x4</span><br><span class="line">X5&lt;-dat$x5</span><br><span class="line">covar &lt;- cov(dat)</span><br><span class="line">covar</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">eigen(cov(dat))</span><br><span class="line">prcomp(dat)</span><br><span class="line">summary(prcomp(dat))</span><br></pre></td></tr></table></figure><script type="math/tex; mode=display">Y_1 = e_1^T X= -0.2228228 X_1  -0.3072900 X_2 -0.1548103 X_3 -0.6389680 X_4 -0.6509044 X_5\\Y_2 = e_2^T X = 0.6252260 X_1 + 0.5703900 X_2 +0.3445049 X_3 -0.2479475 X_4 -0.3218478 X_5\\Y_3 = e_3^T X= -0.32611218 X_1 +  0.24959014 X_2 +0.03763929 X_3 +0.64249741 X_4 -0.64586064 X_5\\Y_4 = e_4^T X=  0.6627590 X_1 -0.4140935X_2 -0.4970499 X_3 +0.3088689 X_4 -0.2163758 X_5\\Y_5 = e_5^T X=-0.11765952 X_1 +0.58860803 X_2 -0.78030428 X_3 -0.14845546 X_4 +0.09371777 X_5</script><h2 id="b-1"><a href="#b-1" class="headerlink" title="b"></a>b</h2><p>From the summary above, the proportion of the total sample variance explained by the rst<br>three principal components is: 89.881%. It means that the first three explain almost all<br>variance.</p><h2 id="c-1"><a href="#c-1" class="headerlink" title="c"></a>c</h2><p>From 8-33, we have the CI of m $\lambda_i$:</p><script type="math/tex; mode=display">[\frac{\hat \lambda_i}{1+z(\alpha/2m)\sqrt{2/n}},\frac{\hat \lambda_i}{1-z(\alpha/2m)\sqrt{2/n}}]</script><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">z &lt;-qnorm(1-1/6)</span><br><span class="line">cical &lt;-function(lambda)&#123;</span><br><span class="line">  c(lambda/(1+z*(1/103)**0.5),lambda/(1-z*(1/103)**0.5))</span><br><span class="line">&#125;</span><br><span class="line">cical(0.0013676780)</span><br><span class="line">cical(0.0007011596) </span><br><span class="line">cical(0.0002538024)</span><br></pre></td></tr></table></figure><p>CIs are: [0.001248653 0.001511786], [0.0006401396 0.0007750385], [0.0002317147 0.0002805447]</p><h2 id="d"><a href="#d" class="headerlink" title="d"></a>d</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">plot(c(0.52926, 0.80059, 0.89881, 0.95399, 1.00000),ylab=&quot;Cumulative proportion&quot;,xlab=&quot;Component number&quot;,type=&apos;b&apos;)</span><br></pre></td></tr></table></figure><p>From the cumulative proportion plot, it seems that three dimensions’ PC are enough.</p><h1 id="4"><a href="#4" class="headerlink" title="4"></a>4</h1><h2 id="a-2"><a href="#a-2" class="headerlink" title="a"></a>a</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">library(bootstrap)</span><br><span class="line">data(scor)</span><br><span class="line">plot(scor)</span><br></pre></td></tr></table></figure><h2 id="b-2"><a href="#b-2" class="headerlink" title="b"></a>b</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cor(scor)</span><br></pre></td></tr></table></figure><h2 id="c-2"><a href="#c-2" class="headerlink" title="c"></a>c</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">prcomp(scor)</span><br><span class="line">summary(prcomp(scor))</span><br></pre></td></tr></table></figure><script type="math/tex; mode=display">Y_1 = e_1^T X= -0.5054457 X_1  -0.3683486 X_2 -0.3456612 X_3 -0.4511226 X_4 -0.5346501 X_5\\Y_2 = e_2^T X = -0.74874751 X_1 -0.20740314 X_2 + 0.07590813 X_3 +0.30088849 X_4 +0.54778205 X_5\\Y_3 = e_3^T X= 0.2997888 X_1   -0.4155900 X_2 -0.1453182 X_3 -0.5966265 X_4 +0.6002758 X_5\\Y_4 = e_4^T X=  -0.296184264 X_1 + 0.78288817X_2 +0.003236339 X_3 -0.518139724 X_4 +0.175732020 X_5\\Y_5 = e_5^T X= -0.07939388 X_1 -0.18887639 X_2 +0.92392015 X_3 -0.28552169 X_4 -0.15123239 X_5</script><h2 id="d-1"><a href="#d-1" class="headerlink" title="d"></a>d</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">plot(c( 0.6191,0.8013 ,0.8948 ,0.97102, 1.00000),ylab=&quot;Cumulative proportion&quot;,xlab=&quot;Component number&quot;,type=&apos;b&apos;)</span><br></pre></td></tr></table></figure><p>I will choose the first too for these three PCs take almost 80% of total variance.</p><h2 id="e"><a href="#e" class="headerlink" title="e"></a>e</h2><p>PC1 may stand for the indicator of scores on all subjects. PC2 has more straightforward mearning: it is related to closed or open rules.</p><h2 id="f"><a href="#f" class="headerlink" title="f"></a>f</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">library(&apos;ggfortify&apos;)</span><br><span class="line">autoplot(prcomp(scor,scale=TRUE),colour=&apos;green&apos;,label=TRUE)</span><br></pre></td></tr></table></figure><h2 id="g"><a href="#g" class="headerlink" title="g"></a>g</h2><p>$\chi^2_2(0.05) = 5.99$<br>I use python to check the outlier:<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> sklearn.decomposition <span class="keyword">import</span> PCA</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">convert</span><span class="params">(strr)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> np.array(strr.split(<span class="string">' '</span>)).astype(<span class="string">'float'</span>).reshape(<span class="number">-1</span>,<span class="number">1</span>)</span><br><span class="line">pca = PCA(n_components=<span class="number">2</span>, svd_solver=<span class="string">'full'</span>)</span><br><span class="line">dat = pca.fit_transform(data)</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">ellipse</span><span class="params">(i)</span>:</span></span><br><span class="line">    x,y = dat[i,<span class="number">0</span>],dat[i,<span class="number">1</span>]</span><br><span class="line">    a =  (x/<span class="number">26.2105</span>)**<span class="number">2</span> + (y/<span class="number">14.2166</span>)**<span class="number">2</span></span><br><span class="line">    <span class="keyword">if</span> a &gt;=<span class="number">5.99</span>:</span><br><span class="line">        <span class="keyword">print</span> (i,a)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(data.shape[<span class="number">0</span>]):</span><br><span class="line">    ellipse(i)</span><br></pre></td></tr></table></figure></p><p>And we can find eight outliers: 1,2,23,28,66,76,81,87</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;The fifth assignment of Multivariate Statistics. The assignment is written in Rmarkdown, a smart syntax supported by RStudio helping with formula, plot visualization and plugin codes running.&lt;/p&gt;
    
    </summary>
    
      <category term="school work" scheme="https://www.cmwonderland.com/categories/school-work/"/>
    
    
      <category term="codes" scheme="https://www.cmwonderland.com/tags/codes/"/>
    
      <category term="R" scheme="https://www.cmwonderland.com/tags/R/"/>
    
      <category term="assignment" scheme="https://www.cmwonderland.com/tags/assignment/"/>
    
      <category term="statistics" scheme="https://www.cmwonderland.com/tags/statistics/"/>
    
      <category term="linear regression" scheme="https://www.cmwonderland.com/tags/linear-regression/"/>
    
  </entry>
  
  <entry>
    <title>MAC用automator创建小应用解决一些简单需求</title>
    <link href="https://www.cmwonderland.com/2018/05/22/customizeapp/"/>
    <id>https://www.cmwonderland.com/2018/05/22/customizeapp/</id>
    <published>2018-05-22T21:35:37.000Z</published>
    <updated>2018-05-22T10:07:02.670Z</updated>
    
    <content type="html"><![CDATA[<p><strong>本来是要解决mac一个很蠢的问题的：无法新建文件。</strong></p><p>之前的策略有两种：一是复制粘贴其他的文件过来。另一个是cd到当前目录下然后touch一个文件。这个有点麻烦，首先cd还得拖到终端一个文件才能找对位置。</p><p>这次用了Mac自带的automator自己做一个简单的生成新文件的应用，非常简单。</p><p>显然既然automator还可以用shell脚本，能低成本实现的东西还是很多的。</p><a id="more"></a><iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width="330" height="86" src="//music.163.com/outchain/player?type=2&id=21224493&auto=1&height=66"></iframe><h2 id="STEPS"><a href="#STEPS" class="headerlink" title="STEPS"></a>STEPS</h2><h3 id="open-automator"><a href="#open-automator" class="headerlink" title="open automator"></a>open automator</h3><h4 id="select-application"><a href="#select-application" class="headerlink" title="select application"></a>select application</h4><p><img src="http://i4.fuimg.com/640680/f7023d7d9b64c7a6.png" alt="Markdown"></p><h4 id="select-AppleScript-on-left-bar"><a href="#select-AppleScript-on-left-bar" class="headerlink" title="select AppleScript on left bar"></a>select AppleScript on left bar</h4><p><strong>WRITE</strong><br><figure class="highlight applescript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">on</span> <span class="built_in">run</span> &#123;input, parameters&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">tell</span> <span class="built_in">application</span> <span class="string">"Finder"</span></span><br><span class="line"><span class="keyword">set</span> selection <span class="keyword">to</span> make new <span class="built_in">file</span> <span class="keyword">at</span> (<span class="keyword">get</span> insertion location)</span><br><span class="line"><span class="keyword">end</span> <span class="keyword">tell</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">return</span> input</span><br><span class="line"><span class="keyword">end</span> <span class="built_in">run</span></span><br></pre></td></tr></table></figure></p><h3 id="save-as-file-app-in-somewhere"><a href="#save-as-file-app-in-somewhere" class="headerlink" title="save as file.app in somewhere"></a>save as file.app in somewhere</h3><blockquote><p>YOU can use your own icon, just change the picture to icns and drag it to apps/contents/resources to replace the original one. And also copy your picture to the <strong>Get Info Menu</strong></p></blockquote><h3 id="customize-toolbar"><a href="#customize-toolbar" class="headerlink" title="customize toolbar"></a>customize toolbar</h3><h4 id="drug-your-new-application-to-toolbar"><a href="#drug-your-new-application-to-toolbar" class="headerlink" title="drug your new application to toolbar"></a>drug your new application to toolbar</h4><p><img src="http://i4.fuimg.com/640680/a26d9176d1204015.png" alt="Markdown"></p><p><strong>DONE</strong></p><p><img src="http://i4.fuimg.com/640680/8d684b37c5e1ad34.png" alt="Markdown"></p><p><strong>NEXT TIME click the button above to enjoy your design</strong></p><p>As you can see, I design another one to sync my scripts, contents:</p><figure class="highlight groovy"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">rsync -avzh <span class="regexp">/Users/</span>james<span class="regexp">/Desktop/</span>website<span class="regexp">/jamesblog /</span>Volumes<span class="regexp">/New\ Drive/</span></span><br><span class="line">rsync -avzh <span class="regexp">/Users/</span>james<span class="regexp">/Desktop/</span>bear <span class="regexp">/Volumes/</span>New\ Drive/</span><br><span class="line">rsync -avzh <span class="regexp">/Users/</span>james<span class="regexp">/Desktop/</span>爷爷回忆录gitbook <span class="regexp">/Volumes/</span>New\ Drive/</span><br><span class="line">rsync -avzh <span class="regexp">/Users/</span>james<span class="regexp">/Desktop/</span>三春 <span class="regexp">/Volumes/</span>New\ Drive/</span><br><span class="line">rsync -avzh --max-size=<span class="string">'5M'</span> --exclude-from=<span class="string">'exclude_me.txt'</span> <span class="string">hpc1:</span><span class="regexp">~/projects/</span> <span class="regexp">/Volumes/</span>New\ Drive/hpc1backup</span><br><span class="line">rsync -avzh --max-size=<span class="string">'5M'</span> --exclude-from=<span class="string">'exclude_me.txt'</span> <span class="string">ibme:</span><span class="regexp">~/projects/</span> <span class="regexp">/Volumes/</span>New\ Drive/ibmebackup</span><br><span class="line">cp <span class="regexp">/Users/</span>james<span class="regexp">/snippets.json /</span>Users<span class="regexp">/james/</span>Library<span class="regexp">/Jupyter/</span>nbextensions<span class="regexp">/snippets/</span>snippets.json</span><br><span class="line">rsync -avzh <span class="regexp">/Users/</span>james<span class="regexp">/snippets.json ibme:/</span>Share<span class="regexp">/home/</span>chenxupeng<span class="regexp">/.local/</span>share<span class="regexp">/jupyter/</span>nbextensions<span class="regexp">/snippets/</span></span><br><span class="line">rsync -avzh <span class="regexp">/Users/</span>james<span class="regexp">/snippets.json cnode:/</span>home<span class="regexp">/chenxupeng/</span>.local<span class="regexp">/share/</span>jupyter<span class="regexp">/nbextensions/</span>snippets/snippets.json</span><br><span class="line">rsync -avzh <span class="regexp">/Users/</span>james<span class="regexp">/snippets.json hpc1:/</span>home<span class="regexp">/chenxupeng/</span>.local<span class="regexp">/share/</span>jupyter<span class="regexp">/nbextensions/</span>snippets/</span><br></pre></td></tr></table></figure><p><img src="http://i4.fuimg.com/640680/4c85f5ca77cab348.png" alt="Markdown"></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;strong&gt;本来是要解决mac一个很蠢的问题的：无法新建文件。&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;之前的策略有两种：一是复制粘贴其他的文件过来。另一个是cd到当前目录下然后touch一个文件。这个有点麻烦，首先cd还得拖到终端一个文件才能找对位置。&lt;/p&gt;
&lt;p&gt;这次用了Mac自带的automator自己做一个简单的生成新文件的应用，非常简单。&lt;/p&gt;
&lt;p&gt;显然既然automator还可以用shell脚本，能低成本实现的东西还是很多的。&lt;/p&gt;
    
    </summary>
    
      <category term="techniques" scheme="https://www.cmwonderland.com/categories/techniques/"/>
    
    
      <category term="techniques" scheme="https://www.cmwonderland.com/tags/techniques/"/>
    
      <category term="setup" scheme="https://www.cmwonderland.com/tags/setup/"/>
    
      <category term="mac" scheme="https://www.cmwonderland.com/tags/mac/"/>
    
  </entry>
  
  <entry>
    <title>混沌之思</title>
    <link href="https://www.cmwonderland.com/2018/05/21/dailythoughts/"/>
    <id>https://www.cmwonderland.com/2018/05/21/dailythoughts/</id>
    <published>2018-05-21T23:47:25.000Z</published>
    <updated>2018-05-22T12:33:10.620Z</updated>
    
    <content type="html"><![CDATA[<h2 id="碎碎念-amp-will-update-daily"><a href="#碎碎念-amp-will-update-daily" class="headerlink" title="碎碎念&amp; will update daily"></a>碎碎念&amp; will update daily</h2><blockquote><p>基本来自于过去的各处的碎碎念，比如记录在朋友圈的，便签里的，笔记里的。和孟孟微信聊天的太不好找了，以及很多时候口头聊的时候觉得自己充满哲理与逻辑的很好的话都没有记录下来，总会有种可惜的感觉，不过也许很多东西都已经内化为大脑结构的一部分了吧。OK，让我们先从中二热血的开始。</p></blockquote><a id="more"></a><hr><p>最爱的龙珠超就这么完结啦，追了两年多，每周日一集，最忙的时候（美赛）都在凌晨补看了，考完托福出来也要先看了龙珠超再吃饭。总有人说龙珠超画面崩坏缺乏诚意，但是真的很好看呀，对于一个上大学了还把龙珠漫画重温了很多遍的人来说，龙珠系列二十多年后还有最爱的漫画的新篇章可以看真的太幸福了，记得去年十月八号109，110两集联播的大战的震撼，让我很多个晚上一遍遍地重温，龙珠总是能在这么多年后还点燃全世界粉丝的热情，龙珠这么赚钱的ip，明年剧场版搞完肯定还会接着画，悟空会更强，故事会继续~<a href="https://bbs.hupu.com/21773153.html" target="_blank" rel="noopener">https://bbs.hupu.com/21773153.html</a> </p><blockquote><p>记于龙珠超完结。</p></blockquote><iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width="330" height="86" src="//music.163.com/outchain/player?type=2&id=536623434&auto=1&height=66"></iframe><hr><p>这么多年了，江南想起来他还有龙族这本书要写了，人已经变成中年大叔了，还能热血得起来么。龙族勾起了我很多美好的回忆，和好哥们一起讨论每一个细节是非常愉快的经历，现在又可以在江南扣扣索索地每次更新两页纸之后吐槽了。</p><blockquote><p>记于龙五开始连载</p></blockquote><hr><p>校庆期间读到最好看的故事，把各个时代的知名人物给串起来了，很多感触：<br>最重要的故事总是被少数几个人推动着，如果没有张钹院士四十年来持续不断对人工智能的大力推进，新一代马少平这样的教师，还有周枫这样的天才，恐怕清华和人工智能未必会有多少缘分；真的很羡慕96年左右的黄金时代，在最早通互联网的地方那批人受到那么好的滋润，实习工资一万一个月，那时候北京房价才两千多吧，到现在也都成了一方互联网大佬，生逢其时很重要；计算机系真的是收纳聪明人，而且课程导向不断把人培养的更聪明有趣的地方，好像唐文斌在文章里说：“我们当时想办法把多门课的大作业凑成一个作业，努力编出一个很合理的题目，既可以交这边又能交那边，就可以用两倍时间把一个大作业做的非常之nb”，上学期用这个方法一个项目用了三遍，可惜没舍得花三倍的时间做好，可见投机取巧容易，有挑战心很难，而编程真的是很容易让人有挑战心的事情；<br>前辈们铺路，这个时代真好。</p><p><a href="https://mp.weixin.qq.com/s/cClINXB242XLmZ6AhE8e_w" target="_blank" rel="noopener">https://mp.weixin.qq.com/s/cClINXB242XLmZ6AhE8e_w</a></p><blockquote><p>记于校庆</p></blockquote><hr><p>太酷炫了，而且看起来更贴近医疗应用，如果设备再小型化商业化的话，是不是可以说细胞尺度的医疗大数据时代就来了，也许有一天真的可以对活体生物做细胞尺度的建模？也许有一天可以对大脑做活体建模呢</p><p><a href="https://mp.weixin.qq.com/s/vn4SL0672OPytqET_4F1Hw" target="_blank" rel="noopener">https://mp.weixin.qq.com/s/vn4SL0672OPytqET_4F1Hw</a></p><hr><p>施boss实在是太了不起了，人生为一件大事来。<br>看校董会俨然是清华系把持的了，说不定几十年后中国教育史”私立大学的曙光”一章会这么写：在当时最好的大学，几位治学与育人都具有超人眼光和手段的学界领袖杨振宁，施一公，钱颖一做出了一步大胆的尝试，从而孕育出了一所世界顶级的中国的私立大学。</p><p><a href="https://mp.weixin.qq.com/s/KiZpIVvtfVHEh-so4szbUQ" target="_blank" rel="noopener">https://mp.weixin.qq.com/s/KiZpIVvtfVHEh-so4szbUQ</a></p><hr><p>强无敌，说好的一年，这才几个月就完成了第一步，我觉得这项目就是杰出科学家和基金会结合的典范，真女神级科学家Regev致力推动的人类细胞图谱，本来也是个上帝工程，需要Regev一遍遍强调一步步慢慢合作就好，不一定要有一个目标，完成一部分也行，结果扎克伯格的资金进来，全球顶级科学家积极申请，提前完成测序，以后数据要是都公开的话计算生物学家要幸福晕了<br>看来下一步忽悠几个富人给连接组学多捐点钱，什么研究大脑结构说不定可以变得更聪明、不得老年痴呆，长生不老什么的，说不定没几年就真把大脑连接全测出来了，要乐观，我看21世纪真还就是生命科学的世纪</p><p><a href="https://mp.weixin.qq.com/s/W2ItMP-LK5qPnAozxgsORg" target="_blank" rel="noopener">https://mp.weixin.qq.com/s/W2ItMP-LK5qPnAozxgsORg</a></p><hr><p>这个真的好啊，快赶上以前mini款嗯价格了，太实惠了，而且支持pencil！ipad pro得五千往上了吧，再加上刚和罗技推出的300的pencil，两千多一点用最新款ipad而且是支持pencil款，ipad真是款越来越没对手的产品，，，</p><p><a href="https://wap.ithome.com/html/352915.htm" target="_blank" rel="noopener">https://wap.ithome.com/html/352915.htm</a></p><hr><p>拿到连接组的电镜数据的时候玩了个无聊的游戏，把空间的一百多层切片按时间顺序展开，发现神经元逐层的伸展在时间尺度上竟然像细胞的无规则运动，感觉这种时空的转换好有趣，如果时间尺度的游动在空间上展开又会是什么有趣的样子呢</p><blockquote><p>记于把空间图像动画呈现后的感想</p></blockquote><hr><p>百度云总裁的程序员表达风格，斯隆管理评论的主席这数据是用matplotlib画的吧，随随便便写个评论发Nature好爽，李稻葵随时装逼：和硅谷风险投资人Peter Thiel吃早餐聊起来人工智能，，，李稻葵是真帅真聪明真有趣，谦虚且学习能力极快，不愧是给boss上课的顶级智囊，李珍老师真幸福，，，疯狂夸工科比MIT强，自黑经管研究很弱，狂黑特朗普，然后自夸政府高科技领域政策宽松，狂赞美国的天才制度，狂黑中国的中庸培养扼杀个性。然后讨论一波不停骂特朗普，估计中美贸易战快被烦死了<br>另外说什么2030还不知道美国总统在哪儿呢，应该搞个AI xian fa，中国想得远，两个一百年，美国整天就在竞选，总感觉在黑，，，</p><blockquote><p>记于中美人工智能前沿论坛</p></blockquote><hr><p>这样做大概就默认了连接组学真的可以从结构还原记忆了，虽然差的太远太远，保存大脑并没有用，还得切成极其薄的切片，亚细胞尺度地成像再重构出来才行，以及如果是致死性的，那就是哲学上的命题了，这个人就不是自己了，应该只有忒休斯之船那样在活着的时候一部分一部分替换才可以，做不影响生命但是存储记忆部位的材料替换。看人类的技术进展，几十年内也没什么戏，这个初创公司的简单的把戏，也就是给个最后一刻的安慰罢了</p><p><a href="https://mp.weixin.qq.com/s/Rq_uUmdllhFvpr_JmNmv3g" target="_blank" rel="noopener">https://mp.weixin.qq.com/s/Rq_uUmdllhFvpr_JmNmv3g</a></p><hr><p>有意思有深度，量子位这位编辑总结的好。之前我还专门总结过<a href="https://www.cmwonderland.com/2018/04/10/Deep-Learning-Practice/">Deep Learning Practice | WonderLand</a></p><p><a href="https://mp.weixin.qq.com/s/R-29UGMvHyBp8OkWk7zdpw" target="_blank" rel="noopener">https://mp.weixin.qq.com/s/R-29UGMvHyBp8OkWk7zdpw</a></p><hr><p>这篇文章读了五遍，体会越来越深。初中的时候从宋老师那里借来犹太名人传科学家册读，活脱脱一部量子力学史，读Hinton的故事则是在读一部深度学习史，让人迫不及待地想知道个究竟，为什么事情这么发生了。Hinton是鼻祖，是如今万人崇敬的精神领袖，还对神经和意识无比痴迷，是个真正的科学家，且家学渊源深厚，“做学者，或者做个失败者”，从这种故事中获得历史上的大师们的精神传承非常有意义。我觉得Hinton有一种真正的科学家的魅力，不够入世，拥有庞大“虚幻”的梦想，  在寒冬和热潮中竟然能够不动摇，这种传世的科学精神非常非常的了不起。</p><p><a href="https://mp.weixin.qq.com/s/vNYMHFhXeEzz6Ny0p1pbNA" target="_blank" rel="noopener">https://mp.weixin.qq.com/s/vNYMHFhXeEzz6Ny0p1pbNA</a></p><hr><p>太厉害了！马斯克原版roadster的神秘力量，太了不起了，并联发动机还可以回收，有人总是想找借口说马斯克没什么了不起，就是一个使用NASA技术的商人，NASA养多少亲儿子了怎么没见到成功的，马斯克想搞火箭的时候还得跑到俄罗斯买的时候又怎么说呢。我总觉得用凡人和投资人的角度去揣度马斯克是现实却狭隘庸俗的，当你被自我眼界所拘束的时候，他在用他的方式追寻星辰大海</p><p><a href="https://wap.ithome.com/html/346687.htm" target="_blank" rel="noopener">https://wap.ithome.com/html/346687.htm</a></p><hr><p>上午的讲座是听过的最佳之一，付向东太了不起了，RNA world的领军人物，人生经历颇为传奇，培养的学生也极其传奇，谈起人生经验科研经验培养经验，不知道台下一堆PI们有没有脸红耳赤的感觉，而且总能抓住重大科学问题，领域的切换也非常顺滑，还滑到了那么有趣的神经领域，最后落到疾病的治疗，实在是很了不起。现在愈发想以后去读个postdoc了，就是不知道六七年后付老师还搞不搞得动啦。午餐的时候听付老师继续谈笑风生，实在是太了不起了了，CNS和子刊发了七十多篇了，，大谈科研经验培养经验人生经验，感觉比施一公还强啊，，，博后做PI成功率80%，简直高的可怕，越来越想去读个postdoc了，，</p><blockquote><p>记于付向东讲座后</p></blockquote><hr><p>哇，从来没有看过这么特别且震撼的电影，今天和朋友去偏僻的海淀工人电影院看一部上映很久的梵高的电影，六万多帧，要么是彩色的油画，要么是黑白的胶片，抽象的影像把让细节和故事本身特别抓人，故事竟然有了点罗生门式的悬疑的味道，却没有走偏，原来观众想寻找的害死梵高的凶手并不存在，谁都没有错，每个人都不是坏人，只剩对天才的崇敬和对天才带来的性格与命运的诅咒的旁观式的叹惋，感觉现在越来越能理解这种事情了，那些让未来的世界变得美好的却承受着来自这个世界的痛苦的人，努力地付出却得痛苦地承认自己得不到回报，这种天赋让人羡慕也让人心疼。</p><blockquote><p>记观影《至爱梵高》</p></blockquote><hr><p>哇很美很美，顺便夹带点私货，学细胞的时候就在想，这么枯燥无味的讲述方式和背书方式简直是毫无用处的折磨，如果有人做了一个3D可视化或者VR的细胞全景，整个学期大家可以真的在细胞里遨游畅览，效果会好太多。只可惜有能力做出来的人大多没时间做。施一公几年前就想做Science is cool的让更多人爱上自然科学的工作了，看起来也没空做，而广大PI们工作繁重，而且恐怕也不舍得让实验室博士生们花很久做这个，我见过的最开明的就是张强锋老师让学生玩了几个月VR看结构了。最后还有一个很重要的问题，对于生物和化学这样公认的大坑来说，爱和理想能不能解决问题。我的很多同学在一门门地背生物课的时候都深受其苦，新雅的学生们和外系混住，意识到自己数学计算机和各种专业课学的太简单太水的也分分修辅修额外选课，让我觉得这么学科和它的解决方法太不匹配了，生物是最美丽动人的学科，但是仿佛在蛮荒时期的不定量，不系统，有大量难解之谜的研究方法让人失落，好像生物应该是四十二世纪的科学，我们都生的太早了一样。<br>而且最好玩的事情在于，生物学细碎却庞大，不需要多么费劲就总能发出来文章，而且往往数据和意义可以帮忙保驾护航，让一堆数学家物理学家嫉妒经费，这些更加美丽理性的学科更难推进，LHC烧了多少经费也验证不出来几个美妙的理论，有的数理出身的还会转生物来抢饭碗，这些人往往都聪明强大，包括在就业工作的时候，这些学了无数门复杂可怕的专业课的学生的综合水平也大大高于生物系的学生，我见识过电子系物理系计算机系这些专业的学生，课程设置上生物系一学期的课程带来的智力挑战未必比得上这些院系一门课程，院系间的差距大的可怕，最终带来了人的巨大差距。<br>所以结论就是，生物学太美丽而宏大，这么复杂的系统让我们丧失了定量化的工具，丧失了运用大脑更复杂的数理逻辑体系化解决的能力，于是我们选择了一条很卑微的道路，把精彩的专业课变成了专业内常识与名词介绍与记忆课程。其实一条复杂的信号通路，一个蛋白的互作网络，都需要大量的数学模型去描述，但是这样没法讲，没法做，投入太多，大家压力太大，研究看起来用不着这么难也能发文章，那就索性不要了好了。我去听过北大开的生物数学物理的课，讲如何在生物学问题里建模，课是数学系老师开的，课是数学物理专业学生选的。<br>从就业角度，大家当然愿意学金融，同样一片混沌模糊的专业，但是能赚钱，选计算机电子，累，动脑子但是赚钱，选数学物理，累动脑子，回头转行赚钱。选生物的话，似乎陷入了一个低门槛的陷阱，前两天一个PI问我，觉不觉得生物是一个大坑，每个人都知道它的问题，但是看起来这不是生物学的错，也不是大家的错，也许是生的早了的错，也许也有点想的不够多不够努力的错，所以对于广大社会群体来讲，同样不思考那么多，大学直接去计算机电子数学物理这些专业跟着被虐，都能学到很多能力，所以也怪大家主观能动性不强，还要怪生物学现象太多太多了，等二十五世纪一切尘埃落定，所有的一切发现都摆在生物学家眼前等待解释的时候，我相信那个时代的生物会是门槛最高最需要思维与理性的专业。希望这两代科学家解决好疾病衰老和神经的问题，让我们都有机会看到那一天吧。</p><p><a href="https://mp.weixin.qq.com/s/enkZra8kLqnV2gEi3ThTMA" target="_blank" rel="noopener">https://mp.weixin.qq.com/s/enkZra8kLqnV2gEi3ThTMA</a></p><hr><p>学校终于批复了嘛。中国最好的大学损失了一位未来的杰出校长，但是中国教育史上可能因此留下一个更加传奇的名字，这是件功德无量的大事，施一公说他要把西湖大学建成中国的Berkeley，一定可以的</p><p>前几任校领导，陈希是今上化工系的老同学，如今已经是中组部长，书记处书记，胡和平水利系出来，已经是陕西书记，陈吉宁卸任不过两年多，已经是北京市长，施boss本来就深得今上厚爱，政治上也是前途无量的，但是我感觉他仔细考虑过这些，作为一个更加纯粹的科学家和教育家，他找到了一种我们这些书生看起来更棒的青史留名的机会，他衡量过什么才是他后半生的人生大事，所以这份事业在他心中真的拥有无比厚重的分量</p><blockquote><p>记于施一公辞任清华副校长，全职执掌西湖大学</p></blockquote><hr><p>最后一节体育课，一同学突然凑过来说，同学你以前是不是法学院的，看着好熟悉，我以为是法学院一同学，交谈几句发现是五字班的，生物竞赛生，开学就转了物理。高三的时候第一届生物预科班提前来了清华。三年前的春天，我当时还没转生物，看到了一个去NIBS罗敏敏老师实验室参观的通知，就跟着几个人一起跑过去了，想想真的神奇，在那里看了光遗传学的技术，还有透明脑的技术，罗敏敏老师一直是个非常fancy的人，然后认识了这个同学，我还感慨着我还没有转到生物系，这边高三的学生就被施一公拉过来提前上课了。我记得罗老师请吃饭的时候，我还问他，老师您觉得有没有可能我们可以把大脑里的所有结构研究清楚，这样人就可以永生了，我就是为这个梦想转系的。两年后上了钟毅和罗敏敏老师的神经课，知道王立元学长在钟老师那里也在做透明脑的成像，还认识了林祖迪学长做深度学习，但是暑研去了Harvard的Litchman那里用深度学习重构三维的大脑连接组学图像。而这位转物理的同学依然做着生物相关的研究，我一问，果然在宋森老师那里，做类脑计算的东西，宋森老师现在想用深度学习建模模拟出海马体的功能，感觉会非常有趣。钟老师本来也对我说想做类脑计算方面的工作，只可惜我本事太差，于是还是决定先踏踏实实学好深度学习的东西，心理先继续惦记着更加有趣但是飘渺的事物。没想到三年后遇到，大家围着的还是差不多的东西，大脑与意识真是古往今来最吸引人的事物，世界真小，但是有意思的人真多呀。</p><blockquote><p>记大三上网球课</p></blockquote><hr><p>2017年是我人生中真正感到满意的一年，难以想象这一年发生了多少事情，我热爱牵挂的人们依然幸福，最爱的人常在身边，发现了更多更大的世界，过去很多做不到的事情竟然一点点做到，遇到了很棒很了不起的人，感觉到人生的幸运和幸福，这半年做了很多不敢想象的事情，为自己喜爱的事情而奋斗是最佳的幸福条件，这一年感到自己多年来希冀的渺茫的梦想也许依然有为之奋斗一生的意义，这一年心想事成，感觉人类和科学的未来也光明可爱。<br>希望接下来的每一年，都能满意如今年，要倍加珍惜眼前美好的一切，要做更多真正了不起的事，要多想想真正的梦想，2018会更美好~</p><blockquote><p>2017年终</p></blockquote><hr><p>“在AI领域，我真的非常期待实现一个“没有AI就无法实现的”大的科学突破。我想这可能会在2018年发生在一个领域类似生物或化学领域。”<br>我们需要一个生物学领域的alpha go给老头子们带来点激荡了</p><p><a href="https://mp.weixin.qq.com/s/TyfT9bYq7tOkwuw7m0_SKg" target="_blank" rel="noopener">https://mp.weixin.qq.com/s/TyfT9bYq7tOkwuw7m0_SKg</a></p><hr><p>Amazing，真的听不出来，那么不久电话客服就可以被淘汰了吗？既然他们大多数本身就在做信息检索，按照一套标准流程回应，现在有很自然的声音的话人们好接受多了。以及Google assistant 配上这样的声音能把siri甩开几个身位了，观感上差太多</p><p><a href="https://wap.ithome.com/html/340735.htm" target="_blank" rel="noopener">https://wap.ithome.com/html/340735.htm</a></p><hr><p>我看以后可以这样做三语教学：从胎教开始，播放中英文儿歌童谣，再加上把python代码转成语音？我看可以把github加星前一百的python项目的文档转成语音，培养孩子的语感，还可以把python常用package以及Sklearm,TensorFlow以及PyTorch的官方文档转成音频，还可以实时监测胎儿心动数据，搞一个机器学习算法，听的比较迷惑的地方多重复几遍。三岁开始教数值计算，五岁开始教基本数理逻辑，不能让孩子输在起跑线上，，<br>欢迎各位探讨新时代育儿经验</p><p><a href="https://mp.weixin.qq.com/s/WnH42-PLYDokJWpOPumurQ" target="_blank" rel="noopener">https://mp.weixin.qq.com/s/WnH42-PLYDokJWpOPumurQ</a></p><hr><p>这家公司有趣了，暑假参加的比赛，SRT题和ANN大作业都在做胸片的识别，模式识别又做了CT的识别。感觉了感觉这家公司的创业成本和难度，CT的肺部识别因为kaggle和阿里两个大赛成熟方案很多，胸片的也有一些方案，吴恩达也在做，也好解决一些。鉴于代码开源好找到，难度基本就在于和医院合作拿到数据资源了。所以我猜最后还是腾讯来终结医疗图像市场，毕竟国家战略给BAT分山头的时候把AI医疗图像分给企鹅家了</p><p><a href="http://www.infervision.com/Infer/product?from=timeline&amp;isappinstalled=0" target="_blank" rel="noopener">http://www.infervision.com/Infer/product?from=timeline&amp;isappinstalled=0</a></p><hr><p>等老了是不是可以吹牛：年轻的时候看着比特币从几美元涨到了几十万美元一枚，，一天感慨一回，这种不知道顶点和终点的东西，和股市一样让人着迷，，比特币太贵了，要不买挖矿设备提供商英伟达好了</p><p><a href="https://wap.ithome.com/html/336543.htm" target="_blank" rel="noopener">https://wap.ithome.com/html/336543.htm</a></p><hr><p>真是愚蠢到令人瞠目结舌，核心的威力真是强，自己喜欢中医所以就这么搞？这是药，有可能会吃死人的东西，你为了推广中医临床都不用做了？以及中医不用参加医学考试，开诊所登记一下就行，我随意揣测一下：国家社保资金有巨大的缺口，医保承受不住，你们干脆就吃中药别吃那么贵的西药了，反正我们自己研发不出来有专利有仿制不了。我看以后谁敢看中医。</p><p><a href="https://mp.weixin.qq.com/s/6xQOUk3vgUe5mIWjhnPDKQ" target="_blank" rel="noopener">https://mp.weixin.qq.com/s/6xQOUk3vgUe5mIWjhnPDKQ</a></p><hr><p>马斯克不是最完美的那个，却永远是最有魅力的那个，但是魅力这个词又这么单薄，这个人活得就像救世主一样，如果未来有什么决定命运的关头被挽救，那一定有他的功劳</p><p><a href="https://mp.weixin.qq.com/s/8tVLtErD7FvJ6jf7W45yLg" target="_blank" rel="noopener">https://mp.weixin.qq.com/s/8tVLtErD7FvJ6jf7W45yLg</a></p><hr><p>没看成直播，十年后roadster2终于来了，百公里1.9，续航一千公里，史上速度最快的，在我看来也是最美的跑车，，但是另一款产品其实更伟大，这个拥有SpaceX公司的男人一定经常畅想着未来的模样，这个梦幻的卡车是为一个非常干净高效梦幻的未来而生的</p><p><a href="https://36kr.com/p/5103430.html?from=timeline&amp;isappinstalled=0" target="_blank" rel="noopener">https://36kr.com/p/5103430.html?from=timeline&amp;isappinstalled=0</a></p><hr><p>Oh my god！中国人的壮举！好想合作一波不过人家也说了合作的人太多了，，这实在太了不起了，有的事真的得有钱任性的社会主义大国才能干得了，你哈佛的每年上千万经费依然比不上我五十台设备并行成像，，遥远的梦想只要砸钱总是能实现的，大胆预测二十年后连接组学就能带来第一个完整的大脑三维重构图谱，那将是怎样壮观的景象，这个领域值得几代人奋斗终生</p><p><a href="https://mp.weixin.qq.com/s/oEaCUkM59tnNUPKIRZwekA" target="_blank" rel="noopener">https://mp.weixin.qq.com/s/oEaCUkM59tnNUPKIRZwekA</a></p><hr><p>更幸运的是英伟达，，不到两年股价涨了六倍多，明知道AI火的要死矿主们还挖币挖的飞起，这笔这么好赚的买卖怎么就没想到呢，，</p><p><a href="https://mp.weixin.qq.com/s/EocGKMcs3tzY3O_dxbCVZQ" target="_blank" rel="noopener">https://mp.weixin.qq.com/s/EocGKMcs3tzY3O_dxbCVZQ</a></p><hr><p>刚刚上市的搜狗和科大讯飞都是这波AI浪潮的超级幸运儿，浪潮一来发现自己这么多年的技术积累全都用得上，，</p><p><a href="https://mp.weixin.qq.com/s/ik3FKrwzhx_rrUEkKzyeqg" target="_blank" rel="noopener">https://mp.weixin.qq.com/s/ik3FKrwzhx_rrUEkKzyeqg</a></p><hr><p>杰夫花了几天时间，最终想出“要弄清这个问题的最佳方式是把我的生活向前推进到80岁”，并做出“最小化遗憾”的决定。<br>杰夫说，你并不想最后记录下来的都是自己的遗憾。虽然你可能会对自己做错的事情感到懊悔，但更多的遗憾来自于“没有采取任何行动”</p><p>这个时代和马斯克一样强悍聪慧，并且有很终极的梦想的人。</p><p><a href="https://wap.ithome.com/html/333336.htm" target="_blank" rel="noopener">https://wap.ithome.com/html/333336.htm</a></p><hr><p>最后一场报告，生物物理所的副所长许老师真是好厉害，物理学出身，博士老板跟他发了第一篇文章后要转神经了，后来还当了院士，还是宋森的老板，然后许老师两轮博后跟着温伯格和杨振宁两位巨匠[可怜]，然后去冷泉港之前竟然没听过这地方，以为沃森早就死了，后来第一篇生物文章沃森还当了通讯作者，后来一路砍瓜切菜到现在，脑子聪明干啥都行</p><blockquote><p>记于博士生论坛 2017</p></blockquote><hr><p>搜狗就要IPO了，读到这篇旧文，很有感慨，王小川学长大概是这两代互联网巨头们最有情怀的了，不仅有科技情怀，而且有科学情怀，去年来讲AI是我听过的对AI最好的最有境界的思考之一，当年先是低谷，然后在巨头的庇护下找到生存之路，后来保护者弱了下来，自己倒是一步步变强，如今拆开独立上市，中间的博弈肯定也很不轻松，真的很厉害~</p><p><a href="https://mp.weixin.qq.com/s/MdA0w5ebce569XoCqA2rMg" target="_blank" rel="noopener">https://mp.weixin.qq.com/s/MdA0w5ebce569XoCqA2rMg</a></p><hr><p>“我和爸爸通过一段程序对苏轼的3458首词进行了分词处理”，下面不用看了，改天施清华也可以”我和爸爸通过一台冷冻电镜解了个结构”，玩素质教育普通人怎么玩的过北上学生嘛，学学苏轼就能请到康震去讲，普通人被忽悠了那么多年素质教育，兜兜转转最后发现，应试教育才是公平的最好的出路，，</p><p>当然这样培训出来的学生确实可以很精英，高一去清华附中夏令营，发现附中的学生虽然数学题做的不如，但是数学建模很厉害，大多数地方的同学几乎是第一次接触这个概念，这种对普通人不公平的精英训练确实有效</p><p><a href="http://app.myzaker.com/news/article.php?pk=59dc08069490cbb27b00001f&amp;from=timeline&amp;isappinstalled=0" target="_blank" rel="noopener">http://app.myzaker.com/news/article.php?pk=59dc08069490cbb27b00001f&amp;from=timeline&amp;isappinstalled=0</a></p><hr><p>乍一看相当激动人心的样子，不过发现是去年的论文，似乎研究者也是耕耘在材料领域的，看到芯片真的长成神经元性状还是蛮震撼的，临界自组织还有幂率分布倒也很契合一派人对智能的看法。不过就像文中说的，这种芯片本身就有点不可解释了，人虽然不能解释人脑的具体机制依然可以使用人脑，不知道这种芯片的编码解码如何做下去呢，说不定机理上黑箱芯片不比普通芯片加黑箱算法容易解释，但是依然是类脑计算的一个有趣的也许有用的好研究~</p><p><a href="https://mp.weixin.qq.com/s/LqR4KLBkOWtu4UiElsQvIA" target="_blank" rel="noopener">https://mp.weixin.qq.com/s/LqR4KLBkOWtu4UiElsQvIA</a></p><hr><p><strong>TO BE CONTINUED，上面的只是从朋友圈整理过来的。不得不说苹果的生态做的很好，靠着在手机上复制及时粘贴到电脑的功能，整理着还是非常快的。</strong></p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;碎碎念-amp-will-update-daily&quot;&gt;&lt;a href=&quot;#碎碎念-amp-will-update-daily&quot; class=&quot;headerlink&quot; title=&quot;碎碎念&amp;amp; will update daily&quot;&gt;&lt;/a&gt;碎碎念&amp;amp; will update daily&lt;/h2&gt;&lt;blockquote&gt;
&lt;p&gt;基本来自于过去的各处的碎碎念，比如记录在朋友圈的，便签里的，笔记里的。和孟孟微信聊天的太不好找了，以及很多时候口头聊的时候觉得自己充满哲理与逻辑的很好的话都没有记录下来，总会有种可惜的感觉，不过也许很多东西都已经内化为大脑结构的一部分了吧。OK，让我们先从中二热血的开始。&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="thoughts" scheme="https://www.cmwonderland.com/categories/thoughts/"/>
    
    
      <category term="thoughts" scheme="https://www.cmwonderland.com/tags/thoughts/"/>
    
      <category term="life" scheme="https://www.cmwonderland.com/tags/life/"/>
    
  </entry>
  
  <entry>
    <title>Death &amp; Dream</title>
    <link href="https://www.cmwonderland.com/2018/05/21/death-and-dream/"/>
    <id>https://www.cmwonderland.com/2018/05/21/death-and-dream/</id>
    <published>2018-05-21T04:49:39.000Z</published>
    <updated>2018-05-21T17:17:53.425Z</updated>
    
    <content type="html"><![CDATA[<p><strong>I have the feeling that it will be the hardest passage I ever write. My life is filled with these two strong words, but it seems so painful to organize them into words.</strong> Usually I will ignore and avoid thinking about these sad topics, but it always haunted me. I know how much fear and sadness I will suffer from writing them done. </p><p>The <strong>fear</strong> about them, the <strong>sadness</strong> about my inability to conquer them, the <strong>envy</strong> of far future’s generations’ potential power to conquer death, traveling through the huge universe, the <strong>despair</strong> of our generations’ lack of  science and technological methods.</p><p>But I will write it down. I am strong when facing anything else, I can always find ways to solve the hard problems, I find my biggest fear happens to be something I should try to solve. I may find some way to hide my fear, to do something else, to earn many money, to live a happy, ignorant life. <strong>But I find out that the only way help me not to regret about my life is doing it, trying all the ways to conquer my fear, and the origin of my fear.</strong></p><iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width="330" height="86" src="//music.163.com/outchain/player?type=2&id=449818741&auto=1&height=66"></iframe><a id="more"></a><hr><p>我已经感觉到了这个东西有多难写了，情绪好的时候不能写，因为写不出来心思，情绪差的时候不敢写，夜深人静不敢写，这是篇很难写的文章，所以我就慢慢一点一点打磨这篇文章，先把一些零碎的东西放上去吧。</p><hr><h3 id="我的人生、转折和恐惧"><a href="#我的人生、转折和恐惧" class="headerlink" title="我的人生、转折和恐惧"></a>我的人生、转折和恐惧</h3><p>我一直讨厌和害怕直面这样的恐惧，当然恐惧会找上门来，有的时候它是好事，它会指引着我做一些事情，不错过一些东西。它指引我放弃了读法律，转身去读生物，纵然“不挣钱、是个大坑”，它指引我关注到好几个领域的重大进展：深度学习、成像技术，它指引我一切考虑它们，它指引我别忘了我的梦想，我的恐惧感其实是我最大的动力源泉。诸如出人头地、挣大钱这样的愿望总是会被我和周围的科学大师们对世俗平庸之物的鄙视所消解，而恐惧死亡，希望了解意识之谜，进而获得一切终极的答案，这是我恐惧和梦想的根源。</p><hr><h3 id="我的梦想"><a href="#我的梦想" class="headerlink" title="我的梦想"></a>我的梦想</h3><p>非常喜欢做梦的人，梦想远大的遥远的东西<br>理想主义，中国这样的人很少，做科研做的功利，细节严谨但丧失了热情而只作为工作。而我热情又专注，格外理想主义，我向往无限远处的宇宙和无限远的未来，思考科学的终极，痴迷于生命，<strong>但是又往往格外因此沮丧、难过</strong></p><p>理想主义，学生物，不想做循环，想搞清楚终极的东西，施一公，一个理想主义者，做很好的科研，让功利的国人意识到科学很酷，科学的意义，在中国最好的大学保护被功利的人们抨击嘲笑的生物学科，受到人们的尊重，如今更是尝试办中国的第一个私立大学，试图培养真正的科学大师。我无比尊敬和喜爱这样的人，从十六岁开始，每一次听施一公讲话，和他聊天，我都热血澎湃。他太了不起了，一年前他告诉我们：人生为一件大事来，他找到了他的人生大事，去办一所真正在中国的公立体制下突破限制的研究型大学，一年多过去了，他做的简直完美，这种梦想的力量，是我坚信我未来有一天也可以完成了不起的事情的源动力，这样的理想主义者和热血澎湃的dreamer是真正了不起的人。</p><p>我无比珍惜生命，我多年来感慨与人择原理，无比珍惜自己的一切，又对意识和永生的话题念念不忘</p><p>“两年前和林祖迪聊完，我这样在日记里写道：Lichtman做的事情让我振奋起来，我无比的兴奋和激动，还有这样的世界呀，感谢我现在有的一切让我有机会去哪怕触摸一下我所向往的东西，看来我那愚蠢又遥远的梦想未必做不到”</p><p><strong>我多年来的想法：</strong></p><p>我想这就是我的终极之梦了，一个妄图超越世俗和生物规律的，相当疯狂的梦想，我常常为之痴迷，又为之害怕，为之沮丧，想尽办法不去想它，但是我的人生轨迹始终被它吸引着，发现另外的dreamer，做着巨大的伟大的梦想的人，如今我就要有机会去看一看试一试体会一下，看看我所向往的终极是什么了。</p><p>我小时候就想过，既然很多人都怕死，为什么不停掉大家做的“暂时的工作”，全人类一起先解决长生不死，然后有的是时间做自己的事情呢。后来我发现世界的复杂、人性的复杂，复杂的超乎想象，但是我也发现人和人是不一样的，dreamer和普通人不一样，有的时候一个强有力的有梦想的人，就足以推动一件伟大的历史事件，比如Lichtman</p><p>人生为一件大事来，我的大事就是这样。</p><p>放牛娃的轮回</p><p>我给十几个人或多或少的讲过，女友，父母，读生物的好友，一些可能听不懂的人，凌晨被我叫醒的室友，我还在回家的火车上感喟一通放在QQ空间，发在我以为会有同伴的论坛。这是件孤独的事情，很难期待有人懂，能体会的人，恐怕我也不想和他多交流。。</p><hr><h3 id="达成终极的思路"><a href="#达成终极的思路" class="headerlink" title="达成终极的思路"></a>达成终极的思路</h3><h4 id="我们的大脑太强"><a href="#我们的大脑太强" class="headerlink" title="我们的大脑太强"></a>我们的大脑太强</h4><p>大脑是复杂的，每个人类大脑包含约1000亿个神经元，能产生100多万亿个连接，常被拿来和另一个复杂系统做比较：电子计算机。它们都具有强大的问题处理能力，都包含大量基本单元——分别是神经元和晶体管，这些基本单元都相连成复杂的环路，处理以电信号形式传输的信息。整体上看，人脑和计算机也有着相似的架构：用于输入、输出、中央处理和记忆存储的环路可以被大致区分开，又能协同工作。</p><p>它们谁处理问题的能力更强？人脑还是计算机？考虑到过去几十年计算机技术发展之快，你可能会觉得答案是计算机。的确，通过适当的组装构建和编程，计算机已经能在复杂游戏中打败人类顶尖高手了：上世纪90年代它打败了国际象棋世界冠军，最近Alpha Go打败了围棋顶尖高手，还有百科知识竞赛电视节目《危险边缘》（Jeopardy！）中机器人也获得了胜利。然而，在我落笔之际，人类仍然能在众多日常任务中更胜一筹——例如在拥挤的城市街道上认出一辆自行车或一个特定的人，例如举起茶杯将它平稳地移到嘴边 ——更不用说大脑的概念化能力和创造力。</p><div class="table-container"><table><thead><tr><th>性能指标</th><th style="text-align:right">计算机</th><th style="text-align:center">人类大脑</th></tr></thead><tbody><tr><td>基本单元数量</td><td style="text-align:right">接近100亿个晶体管</td><td style="text-align:center">～1000亿个神经元 ～100万亿个突触</td></tr><tr><td>基本运算速度</td><td style="text-align:right">100亿/秒</td><td style="text-align:center">&lt;1000/秒</td></tr><tr><td>精度</td><td style="text-align:right">～1/42亿（对于32位处理器）</td><td style="text-align:center">～1/100</td></tr><tr><td>功耗</td><td style="text-align:right">～100瓦</td><td style="text-align:center">～10瓦</td></tr><tr><td>信息处理模式</td><td style="text-align:right">基本是串行</td><td style="text-align:center">串行以及大规模并行</td></tr><tr><td>每个单元的输入/输出</td><td style="text-align:right">1-3</td><td style="text-align:center">～1000</td></tr><tr><td>信号模式</td><td style="text-align:right">数字</td><td style="text-align:center">数字和模拟</td></tr></tbody></table></div><p>Based on data of computer in 2008<br>reference：John von Neumann, The Computer and the Brain （New Haven: Yale University Press, 2012）; D.A Patterson and J.L. Hennessy, Computer Organization and Design(Amsterdam: Elsevier, 2012)</p><p>神经元放电的频率最高大约每秒1,000次。 再举个例子，神经元主要通过在轴突终端特定结构——突触（synapse）上释放化学神经递质来将信息传递给下游神经元，后者将其重新转换为电信号——这个过程我们称之为突触传递。突触传输最快大约需要1毫秒。因此无论是尖峰还是突触传递，大脑每秒最多可执行大约1000次基本运算，比计算机慢1000万倍。</p><p><strong>~职业的网球运动员可以追踪以160英里/小时运行的网球的运动轨迹。~</strong></p><p>然而，大脑的计算表现既不算慢也不算坏。比如，一个职业的网球选手可以追踪高达160英里/小时速度运行的网球的运动轨迹，移到球场最佳位置，挥动手臂，甩动球拍，将球击回对面，一系列动作发生在几百毫秒之间。此外，大脑完成所有这些任务（在其控制的身体的帮助下），功耗比计算机大约低十倍。大脑是怎么做到的？计算机和大脑之间的一个重要区别是每个系统内部处理信息的模式。计算机任务主要以顺序步骤执行，这点我们可以从工程师创建顺序指令流的编程方式中看出来。对于这种连续级联运算，每个步骤都需要高度精确，因为误差会在连续步骤中积累、放大。大脑也使用连续步骤来处理信息。在网球回击的例子中，信息从眼睛传向大脑，然后传向脊髓，从而控制腿部、躯干、手臂和手腕的肌肉收缩。</p><p>但利用数量众多的神经元和每个神经元发出的大量连接，大脑也同时采用大规模的并行处理。例如，移动的网球会激活视网膜中许多称为光感受器的细胞，它的作用是将光转换为电信号。这些信号随之被并行传送给视网膜中许多不同类型的神经元。只消两个至三个突触传递的时间，球的位置、方向、速度的相关信息已经被不同神经环路提取，并平行地传输到大脑。同样地，运动皮层（大脑中负责有意识运动的部分）会发出平行的指令分别控制腿部、躯干、手臂和手腕的肌肉收缩，这样身体和手臂能同时协调，调整身体到接球的最佳姿势。</p><p>这种大规模并行策略是是可行的，因为每个神经元都从许多其他神经元那儿输入、输出信息——一个哺乳动物神经元平均有数以千计的输入和输出。相比之下，计算机每个晶体管仅有三个输入和输出节点。来自单个神经元的信息可以被传递到许多并行的下游路径。与此同时，许多处理相同信息的神经元，可以将它们的输入信息集中到相同的下游神经元。后一种特性对于提高信息处理的精度特别有用。例如，由单个神经元表示的信息可能是“嘈杂”的（比如说，精确度为1/100）。通过取平均值，下游的神经元小伙伴通常能够从100个输入神经元中提取更精确的信息（这种情况下，精确度能到千分之一） [6]。</p><p>计算机和大脑相比，基本单元的信号模式也有相同和不同之处。晶体管采用数字信号，使用离散值（0和1）来表示信息。神经元轴突中的峰值也是一个数字信号，因为神经元在任何时间要么处于尖峰状态，要么处于非激活状态。当神经元被激活时，所有尖峰都是差不多相同大小、形状，这一特性有助于实现可靠的远距离尖峰传播。不过，神经元也利用模拟信号，它使用连续的值来表示信息。一些神经元（像我们视网膜中的大多数神经元）是无尖峰的，它们的输出通过分级的电信号传输的（与尖峰信号不同，它的大小可以连续变化），可传输比尖峰信号更多的信息</p><p>回击网球的例子还彰显了大脑另一个显著特点：它可以基于当前状态和历史经验，修改神经元之间的连接强度——神经科学家们普遍认为，这是学习与记忆的基础。重复的训练能使神经元环路为任务优化其连接方式，从而大幅提高速度与精确度。</p><p>在过去的几十年里，工程师不断受大脑启发来改进计算机设计。并行处理的原则，还有根据使用情况调整连接强度，都被纳入了现代计算机。比如，增加并行性，即在单个计算机中使用多个核心处理器，已经是当前计算机设计的趋势。又比如，机器学习和人工智能领域的“深度学习”，近年来取得了巨大的成功，计算机和移动设备中的物体识别和语音识别方面的迅速进展都得益于它，其就是受到了哺乳动物视觉系统的启发。和哺乳动物的视觉系统一样，深度学习采用越来越深的层次来表示越来越抽象的特征（比如视觉目标或者言语），不同层次之间的连接权重也通过机器学习动态调整，而不是由工程师手工设计。这些最新进展已经扩展了计算机能够执行的任务的指令表。尽管如此，大脑还是比最先进的计算机具有更高的灵活性、泛化与学习能力。</p><h4 id="将意识硅基化"><a href="#将意识硅基化" class="headerlink" title="将意识硅基化"></a>将意识硅基化</h4><h5 id="忒休斯之船的困难"><a href="#忒休斯之船的困难" class="headerlink" title="忒休斯之船的困难"></a>忒休斯之船的困难</h5><p>克隆、量子力学下的复制等等都是和将意识存储到另一台机器上一样的，并不切实可行的思路，因为它们都犯了同样的一个错误：未能成功地将一份独一无二的记忆更好地保存，而是试图复制出一份更新的来替代，这是错误的。</p><blockquote><p>假定某物体的构成要素被置换后，但它依旧是原来的物体吗？1世纪时的希腊作家普鲁塔克提出一个问题：如果<strong>忒修斯</strong>的船上的木头被逐渐替换，直到所有的木头都不是原来的木头，那这艘船还是原来的那艘船吗？</p></blockquote><p><strong>是的</strong>，我想是的。如果我们重新完美地复制了一艘船，那当然不是忒休斯之船，如果我们重新复制出来一个人，那一定不是原来的人，先不论其难度几何（比如克隆一定是远远不够的，其脑部的结构细节可以说差的太远，因而思维和记忆的硬件基础已经完全不同，更何况他还是没有被灌注那份原始的拷贝）。</p><p>事实上我们一定不能先造出另一个硬件的容器，然后用灌注我们的记忆的方式把我们的一切意识灌入另一个容器（且不论其难度）。因为那只是从他人的角度保存自己的思想的方式，这是残酷的，冰冷的，也是没有意义的。既然我们已经在讨论永生这么疯狂又自我的事情，我们就得明白，这里面最关键的点乃是 ~<strong>“自我的感受”</strong>~，如果你眼睁睁地看着克隆出来的你、或者意识被灌注到机器上的你活着，但是你柔软的大脑依然会渐渐枯萎，那么，你还是会死去，你并未获得永恒。</p><p>我想我们唯一的思路恰恰是<strong>忒休斯之船的思路</strong>，这个思路很好想，在刚进入大学的夏日的凉夜，我就把这个想法在闲聊中告知了一位同学，大多数人都不会想这种奇怪的故事，但是他同意了我的想法，有趣的是这位同学还会继续留在清华读深度学习领域的phd。我说，</p><blockquote><p>我们应该一点一点地用电子元件替代</p></blockquote><h4 id="深度学习与类脑芯片"><a href="#深度学习与类脑芯片" class="headerlink" title="深度学习与类脑芯片"></a>深度学习与类脑芯片</h4><blockquote><p>深度学习未来一定会帮人类社会迈入伟大，而且会带给孕育它的主要对象之一，神经生物学带来天翻地覆的影响。<br>我一直觉得自己生在一个很美妙的时代，一个黎明前的时代，有太多东西需要摸索，一个科学家喜爱的时代，有无尽的探索可以去做。尤其是似乎处在人工智能的黎明</p></blockquote><h4 id="系统计算神经生物学"><a href="#系统计算神经生物学" class="headerlink" title="系统计算神经生物学"></a>系统计算神经生物学</h4><blockquote><p>深度学习未来一定会帮人类社会迈入伟大，而且会带给孕育它的主要对象之一，神经生物学带来天翻地覆的影响。<br>我一直觉得自己生在一个太早的时代，一个黎明前黑暗的时代，有太多东西需要摸索，一个科学家喜爱的时代，有无尽的探索可以去做。只可惜我一直处于一种<strong>朝闻道，夕死可矣</strong>的急迫中，我迫不及待地想窥探终极，无法获知终极的渺小感十分的让人痛苦。而且这个黎明前的黑暗距离破晓还有遥远的时间长度。</p></blockquote><p>深度学习的发展救回了我的梦想，但是梦想依然遥不可及，我们无法一直沿着同一条路走下去，人工智能有很多光明的应用，而我所探讨的东西是黑暗中的弥散在各处的不被过多探讨的迷雾，我们探讨的，是一个<strong>终极前景伟大到超越一切，但是直到终极前都毫无意义，天才和工作人员和愿意投入的人</strong>远少于人工智能这样的行业的东西。到最后我们必须自己想办法。我们有一些了不起的人物，Geoffrey Hinton从神经科学的角度做出了深度学习，而他依然执着于大脑的意义，这样的天才与大师是我们的希望之一。另外的更重要的希望还在深耕于神经科学领域的大师。</p><p>感谢我的女朋友孟孟，她总是在我脆弱的时候给我力量，一份美好的心态可以帮人面对一切。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;strong&gt;I have the feeling that it will be the hardest passage I ever write. My life is filled with these two strong words, but it seems so painful to organize them into words.&lt;/strong&gt; Usually I will ignore and avoid thinking about these sad topics, but it always haunted me. I know how much fear and sadness I will suffer from writing them done. &lt;/p&gt;
&lt;p&gt;The &lt;strong&gt;fear&lt;/strong&gt; about them, the &lt;strong&gt;sadness&lt;/strong&gt; about my inability to conquer them, the &lt;strong&gt;envy&lt;/strong&gt; of far future’s generations’ potential power to conquer death, traveling through the huge universe, the &lt;strong&gt;despair&lt;/strong&gt; of our generations’ lack of  science and technological methods.&lt;/p&gt;
&lt;p&gt;But I will write it down. I am strong when facing anything else, I can always find ways to solve the hard problems, I find my biggest fear happens to be something I should try to solve. I may find some way to hide my fear, to do something else, to earn many money, to live a happy, ignorant life. &lt;strong&gt;But I find out that the only way help me not to regret about my life is doing it, trying all the ways to conquer my fear, and the origin of my fear.&lt;/strong&gt;&lt;/p&gt;
&lt;iframe frameborder=&quot;no&quot; border=&quot;0&quot; marginwidth=&quot;0&quot; marginheight=&quot;0&quot; width=&quot;330&quot; height=&quot;86&quot; src=&quot;//music.163.com/outchain/player?type=2&amp;id=449818741&amp;auto=1&amp;height=66&quot;&gt;&lt;/iframe&gt;
    
    </summary>
    
      <category term="thoughts" scheme="https://www.cmwonderland.com/categories/thoughts/"/>
    
    
      <category term="thoughts" scheme="https://www.cmwonderland.com/tags/thoughts/"/>
    
      <category term="life" scheme="https://www.cmwonderland.com/tags/life/"/>
    
      <category term="dream" scheme="https://www.cmwonderland.com/tags/dream/"/>
    
  </entry>
  
  <entry>
    <title>Linear Regression Assignment 12</title>
    <link href="https://www.cmwonderland.com/2018/05/20/line6/"/>
    <id>https://www.cmwonderland.com/2018/05/20/line6/</id>
    <published>2018-05-20T14:01:19.000Z</published>
    <updated>2018-05-21T14:14:32.125Z</updated>
    
    <content type="html"><![CDATA[<p>The twelveth assignment of Linear Regression. The assignment is written in Rmarkdown, a smart syntax supported by RStudio helping with formula, plot visualization and plugin codes running.</p><a id="more"></a><blockquote><p><strong>most recommend:</strong> <a href="https://www.cmwonderland.com/linearweek12.html"><strong>click here</strong></a> for <strong>html version</strong> of assignment, you can see codes as well as plots.</p></blockquote><p>You may also find the <a href="https://github.com/james20141606/somethingmore/tree/master/linear_regression_pdf" target="_blank" rel="noopener"><strong>PDF Version</strong></a> of this assignment from github. <strong>Or if you can cross the fire wall, just see below</strong>:<br><div class="row"><iframe src="https://drive.google.com/file/d/1TV_5cYmIUSdir6iIdjqZsDHVsWejGBpE/preview" style="width:100%; height:550px"></iframe></div></p><h1 id="1"><a href="#1" class="headerlink" title="1"></a>1</h1><h2 id="a"><a href="#a" class="headerlink" title="a"></a>a</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">x &lt;- seq(-5, 18, length=1000)</span><br><span class="line">y1 &lt;- dnorm(x, mean=5.1, sd=2.8)</span><br><span class="line">y2 &lt;- dnorm(x, mean=6.3, sd=2.8)</span><br><span class="line">y3 &lt;- dnorm(x, mean=7.9, sd=2.8)</span><br><span class="line">y4 &lt;- dnorm(x, mean=9.5, sd=2.8)</span><br><span class="line">plot(x, y1, type=&quot;l&quot;, lwd=1)</span><br><span class="line">lines(x, y2, type=&quot;l&quot;, lwd=1)</span><br><span class="line">lines(x, y3, type=&quot;l&quot;, lwd=1)</span><br><span class="line">lines(x, y4, type=&quot;l&quot;, lwd=1)</span><br><span class="line">abline(v=5.1)</span><br><span class="line">abline(v=6.3)</span><br><span class="line">abline(v=7.9)</span><br><span class="line">abline(v=9.5)</span><br></pre></td></tr></table></figure><h2 id="b"><a href="#b" class="headerlink" title="b"></a>b</h2><script type="math/tex; mode=display">E(MSE) = \sigma^2 = 7.84 \\E(MSTR) = \sigma^2 + \frac{\sum _i(\mu_i-\mu  )^2}{\gamma -1} \\=7.84+\frac{100[(5.1-7.2)^2+(6.3-7.2)^2+(7.9-7.2)^2+(9.5-7.2)^2]}{3} \approx 374</script><p>It suggests that the different treatments have substantially impact on Y</p><h2 id="c"><a href="#c" class="headerlink" title="c"></a>c</h2><script type="math/tex; mode=display">\text{Use same equation as b, we have: }E(MSTR) = \sigma^2 + \frac{\sum _i(\mu_i-\mu  )^2}{\gamma -1} \\=7.84+\frac{100[(5.1-7.2)^2+(5.6-7.2)^2+(9-7.2)^2+(9.5-7.2)^2]}{3} \approx 523</script><p>It is because the points distribution are more scattered compared to b.</p><h1 id="2"><a href="#2" class="headerlink" title="2"></a>2</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">dat&lt;-read.table(&quot;CH16PR10.txt&quot;)</span><br><span class="line">names(dat)&lt;-c(&quot;x&quot;,&quot;age&quot;,&quot;num&quot;)</span><br></pre></td></tr></table></figure><h2 id="a-1"><a href="#a-1" class="headerlink" title="a"></a>a</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">plot(x=dat$age, y=dat$x)</span><br></pre></td></tr></table></figure><p>The factor level means seem to differ, at least the middle ge group differs from the other two.<br>The variability within each factor level seems to be constant.</p><h2 id="b-1"><a href="#b-1" class="headerlink" title="b"></a>b</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">code &lt;- rbind(diag(1,3),-1)</span><br><span class="line">xx &lt;- code[dat$age,]</span><br><span class="line">dat$x1 &lt;- xx[,1]</span><br><span class="line">dat$x2 &lt;- xx[,2]</span><br><span class="line">dat$x3 &lt;- xx[,3]</span><br><span class="line"></span><br><span class="line">dat$age &lt;- factor(dat$age)</span><br><span class="line">fit1 &lt;- aov(x~age,data=dat)</span><br><span class="line">yhat &lt;- fitted(fit1)</span><br><span class="line">yhat</span><br></pre></td></tr></table></figure><h2 id="c-1"><a href="#c-1" class="headerlink" title="c"></a>c</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">resid1 &lt;- resid(fit1)</span><br><span class="line">resid1</span><br></pre></td></tr></table></figure><h2 id="d"><a href="#d" class="headerlink" title="d"></a>d</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">summary(fit1)</span><br></pre></td></tr></table></figure><h2 id="e"><a href="#e" class="headerlink" title="e"></a>e</h2><script type="math/tex; mode=display">H_0:\mu_1=\mu_2=\mu_3 \\H_1: \text{otherwise} \\F^*=\frac{MSTR}{MSE}</script><p>Reject null hypothesis if F*&gt;$F_{0.99,3,33}$<br>The p value is 4.769e-12<br>We reject H_0</p><h2 id="f"><a href="#f" class="headerlink" title="f"></a>f</h2><p>If seems that middle aged people tend to offer more cash for a used car, while young and old people tend to offer less.</p><h1 id="3"><a href="#3" class="headerlink" title="3"></a>3</h1><h2 id="a-2"><a href="#a-2" class="headerlink" title="a"></a>a</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">fit2 &lt;- lm(x~age,data=dat)</span><br><span class="line">summary(fit2)</span><br></pre></td></tr></table></figure><p>The model is $Yhat=21.5+6.25X_1-0.0833X_2$<br>The intercept term estimates the average cell sample mean.</p><h2 id="b-2"><a href="#b-2" class="headerlink" title="b"></a>b</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">anova(fit2)</span><br></pre></td></tr></table></figure><p>$H_0:\tau_1=\tau_2$ </p><p>$H_1: otherwise$</p><p>$F^*=\frac{MSR}{MSE}$</p><p>Reject null hypothesis if F*&gt;$F_{0.99,2,33}$<br>The p value is 4.769e-12, so We reject H_0</p><h1 id="4"><a href="#4" class="headerlink" title="4"></a>4</h1><h2 id="b-3"><a href="#b-3" class="headerlink" title="b"></a>b</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">Young=c(23, 25, 21, 22, 21, 22, 20, 23, 19, 22, 19, 21)</span><br><span class="line">Middle=c(28, 27, 27, 29, 26, 29, 27, 30, 28, 27, 26, 29)</span><br><span class="line">Elderly=c(23, 20, 25, 21, 22, 23, 21, 20, 19, 20, 22, 21)</span><br><span class="line">FactorLevels=c(1,2,3)</span><br><span class="line">n1=length(Young)</span><br><span class="line">n2=length(Middle)</span><br><span class="line">n3=length(Elderly)</span><br><span class="line"></span><br><span class="line">MyData=data.frame(</span><br><span class="line">Values=c(Young,Middle,Elderly),</span><br><span class="line">Treatment=c(rep(1,n1),rep(2,n2),rep(3,n3)))</span><br><span class="line"></span><br><span class="line">y=MyData$Values</span><br><span class="line">x=factor(MyData$Treatment)</span><br><span class="line">means=tapply(y,x,mean)</span><br><span class="line">n=tapply(y,x,length)</span><br><span class="line">df=sum(n)-2</span><br><span class="line"></span><br><span class="line">MSE=2.49</span><br><span class="line">alpha=0.01</span><br><span class="line">l1=means[1]-qt(1-alpha/2,df)*sqrt(MSE/n[1])</span><br><span class="line">u1=means[1]+qt(1-alpha/2,df)*sqrt(MSE/n[1])</span><br><span class="line">print(c(l1,u1))</span><br></pre></td></tr></table></figure><p>So the confidence level is (20.2572,22.7428)</p><h2 id="c-2"><a href="#c-2" class="headerlink" title="c"></a>c</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">MSE=2.49</span><br><span class="line">alpha=0.01</span><br><span class="line">l31=means[3]-means[1]-qt(1-alpha/2,df)*sqrt(MSE/n[1]+MSE/n[3])</span><br><span class="line">u31=means[3]-means[1]+qt(1-alpha/2,df)*sqrt(MSE/n[1]+MSE/n[3])</span><br><span class="line">print(c(l31,u31))</span><br></pre></td></tr></table></figure><p>This confidence interval contains 0, so we cannot reject the null hypothesis that $\mu_1=\mu_3$</p><h2 id="d-1"><a href="#d-1" class="headerlink" title="d"></a>d</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">MSE=2.49;</span><br><span class="line">alpha=0.01;</span><br><span class="line">lcontrast=-means[1]+2*means[2]-means[3]-qt(1-alpha/2,df)*sqrt(MSE/n[1]+4*MSE/n[2]+MSE/n[3])</span><br><span class="line">ucontrast=-means[1]+2*means[2]-means[3]+qt(1-alpha/2,df)*sqrt(MSE/n[1]+4*MSE/n[2]+MSE/n[3])</span><br><span class="line">print(c(lcontrast,ucontrast))</span><br></pre></td></tr></table></figure><p>$H_0:\mu_2-\mu_1=\mu_3-\mu_2$<br>$H_1: otherwise$<br>Since the confidence interval for the contrast does not contain 0, we do not reject the null hypothesis.</p><h2 id="e-1"><a href="#e-1" class="headerlink" title="e"></a>e</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">results&lt;-aov(y~x);</span><br><span class="line">TukeyHSD(results)</span><br></pre></td></tr></table></figure><p>There is significant difference between young and middle aged people, as well as mlderly and middle aged people. But there is no significant difference between the young and elderly.</p><h2 id="f-1"><a href="#f-1" class="headerlink" title="f"></a>f</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pairwise.t.test(y,x,p.adjust=&quot;bonferroni&quot;)</span><br></pre></td></tr></table></figure><p>The bonferroni method gives the same result. But it won’t be more efficient, since it “overstates” the significnce level.</p><h1 id="5"><a href="#5" class="headerlink" title="5"></a>5</h1><h2 id="a-3"><a href="#a-3" class="headerlink" title="a"></a>a</h2><p>This has been done in the previous problem, question d.<br>The confidence interval is (-15.627664,-9.539003)</p><h2 id="b-4"><a href="#b-4" class="headerlink" title="b"></a>b</h2><p>D_1 = 6.2500, D_2 =  −6.3333, D_3 = −0.0833, </p><p>L_1 =−12.5833, s{D_i} =0.6442 (i = 1, 2, 3), s{L_1} = 1.1158,</p><p>F(0.90, 2, 33) = 2.47, S = 2.223</p><p>Then we can obtain the family intervals:</p><p>(4.818,7.682)</p><p>(−7.765,−4.901)</p><p>(−1.515,1.349)</p><p>(−15.064,−10.103)</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;The twelveth assignment of Linear Regression. The assignment is written in Rmarkdown, a smart syntax supported by RStudio helping with formula, plot visualization and plugin codes running.&lt;/p&gt;
    
    </summary>
    
      <category term="school work" scheme="https://www.cmwonderland.com/categories/school-work/"/>
    
    
      <category term="codes" scheme="https://www.cmwonderland.com/tags/codes/"/>
    
      <category term="R" scheme="https://www.cmwonderland.com/tags/R/"/>
    
      <category term="assignment" scheme="https://www.cmwonderland.com/tags/assignment/"/>
    
      <category term="statistics" scheme="https://www.cmwonderland.com/tags/statistics/"/>
    
      <category term="linear regression" scheme="https://www.cmwonderland.com/tags/linear-regression/"/>
    
  </entry>
  
  <entry>
    <title>Linear Regression Assignment 10</title>
    <link href="https://www.cmwonderland.com/2018/05/20/line5/"/>
    <id>https://www.cmwonderland.com/2018/05/20/line5/</id>
    <published>2018-05-20T14:01:19.000Z</published>
    <updated>2018-05-21T14:14:21.429Z</updated>
    
    <content type="html"><![CDATA[<p>The eleventh assignment of Linear Regression. The assignment is written in Rmarkdown, a smart syntax supported by RStudio helping with formula, plot visualization and plugin codes running.</p><a id="more"></a><blockquote><p><strong>most recommend:</strong> <a href="https://www.cmwonderland.com/linearweek11.html"><strong>click here</strong></a> for <strong>html version</strong> of assignment, you can see codes as well as plots.</p></blockquote><p>You may also find the <a href="https://github.com/james20141606/somethingmore/tree/master/linear_regression_pdf" target="_blank" rel="noopener"><strong>PDF Version</strong></a> of this assignment from github. <strong>Or if you can cross the fire wall, just see below</strong>:</p><div class="row"><iframe src="https://drive.google.com/file/d/1rQelXNDGk1GJmcFKDWqpeJGFbHUiDe1k/preview" style="width:100%; height:550px"></iframe></div><h3 id="15-8"><a href="#15-8" class="headerlink" title="15.8"></a>15.8</h3><h4 id="a"><a href="#a" class="headerlink" title="a"></a>a</h4><p>This study is mixed exeprimental and observational.</p><p>A randomization of treatments (standard or computer based curriculum) does occur. However, the choice of which school is not randomized.</p><h4 id="b"><a href="#b" class="headerlink" title="b"></a>b</h4><ul><li>Experimental factor: Type of curriculum; </li><li><p>Levels: Standard or computer based.</p></li><li><p>observational factor: schools;</p></li><li>Levels: school no.1, 2 and 3</li></ul><p>There are 6 factor-level combinations:</p><ul><li>School1 +standard; </li><li>School 1 +computer based;</li><li>School2 +standard; </li><li>School 2 +computer based;</li><li>School3 +standard; </li><li>School 3 +computer based.</li></ul><h4 id="c"><a href="#c" class="headerlink" title="c"></a>c</h4><p>Completely randomized design</p><h4 id="d"><a href="#d" class="headerlink" title="d"></a>d</h4><p>The basic unit is each of the 24-student sections</p><h3 id="15-10"><a href="#15-10" class="headerlink" title="15.10"></a>15.10</h3><h4 id="a-1"><a href="#a-1" class="headerlink" title="a"></a>a</h4><p>Mixed exeprimental and observational.</p><p>The color of paper is randomized but the nature of parking lots is just observed.</p><h4 id="b-1"><a href="#b-1" class="headerlink" title="b"></a>b</h4><ul><li>Experimental factor: Color of the paper</li><li><p>Levels: Blue, green, orange</p></li><li><p>observational factor: What parking lot the papers are distributed in;</p></li><li>Levels: Parking lot a, b, c, d.</li></ul><p>There are 12 factor-level combinations:</p><ul><li>Parking lot a+ Blue;</li><li>Parking lot b+ Blue;</li><li>Parking lot c+ Blue;</li><li>Parking lot d+ Blue;</li><li>Parking lot a+ Orange;</li><li>Parking lot b+ Orange;</li><li>Parking lot c+ Orange;</li><li>Parking lot d+ Orange;</li><li>Parking lot a+ Green;</li><li>Parking lot b+ Green;</li><li>Parking lot c+ Green;</li><li>Parking lot d+ Green;</li></ul><h4 id="c-1"><a href="#c-1" class="headerlink" title="c"></a>c</h4><p>Repeated measures design</p><h4 id="d-1"><a href="#d-1" class="headerlink" title="d"></a>d</h4><p>The basic unit is the papers of the same color distributed in each of the parking lots.</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;The eleventh assignment of Linear Regression. The assignment is written in Rmarkdown, a smart syntax supported by RStudio helping with formula, plot visualization and plugin codes running.&lt;/p&gt;
    
    </summary>
    
      <category term="school work" scheme="https://www.cmwonderland.com/categories/school-work/"/>
    
    
      <category term="codes" scheme="https://www.cmwonderland.com/tags/codes/"/>
    
      <category term="R" scheme="https://www.cmwonderland.com/tags/R/"/>
    
      <category term="assignment" scheme="https://www.cmwonderland.com/tags/assignment/"/>
    
      <category term="statistics" scheme="https://www.cmwonderland.com/tags/statistics/"/>
    
      <category term="linear regression" scheme="https://www.cmwonderland.com/tags/linear-regression/"/>
    
  </entry>
  
  <entry>
    <title>Linear Regression Assignment 9</title>
    <link href="https://www.cmwonderland.com/2018/05/20/line4/"/>
    <id>https://www.cmwonderland.com/2018/05/20/line4/</id>
    <published>2018-05-20T14:01:19.000Z</published>
    <updated>2018-05-23T13:42:39.625Z</updated>
    
    <content type="html"><![CDATA[<p>The nineth assignment of Linear Regression. The assignment is written in Rmarkdown, a smart syntax supported by RStudio helping with formula, plot visualization and plugin codes running.</p><a id="more"></a><blockquote><p><strong>most recommend:</strong> <a href="https://www.cmwonderland.com/linearweek9.html"><strong>click here</strong></a> for <strong>html version</strong> of assignment, you can see codes as well as plots.</p></blockquote><p>You may also find the <a href="https://github.com/james20141606/somethingmore/tree/master/linear_regression_pdf" target="_blank" rel="noopener"><strong>PDF Version</strong></a> of this assignment from github. <strong>Or if you can cross the fire wall, just see below</strong>:</p><div class="row"><iframe src="https://drive.google.com/file/d/1ZKK3pAgtaT3coeGr6Qm_tmOyXWi7dpNS/preview" style="width:100%; height:550px"></iframe></div><h1 id="1"><a href="#1" class="headerlink" title="1"></a>1</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># read the data</span><br><span class="line">dat&lt;-read.csv(&quot;ch6.csv&quot;)</span><br><span class="line">names(dat)&lt;-c(&quot;y&quot;,&quot;x1&quot;,&quot;x2&quot;,&quot;x3&quot;)</span><br></pre></td></tr></table></figure><h2 id="a"><a href="#a" class="headerlink" title="a"></a>a</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">fit&lt;-lm(y~x1+x2,data=dat)</span><br><span class="line">summary(fit)</span><br></pre></td></tr></table></figure><p>So the regression model is $Y=3995+0.0009192X_1+12.12X_2$</p><h2 id="b"><a href="#b" class="headerlink" title="b"></a>b</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">par(mfrow=c(1,2))</span><br><span class="line">yonx1 &lt;- lm(y~x1,data=dat)</span><br><span class="line">yonx2 &lt;- lm(y~x2,data=dat)</span><br><span class="line">x1onx2 &lt;- lm(x1~x2,data=dat)</span><br><span class="line">x2onx1 &lt;- lm(x2~x1,data=dat)</span><br><span class="line">plot(x1onx2$residuals,yonx2$residuals,col=&quot;blue&quot;,pch=16,</span><br><span class="line">xlab=&quot;e(X_1|X_2)&quot;, ylab=&quot;e(Y|X_2)&quot;)</span><br><span class="line">abline(0,fit$coefficients[2])</span><br><span class="line">abline(0,0,lty=2,col=&quot;gray&quot;)</span><br><span class="line">plot(x2onx1$residuals,yonx1$residuals,col=&quot;red&quot;,pch=16,</span><br><span class="line">xlab=&quot;e(X_2|X_1)&quot;, ylab=&quot;e(Y|X_1)&quot;,xlim=c(-50000,150000))</span><br><span class="line">abline(0,fit$coefficients[3])</span><br><span class="line">abline(0,0,lty=2,col=&quot;gray&quot;)</span><br></pre></td></tr></table></figure><h2 id="c"><a href="#c" class="headerlink" title="c"></a>c</h2><p> We use about the same scale in the two plots. In the first plot, the scatter of points around the least square line does not differ much compared to the scatter around the horizontal line. However, in the second plot, we can see that the scatter around the regression line (which is almost verticle under this scale) is significantly smaller than the scatter around the horizontal line. This tells us that X1 is of little use when X2 is in the model, while X2 can still explain a lot when X1 is present.<br>So perhaps X1 can be discarded.</p><h2 id="d"><a href="#d" class="headerlink" title="d"></a>d</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># The regression functions below are required</span><br><span class="line">fit1&lt;-lm(resid(yonx2)~resid(x1onx2)-1)</span><br><span class="line">summary(fit1)</span><br><span class="line">summary(yonx2)</span><br><span class="line">summary(x1onx2)</span><br></pre></td></tr></table></figure><p>Summarizing from the results above, the regression function is<br>$[Y-(4237.47+17.04x_2)=0.000919[x_1-(263272+5348x_2)]$</p><p>Which is equivalent to $Y=3995+0.0009192X_1+12.12X_2$</p><h1 id="2"><a href="#2" class="headerlink" title="2"></a>2</h1><h2 id="a-1"><a href="#a-1" class="headerlink" title="a"></a>a</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">newfit&lt;-lm(y~x1+x2+x3,data=dat)</span><br><span class="line">sdr&lt;-rstudent(newfit)</span><br><span class="line">sdr # the studentized deleted residuals</span><br></pre></td></tr></table></figure><p>H_0: Not an outlier; H_1: Is an outlier<br>We reject the null hypothesis when the studentized deleted residuals are larger than $t(1-\alpha/2n;n-p-1)$<br>The results are below<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">n=52</span><br><span class="line">p=4</span><br><span class="line">ifelse(sdr &gt; qt(1-0.90/2/n,n-p-1), &quot;YES&quot;, &quot;NO&quot;)</span><br></pre></td></tr></table></figure></p><p>So none is deemed as an outlier.</p><h2 id="b-1"><a href="#b-1" class="headerlink" title="b"></a>b</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">hatd&lt;-hatvalues(newfit)</span><br><span class="line">hatd</span><br><span class="line">n=52</span><br><span class="line">p=4</span><br><span class="line">ifelse(hatd &gt; 2*p/n, &quot;YES&quot;, &quot;NO&quot;)</span><br></pre></td></tr></table></figure><p>According to the rule of the thumb, cases 4,16,21,22,43,44,48 are thought to be outliers.</p><h2 id="c-1"><a href="#c-1" class="headerlink" title="c"></a>c</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">attach(dat)</span><br><span class="line">plot(x1,x2)</span><br></pre></td></tr></table></figure><p>Judging from the scatter plot, this prediction does not seem to involve extrapolation beyond the range of the data.<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">xnew&lt;-c(300000,7.2,0)</span><br><span class="line">X&lt;-as.matrix(dat)</span><br><span class="line">X&lt;-X[,-1]</span><br><span class="line">hnew&lt;-t(xnew)%*%solve(t(X)%*%X)%*%xnew</span><br><span class="line">ifelse(hnew &gt; 2*p/n, &quot;YES&quot;, &quot;NO&quot;)</span><br></pre></td></tr></table></figure></p><p>Using (10.29), the conclusion is the same, as stated above.</p><h2 id="d-1"><a href="#d-1" class="headerlink" title="d"></a>d</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">test&lt;-cbind(</span><br><span class="line">  &quot;DFFITS&quot;  = dffits(newfit),</span><br><span class="line">  &quot;DFBETA0&quot; = dfbetas(newfit)[,1],</span><br><span class="line">  &quot;DFBETA1&quot; = dfbetas(newfit)[,2],</span><br><span class="line">  &quot;DFBETA2&quot; = dfbetas(newfit)[,3],</span><br><span class="line">  &quot;DFBETA3&quot; = dfbetas(newfit)[,4],</span><br><span class="line">  &quot;Cook&apos;s Distance&quot; =cooks.distance(newfit))</span><br><span class="line">test&lt;-test[c(16,22,43,48,10,32,38,40),]</span><br><span class="line">test</span><br><span class="line">qf(0.1,4,48)# 10% quantile of the corresponding F distribution</span><br><span class="line">abs(test[,1])&gt;2*sqrt(p/n)</span><br><span class="line">abs(test[,c(2,3,4,5)])&gt;2/sqrt(n)</span><br></pre></td></tr></table></figure><p>It can be seen that  Cook’s distance is well below 10% quantile of the corresponding F distribution which is roughly 0.26.<br>Judging from DEFITS, it seems that case 43 and 32 are influential.<br>Judging from DFDETA, it seems that cse 16,43,10,32,40 are influential.</p><h2 id="f"><a href="#f" class="headerlink" title="f"></a>f</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd&lt;-cooks.distance(newfit)</span><br><span class="line">plot(seq(1:52),cd,xlab=&quot;Case Index Number&quot;,ylab=&quot;Cook&apos;s Distance&quot;, main=&quot;Index Plot&quot;,type=&quot;b&quot;)</span><br></pre></td></tr></table></figure><p>None of the cases is deemed as influential according to this criteron.</p><h1 id="3"><a href="#3" class="headerlink" title="3"></a>3</h1><h2 id="a-2"><a href="#a-2" class="headerlink" title="a"></a>a</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">pairs(~cases+percent+holiday+labor,data=dat, </span><br><span class="line">   main=&quot;Simple Scatterplot Matrix&quot;)</span><br><span class="line">cor(dat[,1:4])</span><br></pre></td></tr></table></figure><p>There does not seem to be significant pairwise linear associations. X2 and X3 seem to have a relatively higher liner correlation.</p><h2 id="b-2"><a href="#b-2" class="headerlink" title="b"></a>b</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">#cases+percent+holiday</span><br><span class="line">summary(lm(cases ~ percent+holiday))$r.squared </span><br><span class="line">summary(lm(percent~cases +holiday))$r.squared</span><br><span class="line">summary(lm(holiday~cases +percent))$r.squared</span><br><span class="line">myfun&lt;-function(a)&#123;</span><br><span class="line">  result &lt;-1/(1-a**2)</span><br><span class="line">  return (result)</span><br><span class="line">  &#125;</span><br><span class="line">myfun(0.01181682)</span><br><span class="line">myfun(0.01172621)</span><br><span class="line">myfun(0.003705038)</span><br></pre></td></tr></table></figure><script type="math/tex; mode=display">(VIF)_j = \frac{1}{1-R_j^2}\\max_j(VIF)_j = 1.00014 \leq 10</script><p>So there is no serious multicolinearity here.</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;The nineth assignment of Linear Regression. The assignment is written in Rmarkdown, a smart syntax supported by RStudio helping with formula, plot visualization and plugin codes running.&lt;/p&gt;
    
    </summary>
    
      <category term="school work" scheme="https://www.cmwonderland.com/categories/school-work/"/>
    
    
      <category term="codes" scheme="https://www.cmwonderland.com/tags/codes/"/>
    
      <category term="R" scheme="https://www.cmwonderland.com/tags/R/"/>
    
      <category term="assignment" scheme="https://www.cmwonderland.com/tags/assignment/"/>
    
      <category term="statistics" scheme="https://www.cmwonderland.com/tags/statistics/"/>
    
      <category term="linear regression" scheme="https://www.cmwonderland.com/tags/linear-regression/"/>
    
  </entry>
  
  <entry>
    <title>Differential Expression&amp;作业</title>
    <link href="https://www.cmwonderland.com/2018/05/20/diffexpress/"/>
    <id>https://www.cmwonderland.com/2018/05/20/diffexpress/</id>
    <published>2018-05-20T04:08:12.000Z</published>
    <updated>2018-05-21T13:08:07.966Z</updated>
    
    <content type="html"><![CDATA[<p>It contains my assignment in Bioinformatics course talking about Differential Expression and this semesters’ code<br><a id="more"></a></p><h1 id="My-assignment-in-bioinformatics-course"><a href="#My-assignment-in-bioinformatics-course" class="headerlink" title="My assignment in bioinformatics course"></a>My assignment in bioinformatics course</h1><div class="row"><iframe src="https://drive.google.com/file/d/1d2YKSKrCfnELAHiAlEqAxIBErdCfR4-L/preview" style="width:100%; height:550px"></iframe></div><h1 id="Codes-using-different-packages-producing-differential-expression-files"><a href="#Codes-using-different-packages-producing-differential-expression-files" class="headerlink" title="Codes using different packages producing differential expression files"></a>Codes using different packages producing differential expression files</h1><figure class="highlight clean"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br></pre></td><td class="code"><pre><span class="line">setwd(<span class="string">'61samplesmatrix'</span>)</span><br><span class="line">#rnanames = <span class="string">'miRNA'</span></span><br><span class="line">rnaname = <span class="string">'miRNA'</span></span><br><span class="line">commonpath = <span class="string">'04.counts/hcc_lulab.sequentialMap.homer.'</span></span><br><span class="line">savepath = <span class="string">'05.diffexp/hcc_lulab.sequentialMap.homer.'</span></span><br><span class="line">mx &lt;- read.table(paste(commonpath,rnaname,<span class="string">".mx"</span>,sep=<span class="string">""</span>),sep=<span class="string">"\t"</span>,header=T)</span><br><span class="line">design &lt;- read.table(<span class="string">"05.diffexp/design.tsv"</span>,sep=<span class="string">"\t"</span>,header=T)</span><br><span class="line">filter_genes &lt;- apply(</span><br><span class="line">  mx[,<span class="number">2</span>:ncol(mx)],</span><br><span class="line">  <span class="number">1</span>,</span><br><span class="line">  function(x) length(x[x &gt; <span class="number">2</span>]) &gt;= <span class="number">2</span></span><br><span class="line">)</span><br><span class="line">mx_filterGenes &lt;- mx[filter_genes,]</span><br><span class="line">####################################################################################</span><br><span class="line">####################################################################################</span><br><span class="line">library(DESeq2)</span><br><span class="line">countData &lt;- mx_filterGenes</span><br><span class="line">colData &lt;- design</span><br><span class="line">dds &lt;- DESeqDataSetFromMatrix(countData, colData, design=~Treatment, tidy=TRUE)</span><br><span class="line">norm &lt;- rlog(dds,blind=FALSE)</span><br><span class="line">norm_matrix &lt;- assay(norm)</span><br><span class="line">norm_df &lt;- data.frame(Gene=rownames(norm_matrix), norm_matrix)</span><br><span class="line"><span class="keyword">if</span>(!file.exists(paste(savepath,rnaname,<span class="string">".DESeq2.rlog.mx"</span>,sep=<span class="string">""</span>)))&#123;</span><br><span class="line">  write.table(norm_df, paste(savepath,rnaname,<span class="string">".DESeq2.rlog.mx"</span>,sep=<span class="string">""</span>), row.names = FALSE,sep=<span class="string">"\t"</span>)&#125;</span><br><span class="line">deg &lt;- DESeq(dds)</span><br><span class="line"></span><br><span class="line">res &lt;- results(deg,contrast=c(<span class="string">"Treatment"</span>,<span class="string">"normal"</span>,<span class="string">"Before"</span>),tidy=TRUE)</span><br><span class="line">merged_res &lt;- merge(norm_df,res,by.x=<span class="string">"Gene"</span>,by.y=<span class="string">"row"</span>)</span><br><span class="line"><span class="keyword">if</span>(!file.exists(paste(savepath,rnaname,<span class="string">".NormalvsBefore.DESeq2.tsv"</span>,sep=<span class="string">""</span>)))&#123;</span><br><span class="line">  write.table(merged_res,file=paste(savepath,rnaname,<span class="string">".NormalvsBefore.DESeq2.tsv"</span>,sep=<span class="string">""</span>),sep=<span class="string">"\t"</span>,row.names=FALSE)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">res1 &lt;- results(deg,contrast=c(<span class="string">"Treatment"</span>,<span class="string">"normal"</span>,<span class="string">"After"</span>),tidy=TRUE)</span><br><span class="line">merged_res1 &lt;- merge(norm_df,res1,by.x=<span class="string">"Gene"</span>,by.y=<span class="string">"row"</span>)</span><br><span class="line"><span class="keyword">if</span>(!file.exists(paste(savepath,rnaname,<span class="string">".NormalvsAfter.DESeq2.tsv"</span>,sep=<span class="string">""</span>)))&#123;</span><br><span class="line">  write.table(merged_res1,file=paste(savepath,rnaname,<span class="string">".NormalvsAfter.DESeq2.tsv"</span>,sep=<span class="string">""</span>),sep=<span class="string">"\t"</span>,row.names=FALSE)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">res2 &lt;- results(deg,contrast=c(<span class="string">"Treatment"</span>,<span class="string">"After"</span>,<span class="string">"Before"</span>),tidy=TRUE)</span><br><span class="line">merged_res2 &lt;- merge(norm_df,res2,by.x=<span class="string">"Gene"</span>,by.y=<span class="string">"row"</span>)</span><br><span class="line"><span class="keyword">if</span>(!file.exists(paste(savepath,rnaname,<span class="string">".AftervsBefore.DESeq2.tsv"</span>,sep=<span class="string">""</span>)))&#123;</span><br><span class="line">  write.table(merged_res2,file=paste(savepath,rnaname,<span class="string">".AftervsBefore.DESeq2.tsv"</span>,sep=<span class="string">""</span>),sep=<span class="string">"\t"</span>,row.names=FALSE)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">####################################################################################</span><br><span class="line">####################################################################################</span><br><span class="line">library(edgeR)</span><br><span class="line">#Read Data <span class="keyword">in</span></span><br><span class="line">countData &lt;- mx_filterGenes[,<span class="number">-1</span>]</span><br><span class="line">rownames(countData) &lt;- mx_filterGenes[,<span class="number">1</span>]</span><br><span class="line">design &lt;- read.table(<span class="string">"05.diffexp/design.tsv"</span>,sep=<span class="string">"\t"</span>,header=T)</span><br><span class="line">colData &lt;- design</span><br><span class="line">y &lt;- DGEList(countData, samples=colData, group=colData$Treatment)</span><br><span class="line">y &lt;- calcNormFactors(y)</span><br><span class="line">design &lt;-model.matrix(~Treatment, data=colData)</span><br><span class="line">y &lt;- estimateDisp(y, design)</span><br><span class="line">et &lt;- exactTest(y)</span><br><span class="line">res &lt;- topTags(et,Inf)</span><br><span class="line">tidyResult &lt;- data.frame(Gene=rownames(res$table), res$table)</span><br><span class="line"><span class="keyword">if</span>(!file.exists(paste(savepath,rnaname,<span class="string">".NCvsHCC.edgeR.classic.tsv"</span>,sep=<span class="string">""</span>)))&#123;</span><br><span class="line">  write.table(tidyResult,file=paste(savepath,rnaname,<span class="string">".NCvsHCC.edgeR.classic.tsv"</span>,sep=<span class="string">""</span>),sep=<span class="string">"\t"</span>,row.names=FALSE)&#125;</span><br><span class="line">fit &lt;- glmFit(y,design)</span><br><span class="line">lrt &lt;- glmLRT(fit,contrast = c(<span class="number">1</span>,<span class="number">-1</span>))</span><br><span class="line">FDR &lt;- p.adjust(lrt$table$PValue, method=<span class="string">"BH"</span>)</span><br><span class="line">padj_lrt &lt;- cbind(lrt$table,FDR)</span><br><span class="line">fit_df &lt;- lrt$fitted.values</span><br><span class="line"><span class="keyword">if</span>(!file.exists(paste(savepath,rnaname,<span class="string">".homer.edgeR.TMM.mx"</span>,sep=<span class="string">""</span>)))&#123;</span><br><span class="line">  write.table(fit_df,file = paste(savepath,rnaname,<span class="string">".homer.edgeR.TMM.mx"</span>,sep=<span class="string">""</span>),row.names = T, sep=<span class="string">"\t"</span>, quote=F)&#125;</span><br><span class="line">merged_lrt &lt;- merge(fit_df,padj_lrt,by=<span class="string">"row.names"</span>)</span><br><span class="line">colnames(merged_lrt)[<span class="number">1</span>] &lt;- <span class="string">"Genes"</span></span><br><span class="line"><span class="keyword">if</span>(!file.exists(paste(savepath,rnaname,<span class="string">".NCvsHCC.edgeR.tsv"</span>,sep=<span class="string">""</span>)))&#123;</span><br><span class="line">  write.table(merged_lrt,file =paste(savepath,rnaname,<span class="string">".NCvsHCC.edgeR.tsv"</span>,sep=<span class="string">""</span>),row.names = F, sep=<span class="string">"\t"</span>, quote=F)&#125;</span><br><span class="line">####################################################################################</span><br><span class="line">####################################################################################</span><br><span class="line"></span><br><span class="line">cpmMx &lt;- read.table(paste(commonpath,rnaname,<span class="string">".homer.rpm.m"</span>,sep=<span class="string">""</span>),sep=<span class="string">"\t"</span>,header=T)</span><br><span class="line">filter_cpm &lt;- apply(</span><br><span class="line">  mx[,<span class="number">2</span>:ncol(cpmMx)],</span><br><span class="line">  <span class="number">1</span>,</span><br><span class="line">  function(x) length(x[x &gt; <span class="number">0</span>]) &gt;= <span class="number">2</span></span><br><span class="line">)</span><br><span class="line">mx_filterCPM &lt;- cpmMx[filter_cpm,]</span><br><span class="line">myFun &lt;- function(x)&#123;</span><br><span class="line">  x = <span class="keyword">as</span>.numeric(x)</span><br><span class="line">  v1 = x[<span class="number">2</span>:<span class="number">4</span>]</span><br><span class="line">  v2 = x[<span class="number">5</span>:<span class="number">10</span>]</span><br><span class="line">  out &lt;- wilcox.test(v1,v2,exact = F) </span><br><span class="line">  out &lt;- out$p.value</span><br><span class="line">&#125;</span><br><span class="line">p_value &lt;- apply(mx_filterCPM,<span class="number">1</span>,myFun)</span><br><span class="line">p_value[is.nan(p_value)] &lt;- <span class="number">1</span></span><br><span class="line">FDR &lt;- p.adjust(p_value,method = <span class="string">"BH"</span>)</span><br><span class="line">mx_filterCPM$avgNC &lt;- apply(mx_filterCPM[,<span class="number">2</span>:<span class="number">4</span>],<span class="number">1</span>,mean)</span><br><span class="line">mx_filterCPM$avgHCC &lt;- apply(mx_filterCPM[,<span class="number">5</span>:<span class="number">10</span>],<span class="number">1</span>,mean)</span><br><span class="line">mx_filterCPM$log2fc &lt;- log2((mx_filterCPM$avgNC+<span class="number">0.25</span>)/(mx_filterCPM$avgHCC+<span class="number">0.25</span>))</span><br><span class="line">results &lt;- cbind(mx_filterCPM,p_value,FDR)</span><br><span class="line"><span class="keyword">if</span>(!file.exists(paste(savepath,rnaname,<span class="string">".NCvsHCC.wilcox.tsv"</span>,sep=<span class="string">""</span>)))&#123;</span><br><span class="line">  write.table(results,file = paste(savepath,rnaname,<span class="string">".NCvsHCC.wilcox.tsv"</span>,sep=<span class="string">""</span>),row.names = F, sep=<span class="string">"\t"</span>, quote=F)&#125;</span><br></pre></td></tr></table></figure><h1 id="some-plot-from-DESeq2"><a href="#some-plot-from-DESeq2" class="headerlink" title="some plot from DESeq2"></a>some plot from DESeq2</h1><p>Here is the pdf version of the following codes and its output.<br><div class="row"><iframe src="https://drive.google.com/file/d/1TYMXYXPnZ_1SDT9FQgQUx8g0wy4QXGQm/preview" style="width:100%; height:550px"></iframe></div></p><p>If you can’t see pdf, please try this more interactive way by <a href="https://www.cmwonderland.com/expression.html"><strong>click here</strong></a></p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">### DESeq2</span></span><br><span class="line">&#123;r <span class="attribute">warning</span>=<span class="literal">FALSE</span>&#125;</span><br><span class="line"><span class="comment"># filter expression matrix</span></span><br><span class="line">mx &lt;- read.table(<span class="string">"04.counts/hcc_example.miRNA.homer.ct.mx"</span>,<span class="attribute">sep</span>=<span class="string">"\t"</span>,header=T)</span><br><span class="line"></span><br><span class="line"><span class="comment"># filter genes</span></span><br><span class="line">filter_genes &lt;- apply(</span><br><span class="line">  mx[,2:ncol(mx)],</span><br><span class="line">  1,</span><br><span class="line">  function(x) length(x[x &gt; 2]) &gt;= 2</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">mx_filterGenes &lt;- mx[filter_genes,]</span><br><span class="line"></span><br><span class="line"><span class="comment"># load experimential design</span></span><br><span class="line">design &lt;- read.table(<span class="string">"05.diffexp/design.tsv"</span>,<span class="attribute">sep</span>=<span class="string">"\t"</span>,header=T)</span><br><span class="line"></span><br><span class="line"><span class="comment">#-----------------------------------------</span></span><br><span class="line"><span class="comment"># basic script for normalizing with DESeq2</span></span><br><span class="line">library(DESeq2)</span><br><span class="line"><span class="comment">#Read Data in</span></span><br><span class="line">countData &lt;- mx_filterGenes</span><br><span class="line">colData &lt;- design</span><br><span class="line"></span><br><span class="line"><span class="comment"># generate Dataset</span></span><br><span class="line">dds &lt;- DESeqDataSetFromMatrix(countData, colData, <span class="attribute">design</span>=~Treatment, <span class="attribute">tidy</span>=<span class="literal">TRUE</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># normlize using rlog method</span></span><br><span class="line">norm &lt;- rlog(dds,<span class="attribute">blind</span>=<span class="literal">FALSE</span>)</span><br><span class="line">norm_matrix &lt;- assay(norm)</span><br><span class="line">norm_df &lt;- data.frame(<span class="attribute">Gene</span>=rownames(norm_matrix), norm_matrix)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span>(!file.exists(<span class="string">"05.diffexp/hcc_example.miRNA.homer.DESeq2.rlog.mx"</span>))&#123;</span><br><span class="line">write.table(norm_df, <span class="string">"05.diffexp/hcc_example.miRNA.homer.DESeq2.rlog.mx"</span>, row.names = <span class="literal">FALSE</span>,<span class="attribute">sep</span>=<span class="string">"\t"</span>)&#125;</span><br><span class="line"></span><br><span class="line">deg &lt;- DESeq(dds)</span><br><span class="line">res &lt;- results(deg,<span class="attribute">tidy</span>=<span class="literal">TRUE</span>)</span><br><span class="line">merged_res &lt;- merge(norm_df,res,by.<span class="attribute">x</span>=<span class="string">"Gene"</span>,by.y="row")</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span>(!file.exists(<span class="string">"05.diffexp/hcc_example.miRNA.NCvsHCC.DESeq2.tsv"</span>))&#123;</span><br><span class="line">write.table(merged_res,<span class="attribute">file</span>=<span class="string">"05.diffexp/hcc_example.miRNA.NCvsHCC.DESeq2.tsv"</span>,sep="\t",row.names=FALSE)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>the codes use DESeq2 and produces files:</p><p>05.diffexp/hcc_example.miRNA.homer.DESeq2.rlog.mx</p><p>05.diffexp/hcc_example.miRNA.NCvsHCC.DESeq2.tsv</p><h2 id="edgeR"><a href="#edgeR" class="headerlink" title="edgeR"></a>edgeR</h2><figure class="highlight plain"><figcaption><span>warning</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><span class="line"># 0.filter expression matrix</span><br><span class="line">mx &lt;- read.table(&quot;04.counts/hcc_example.miRNA.homer.ct.mx&quot;,sep=&quot;\t&quot;,header=T)</span><br><span class="line"></span><br><span class="line"># filter genes</span><br><span class="line">filter_genes &lt;- apply(</span><br><span class="line">    mx[,2:ncol(mx)],</span><br><span class="line">    1,</span><br><span class="line">    function(x) length(x[x &gt; 2]) &gt;= 2</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">mx_filterGenes &lt;- mx[filter_genes,]</span><br><span class="line"></span><br><span class="line"># load experimential design</span><br><span class="line">design &lt;- read.table(&quot;05.diffexp/design.tsv&quot;,sep=&quot;\t&quot;,header=T)</span><br><span class="line"></span><br><span class="line">#--------------------------------</span><br><span class="line"># basic script for running edgeR</span><br><span class="line">library(edgeR)</span><br><span class="line">#Read Data in</span><br><span class="line">countData &lt;- mx_filterGenes[,-1]</span><br><span class="line">rownames(countData) &lt;- mx_filterGenes[,1]</span><br><span class="line">design &lt;- read.table(&quot;05.diffexp/design.tsv&quot;,sep=&quot;\t&quot;,header=T)</span><br><span class="line">colData &lt;- design</span><br><span class="line"></span><br><span class="line"># generate DGE object</span><br><span class="line">y &lt;- DGEList(countData, samples=colData, group=colData$Treatment)</span><br><span class="line">y &lt;- calcNormFactors(y)</span><br><span class="line"></span><br><span class="line">#Estimate Error Model</span><br><span class="line">design &lt;-model.matrix(~Treatment, data=colData)</span><br><span class="line">y &lt;- estimateDisp(y, design)</span><br><span class="line"></span><br><span class="line"># classic methods: compute p-values, then output</span><br><span class="line">et &lt;- exactTest(y)</span><br><span class="line">res &lt;- topTags(et,Inf)</span><br><span class="line">tidyResult &lt;- data.frame(Gene=rownames(res$table), res$table)</span><br><span class="line"></span><br><span class="line">if(!file.exists(&quot;05.diffexp/hcc_example.miRNA.NCvsHCC.edgeR.classic.tsv&quot;))&#123;</span><br><span class="line">write.table(tidyResult,file=&quot;05.diffexp/hcc_example.miRNA.NCvsHCC.edgeR.classic.tsv&quot;,sep=&quot;\t&quot;,row.names=FALSE)&#125;</span><br><span class="line"></span><br><span class="line"># Generalized linear models</span><br><span class="line">fit &lt;- glmFit(y,design)</span><br><span class="line"># likelihood ratio test</span><br><span class="line">lrt &lt;- glmLRT(fit,contrast = c(1,-1))</span><br><span class="line">FDR &lt;- p.adjust(lrt$table$PValue, method=&quot;BH&quot;)</span><br><span class="line">padj_lrt &lt;- cbind(lrt$table,FDR)</span><br><span class="line">fit_df &lt;- lrt$fitted.values</span><br><span class="line">if(!file.exists(&quot;05.diffexp/hcc_example.miRNA.homer.edgeR.TMM.mx&quot;))&#123;</span><br><span class="line">write.table(fit_df,file = &quot;05.diffexp/hcc_example.miRNA.homer.edgeR.TMM.mx&quot;,row.names = T, sep=&quot;\t&quot;, quote=F)&#125;</span><br><span class="line">merged_lrt &lt;- merge(fit_df,padj_lrt,by=&quot;row.names&quot;)</span><br><span class="line">colnames(merged_lrt)[1] &lt;- &quot;Genes&quot;</span><br><span class="line"></span><br><span class="line">if(!file.exists(&quot;05.diffexp/hcc_example.miRNA.NCvsHCC.edgeR.tsv&quot;))&#123;</span><br><span class="line">write.table(merged_lrt,file =&quot;05.diffexp/hcc_example.miRNA.NCvsHCC.edgeR.tsv&quot;,row.names = F, sep=&quot;\t&quot;, quote=F)&#125;</span><br></pre></td></tr></table></figure><p>the codes use edgeR and produces files:</p><p>05.diffexp/hcc_example.miRNA.NCvsHCC.edgeR.classic.tsv</p><p>05.diffexp/hcc_example.miRNA.homer.edgeR.TMM.mx</p><p>05.diffexp/hcc_example.miRNA.NCvsHCC.edgeR.tsv</p><h2 id="Wilcox-Mann-Whitney-U-Test"><a href="#Wilcox-Mann-Whitney-U-Test" class="headerlink" title="Wilcox/Mann-Whitney-U Test"></a>Wilcox/Mann-Whitney-U Test</h2><h3 id="1-normalize-the-reads-by-library-size-edgeR"><a href="#1-normalize-the-reads-by-library-size-edgeR" class="headerlink" title="1. normalize the reads by library size (edgeR)"></a>1. normalize the reads by library size (edgeR)</h3><h3 id="2-identify-differential-expressed-gene-using-wilcoxon-test"><a href="#2-identify-differential-expressed-gene-using-wilcoxon-test" class="headerlink" title="2. identify differential expressed gene using wilcoxon.test()"></a>2. identify differential expressed gene using wilcoxon.test()</h3><figure class="highlight plain"><figcaption><span>warning</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">cpmMx &lt;- read.table(&quot;04.counts/hcc_example.miRNA.homer.rpm.mx&quot;,sep=&quot;\t&quot;,header=T)</span><br><span class="line">filter_cpm &lt;- apply(</span><br><span class="line">    mx[,2:ncol(cpmMx)],</span><br><span class="line">    1,</span><br><span class="line">    function(x) length(x[x &gt; 0]) &gt;= 2</span><br><span class="line">)</span><br><span class="line">mx_filterCPM &lt;- cpmMx[filter_cpm,]</span><br><span class="line"></span><br><span class="line">myFun &lt;- function(x)&#123;</span><br><span class="line">  x = as.numeric(x)</span><br><span class="line">  v1 = x[2:4]</span><br><span class="line">  v2 = x[5:10]</span><br><span class="line">  out &lt;- wilcox.test(v1,v2,exact = F) </span><br><span class="line">  out &lt;- out$p.value</span><br><span class="line">&#125;</span><br><span class="line">p_value &lt;- apply(mx_filterCPM,1,myFun)</span><br><span class="line">p_value[is.nan(p_value)] &lt;- 1</span><br><span class="line">FDR &lt;- p.adjust(p_value,method = &quot;BH&quot;)</span><br><span class="line">mx_filterCPM$avgNC &lt;- apply(mx_filterCPM[,2:4],1,mean)</span><br><span class="line">mx_filterCPM$avgHCC &lt;- apply(mx_filterCPM[,5:10],1,mean)</span><br><span class="line">mx_filterCPM$log2fc &lt;- log2((mx_filterCPM$avgNC+0.25)/(mx_filterCPM$avgHCC+0.25))</span><br><span class="line">results &lt;- cbind(mx_filterCPM,p_value,FDR)</span><br><span class="line">if(!file.exists(&quot;05.diffexp/hcc_example.miRNA.NCvsHCC.wilcox__.tsv&quot;))&#123;</span><br><span class="line">write.table(results,file = &quot;05.diffexp/hcc_example.miRNA.NCvsHCC.wilcox__.tsv&quot;,row.names = F, sep=&quot;\t&quot;, quote=F)&#125;</span><br></pre></td></tr></table></figure><p>the codes use wilconx test and produces files:</p><p>05.diffexp/hcc<em>example.miRNA.NCvsHCC.wilcox</em>.tsv</p><h2 id="exploring-results"><a href="#exploring-results" class="headerlink" title="exploring results"></a>exploring results</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"># filter expression matrix</span><br><span class="line">mx &lt;- read.table(&quot;04.counts/hcc_example.miRNA.homer.ct.mx&quot;,sep=&quot;\t&quot;,header=T)</span><br><span class="line"></span><br><span class="line"># filter genes</span><br><span class="line">filter_genes &lt;- apply(</span><br><span class="line">  mx[,2:ncol(mx)],</span><br><span class="line">  1,</span><br><span class="line">  function(x) length(x[x &gt; 2]) &gt;= 2</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">mx_filterGenes &lt;- mx[filter_genes,]</span><br><span class="line"></span><br><span class="line"># load experimential design</span><br><span class="line">design &lt;- read.table(&quot;05.diffexp/design.tsv&quot;,sep=&quot;\t&quot;,header=T)</span><br><span class="line"></span><br><span class="line">#-----------------------------------------</span><br><span class="line"># basic script for normalizing with DESeq2</span><br><span class="line">library(DESeq2)</span><br><span class="line">#Read Data in</span><br><span class="line">countData &lt;- mx_filterGenes</span><br><span class="line">colData &lt;- design</span><br><span class="line"></span><br><span class="line"># generate Dataset</span><br><span class="line">dds &lt;- DESeqDataSetFromMatrix(countData, colData, design=~Treatment, tidy=TRUE)</span><br><span class="line"></span><br><span class="line"># normlize using rlog mathod</span><br><span class="line">norm &lt;- rlog(dds,blind=FALSE)</span><br><span class="line">norm_matrix &lt;- assay(norm)</span><br><span class="line"></span><br><span class="line"># diff-exp analysis using DESeq2</span><br><span class="line">deg &lt;- DESeq(dds)</span><br><span class="line">res &lt;- results(deg,tidy=TRUE)</span><br></pre></td></tr></table></figure><h4 id="Heatmap-for-count-matrix"><a href="#Heatmap-for-count-matrix" class="headerlink" title="Heatmap for count matrix"></a>Heatmap for count matrix</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">library(&quot;pheatmap&quot;)</span><br><span class="line">select &lt;- order(rowMeans(counts(deg,normalized=TRUE)), decreasing=TRUE)[1:25]</span><br><span class="line">df &lt;- as.data.frame(colData(deg)[,c(&quot;Treatment&quot;,&quot;Sample&quot;)])</span><br><span class="line">pheatmap(assay(norm)[select,], cluster_rows=FALSE, show_rownames=FALSE, cluster_cols=TRUE, annotation_col=df)</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;It contains my assignment in Bioinformatics course talking about Differential Expression and this semesters’ code&lt;br&gt;
    
    </summary>
    
      <category term="techniques" scheme="https://www.cmwonderland.com/categories/techniques/"/>
    
    
      <category term="assignment" scheme="https://www.cmwonderland.com/tags/assignment/"/>
    
      <category term="techniques" scheme="https://www.cmwonderland.com/tags/techniques/"/>
    
      <category term="bioinformatics" scheme="https://www.cmwonderland.com/tags/bioinformatics/"/>
    
  </entry>
  
  <entry>
    <title>分享一个微信机器人</title>
    <link href="https://www.cmwonderland.com/2018/05/16/wechat/"/>
    <id>https://www.cmwonderland.com/2018/05/16/wechat/</id>
    <published>2018-05-17T01:13:27.000Z</published>
    <updated>2018-05-16T13:14:35.997Z</updated>
    
    <content type="html"><![CDATA[<p>去年最火的微信机器人似乎是<a href="https://github.com/littlecodersh/ItChat" target="_blank" rel="noopener">GitHub - littlecodersh/ItChat: A complete and graceful API for Wechat. 微信个人号接口、微信机器人及命令行微信，三十行即可自定义个人号机器人。</a>，尤其是这种文章<a href="https://zhuanlan.zhihu.com/p/25670072" target="_blank" rel="noopener">用微信控制深度学习训练：中国特色的keras插件</a>还是相当有趣的。今天又看到一个功能很全的微信接口<a href="https://github.com/youfou/wxpy" target="_blank" rel="noopener">https://github.com/youfou/wxpy</a></p><a id="more"></a><p>功能非常多，不过有一些bug，比如会多次重复弹出二维码扫描。另外因为中文编码的不友好，用来处理些中文信息还是挺不顺畅的。</p><p>Share一下用来获取所有好友的个性签名的代码，比一篇微信文章的教程多了一些东西，换成了numpy，把好友名字对应起来了。</p><figure class="highlight xl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"># coding=utf-<span class="number">8</span></span><br><span class="line">from wxpy <span class="keyword">import</span> *</span><br><span class="line"># 初始化机器人，扫码登陆</span><br><span class="line">bot = Bot()</span><br><span class="line"># 获取所有好友</span><br><span class="line">my_friends = bot.friends()</span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line">filterdata= &#123;&#125;</span><br><span class="line"><span class="keyword">for</span> i <span class="built_in">in</span> range(len(my_friends)):</span><br><span class="line">    friend = my_friends[i]</span><br><span class="line">    #pattern = re.compile(r<span class="string">'[一-龥]+'</span>)</span><br><span class="line">    filterdata[i] = friend.signature</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">sig = np.array([filterdata[i] <span class="keyword">for</span> i <span class="built_in">in</span> range(len(my_friends))])</span><br><span class="line">induse = np.<span class="keyword">where</span>(sig !=<span class="string">''</span>)[<span class="number">0</span>]</span><br><span class="line"><span class="built_in">sign</span> = np.ndarray([induse.shape[<span class="number">0</span>],<span class="number">2</span>]).astype(<span class="string">'str'</span>)</span><br><span class="line"><span class="keyword">for</span> i <span class="built_in">in</span> range(induse.shape[<span class="number">0</span>]):</span><br><span class="line">    <span class="built_in">sign</span>[i,<span class="number">0</span>] = my_friends[induse[i]].<span class="keyword">name</span></span><br><span class="line">    <span class="built_in">sign</span>[i,<span class="number">1</span>] = sig[induse[i]]</span><br><span class="line">np.savetxt(<span class="string">'signature'</span>,<span class="built_in">sign</span>,fmt=<span class="string">'%s'</span>)#,encoding=<span class="string">'utf-8'</span>)</span><br></pre></td></tr></table></figure><p>看了一遍有签名的的好友的微信签名，蛮有意思的，然后浏览了一下wxpy的文档<a href="http://wxpy.readthedocs.io/zh/latest/" target="_blank" rel="noopener">wxpy: 用 Python 玩微信 — wxpy 0.3.9.8 文档</a>，陷入沉思：<strong>这玩意儿就是为微商量身打造的垃圾信息骚扰利器，，，</strong></p><p>资源：<a href="https://mp.weixin.qq.com/s/hGptTYZnO4DVPTshgiJZmg" target="_blank" rel="noopener">人工智能头条/用Python更加了解微信好友</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;去年最火的微信机器人似乎是&lt;a href=&quot;https://github.com/littlecodersh/ItChat&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;GitHub - littlecodersh/ItChat: A complete and graceful API for Wechat. 微信个人号接口、微信机器人及命令行微信，三十行即可自定义个人号机器人。&lt;/a&gt;，尤其是这种文章&lt;a href=&quot;https://zhuanlan.zhihu.com/p/25670072&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;用微信控制深度学习训练：中国特色的keras插件&lt;/a&gt;还是相当有趣的。今天又看到一个功能很全的微信接口&lt;a href=&quot;https://github.com/youfou/wxpy&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://github.com/youfou/wxpy&lt;/a&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="techniques" scheme="https://www.cmwonderland.com/categories/techniques/"/>
    
    
      <category term="codes" scheme="https://www.cmwonderland.com/tags/codes/"/>
    
      <category term="techniques" scheme="https://www.cmwonderland.com/tags/techniques/"/>
    
  </entry>
  
  <entry>
    <title>Multivariate Statistics Assignment 4</title>
    <link href="https://www.cmwonderland.com/2018/05/16/multi1/"/>
    <id>https://www.cmwonderland.com/2018/05/16/multi1/</id>
    <published>2018-05-16T16:59:19.000Z</published>
    <updated>2018-05-21T14:14:39.228Z</updated>
    
    <content type="html"><![CDATA[<p>The fourth assignment of Multivariate Statistics. The assignment is written in Rmarkdown, a smart syntax supported by RStudio helping with formula, plot visualization and plugin codes running.</p><a id="more"></a><blockquote><p><strong>most recommend:</strong> <a href="https://www.cmwonderland.com/multihw4.html"><strong>click here</strong></a> for <strong>html version</strong> of assignment, you can see codes as well as plots.</p></blockquote><p>You may also find the <a href="https://github.com/james20141606/somethingmore/tree/master/multi-variable_pdf" target="_blank" rel="noopener"><strong>PDF Version</strong></a> of this assignment from github. <strong>Or if you can cross the fire wall, just see below</strong>:<br><div class="row"><iframe src="https://drive.google.com/file/d/1w9ByeBbuVBKFxahOIrwu2oZFuFXydeKa/preview" style="width:100%; height:550px"></iframe></div></p><h1 id="1"><a href="#1" class="headerlink" title="1"></a>1</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">mp &lt;- matrix(c(5,2,2,2),2,2,byrow=T) </span><br><span class="line">eigen(mp)</span><br></pre></td></tr></table></figure><script type="math/tex; mode=display">\text{The eigenvalue-eigenvector pairs are } \lambda_1 =6, e_1 =  [\frac{2}{\sqrt5},\frac{1}{\sqrt5}]; \lambda_2 =1, e_2 =  [-\frac{1}{\sqrt5},\frac{2}{\sqrt5}].\\\text{Therefore, the principle componenets become: }\\Y_1 = e_1^T X = \frac{2}{\sqrt5}X_1 +\frac{1}{\sqrt5}X_2\\Y_1 = e_2^T X = -\frac{1}{\sqrt5}X_1 +\frac{2}{\sqrt5}X_2\\\text{The total population variance explained by first principal component is:}\\\frac{var(Y_1)}{var(Y_1)+var(Y_2)} = \frac{\lambda_1}{\lambda_1+\lambda_2} = \frac{6}{1+6} \approx 85.71\%</script><h1 id="2"><a href="#2" class="headerlink" title="2"></a>2</h1><h2 id="a"><a href="#a" class="headerlink" title="a"></a>a</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cov2cor(mp)</span><br></pre></td></tr></table></figure><script type="math/tex; mode=display">\text{The correlation matrix } \rho = \left[\begin{matrix}1 & \sqrt\frac{2}{5} \\\sqrt\frac{2}{5}&1\end{matrix}\right]\\\text{The eigenvalue-eigenvector pairs are }\\\lambda_1= \frac{5+\sqrt {10}}{5}, \ e_1=\left[\begin{matrix}\frac{1}{\sqrt 2} \\\frac{1}{\sqrt 2}\end{matrix}\right]\\\lambda_1= \frac{5-\sqrt {10}}{5}, \ e_1=\left[\begin{matrix}-\frac{1}{\sqrt 2} \\\frac{1}{\sqrt 2}\end{matrix}\right]</script><script type="math/tex; mode=display">Let \ \mathbf{Z_i} = \frac{\mathbf{X_i}-\mu_i}{\sqrt {\sigma_{ii}}}, i =1,...,p. \text{The principal components become:}\\Y_1 = e_1^T Z = \frac{1}{\sqrt2}Z_1 +\frac{1}{\sqrt2}Z_2\\Y_1 = e_2^T Z = -\frac{1}{\sqrt2}Z_1 +\frac{1}{\sqrt2}Z_2\\\text{The total population variance explained by first principal component is:}\\\frac{var(Y_1)}{var(Y_1)+var(Y_2)} = \frac{\lambda_1}{\lambda_1+\lambda_2} = \frac{5+\sqrt{10}}{10} \approx 81.6\%</script><h2 id="b"><a href="#b" class="headerlink" title="b"></a>b</h2><p>The principal components of Z obtained from the eigenvectors of the correlation<br>matrix ρ of X is different from those calculated from covariance matrix $\Sigma$. Because<br>the eigen pairs derived from $\Sigma$, in general not the same as the ones derived from $\rho$</p><h2 id="c"><a href="#c" class="headerlink" title="c"></a>c</h2><p>THe correlations between $Y_j$ and $Z_i$ are:</p><script type="math/tex; mode=display">\rho_{Y_1,Z_1} = e_{11}\sqrt {\lambda_1} = \frac{1}{\sqrt2}\sqrt \frac{5+\sqrt {10}}{5} \approx0.903 \\\rho_{Y_1,Z_2} = e_{12}\sqrt {\lambda_1} = \frac{1}{\sqrt2}\sqrt \frac{5+\sqrt {10}}{5} \approx0.903 \\\rho_{Y_2,Z_1} = e_{21}\sqrt {\lambda_2} = - \frac{1}{\sqrt2}\sqrt \frac{5-\sqrt {10}}{5} \approx -0.429</script><h1 id="3"><a href="#3" class="headerlink" title="3"></a>3</h1><h2 id="a-1"><a href="#a-1" class="headerlink" title="a"></a>a</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"># read the data</span><br><span class="line">setwd(&apos;~/Desktop/三春/3多元统计分析/作业/作业4/&apos;)</span><br><span class="line">dat&lt;-read.csv(&quot;table8.4.csv&quot;)</span><br><span class="line">X1&lt;-dat$x1</span><br><span class="line">X2&lt;-dat$x2</span><br><span class="line">X3&lt;-dat$x3</span><br><span class="line">X4&lt;-dat$x4</span><br><span class="line">X5&lt;-dat$x5</span><br><span class="line">covar &lt;- cov(dat)</span><br><span class="line">covar</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">eigen(cov(dat))</span><br><span class="line">prcomp(dat)</span><br><span class="line">summary(prcomp(dat))</span><br></pre></td></tr></table></figure><script type="math/tex; mode=display">Y_1 = e_1^T X= -0.2228228 X_1  -0.3072900 X_2 -0.1548103 X_3 -0.6389680 X_4 -0.6509044 X_5\\Y_2 = e_2^T X = 0.6252260 X_1 + 0.5703900 X_2 +0.3445049 X_3 -0.2479475 X_4 -0.3218478 X_5\\Y_3 = e_3^T X= -0.32611218 X_1 +  0.24959014 X_2 +0.03763929 X_3 +0.64249741 X_4 -0.64586064 X_5\\Y_4 = e_4^T X=  0.6627590 X_1 -0.4140935X_2 -0.4970499 X_3 +0.3088689 X_4 -0.2163758 X_5\\Y_5 = e_5^T X=-0.11765952 X_1 +0.58860803 X_2 -0.78030428 X_3 -0.14845546 X_4 +0.09371777 X_5</script><h2 id="b-1"><a href="#b-1" class="headerlink" title="b"></a>b</h2><p>From the summary above, the proportion of the total sample variance explained by the rst<br>three principal components is: 89.881%. It means that the first three explain almost all<br>variance.</p><h2 id="c-1"><a href="#c-1" class="headerlink" title="c"></a>c</h2><p>From 8-33, we have the CI of m $\lambda_i$:</p><script type="math/tex; mode=display">[\frac{\hat \lambda_i}{1+z(\alpha/2m)\sqrt{2/n}},\frac{\hat \lambda_i}{1-z(\alpha/2m)\sqrt{2/n}}]</script><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">z &lt;-qnorm(1-1/6)</span><br><span class="line">cical &lt;-function(lambda)&#123;</span><br><span class="line">  c(lambda/(1+z*(1/103)**0.5),lambda/(1-z*(1/103)**0.5))</span><br><span class="line">&#125;</span><br><span class="line">cical(0.0013676780)</span><br><span class="line">cical(0.0007011596) </span><br><span class="line">cical(0.0002538024)</span><br></pre></td></tr></table></figure><p>CIs are: [0.001248653 0.001511786], [0.0006401396 0.0007750385], [0.0002317147 0.0002805447]</p><h2 id="d"><a href="#d" class="headerlink" title="d"></a>d</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">plot(c(0.52926, 0.80059, 0.89881, 0.95399, 1.00000),ylab=&quot;Cumulative proportion&quot;,xlab=&quot;Component number&quot;,type=&apos;b&apos;)</span><br></pre></td></tr></table></figure><p>From the cumulative proportion plot, it seems that three dimensions’ PC are enough.</p><h1 id="4"><a href="#4" class="headerlink" title="4"></a>4</h1><h2 id="a-2"><a href="#a-2" class="headerlink" title="a"></a>a</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">library(bootstrap)</span><br><span class="line">data(scor)</span><br><span class="line">plot(scor)</span><br></pre></td></tr></table></figure><h2 id="b-2"><a href="#b-2" class="headerlink" title="b"></a>b</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cor(scor)</span><br></pre></td></tr></table></figure><h2 id="c-2"><a href="#c-2" class="headerlink" title="c"></a>c</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">prcomp(scor)</span><br><span class="line">summary(prcomp(scor))</span><br></pre></td></tr></table></figure><script type="math/tex; mode=display">Y_1 = e_1^T X= -0.5054457 X_1  -0.3683486 X_2 -0.3456612 X_3 -0.4511226 X_4 -0.5346501 X_5\\Y_2 = e_2^T X = -0.74874751 X_1 -0.20740314 X_2 + 0.07590813 X_3 +0.30088849 X_4 +0.54778205 X_5\\Y_3 = e_3^T X= 0.2997888 X_1   -0.4155900 X_2 -0.1453182 X_3 -0.5966265 X_4 +0.6002758 X_5\\Y_4 = e_4^T X=  -0.296184264 X_1 + 0.78288817X_2 +0.003236339 X_3 -0.518139724 X_4 +0.175732020 X_5\\Y_5 = e_5^T X= -0.07939388 X_1 -0.18887639 X_2 +0.92392015 X_3 -0.28552169 X_4 -0.15123239 X_5</script><h2 id="d-1"><a href="#d-1" class="headerlink" title="d"></a>d</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">plot(c( 0.6191,0.8013 ,0.8948 ,0.97102, 1.00000),ylab=&quot;Cumulative proportion&quot;,xlab=&quot;Component number&quot;,type=&apos;b&apos;)</span><br></pre></td></tr></table></figure><p>I will choose the first too for these three PCs take almost 80% of total variance.</p><h2 id="e"><a href="#e" class="headerlink" title="e"></a>e</h2><p>PC1 may stand for the indicator of scores on all subjects. PC2 has more straightforward mearning: it is related to closed or open rules.</p><h2 id="f"><a href="#f" class="headerlink" title="f"></a>f</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">library(&apos;ggfortify&apos;)</span><br><span class="line">autoplot(prcomp(scor,scale=TRUE),colour=&apos;green&apos;,label=TRUE)</span><br></pre></td></tr></table></figure><h2 id="g"><a href="#g" class="headerlink" title="g"></a>g</h2><p>$\chi^2_2(0.05) = 5.99$<br>I use python to check the outlier:<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> sklearn.decomposition <span class="keyword">import</span> PCA</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">convert</span><span class="params">(strr)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> np.array(strr.split(<span class="string">' '</span>)).astype(<span class="string">'float'</span>).reshape(<span class="number">-1</span>,<span class="number">1</span>)</span><br><span class="line">pca = PCA(n_components=<span class="number">2</span>, svd_solver=<span class="string">'full'</span>)</span><br><span class="line">dat = pca.fit_transform(data)</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">ellipse</span><span class="params">(i)</span>:</span></span><br><span class="line">    x,y = dat[i,<span class="number">0</span>],dat[i,<span class="number">1</span>]</span><br><span class="line">    a =  (x/<span class="number">26.2105</span>)**<span class="number">2</span> + (y/<span class="number">14.2166</span>)**<span class="number">2</span></span><br><span class="line">    <span class="keyword">if</span> a &gt;=<span class="number">5.99</span>:</span><br><span class="line">        <span class="keyword">print</span> (i,a)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(data.shape[<span class="number">0</span>]):</span><br><span class="line">    ellipse(i)</span><br></pre></td></tr></table></figure></p><p>And we can find eight outliers: 1,2,23,28,66,76,81,87</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;The fourth assignment of Multivariate Statistics. The assignment is written in Rmarkdown, a smart syntax supported by RStudio helping with formula, plot visualization and plugin codes running.&lt;/p&gt;
    
    </summary>
    
      <category term="school work" scheme="https://www.cmwonderland.com/categories/school-work/"/>
    
    
      <category term="codes" scheme="https://www.cmwonderland.com/tags/codes/"/>
    
      <category term="R" scheme="https://www.cmwonderland.com/tags/R/"/>
    
      <category term="assignment" scheme="https://www.cmwonderland.com/tags/assignment/"/>
    
      <category term="statistics" scheme="https://www.cmwonderland.com/tags/statistics/"/>
    
      <category term="linear regression" scheme="https://www.cmwonderland.com/tags/linear-regression/"/>
    
  </entry>
  
  <entry>
    <title>J-1 Visa申请流程回忆</title>
    <link href="https://www.cmwonderland.com/2018/05/15/J-1-Visa/"/>
    <id>https://www.cmwonderland.com/2018/05/15/J-1-Visa/</id>
    <published>2018-05-16T02:32:38.000Z</published>
    <updated>2018-05-21T15:38:03.381Z</updated>
    
    <content type="html"><![CDATA[<p>5.14下午去美国大使馆非移民签证处办了签证，虽然很折腾但是比较顺利，把过程记录一下。</p><p>首先是漫长的准备阶段，参考了去年和以前的去芝加哥的学长学姐的各种流程和资料，网上搜罗的各种经验（靠谱的不靠谱的）甚至看了看youtube的面试指南。办签证这件事网上的信息实在是非常杂乱，信息量很低，包括很多out of date的信息，需要大量的过滤。</p><p>办签证的过程一定要记好笔记，因为这可能是个持续数月的艰苦工程，笔记太凌乱就不放上来了，现在简单总结下重要的材料和节点</p><iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width="330" height="86" src="//music.163.com/outchain/player?type=2&id=28828120&auto=1&height=66"></iframe><a id="more"></a><ul><li>办<strong>护照</strong>，早点去办，和其他环节独立，带上交通时间花费不会超过三个小时，提前填好表格带过去就行，攻略很多</li><li>配合对方负责人填写各种表格，基本对方给什么就填什么，该签字就签字扫描再传回去，Harvard还有网上填的一个表格，然后等待对方审核最后把<strong>DS7002和2019</strong>寄回来，学校还寄了其他的一堆指南，不过有个bug是对方导师没在DS7002签字，我专门又要了签名扫描传过来，没用上。这个过程和校内的批件什么的是独立的，和对方学校联系好就可以办了，持续时间将近两个月。。。包括Harvard的国际处从审核开始到联邦快递寄过来就花了半个月。</li><li>学校的<strong>派出证明、在学证明、资助证明</strong>。资助证明和派出证明还是有用的，作为J-1签证申请者，有学校资助证明就不需要再用什么财产证明了（所以之前爸妈弄得财产证明什么的也没什么用）。要开这几个证明就要先办好<strong>批件</strong>，批件可以先填着各种表格，找各种地方签字，以前的资料都可以参考，芝加哥的指南最详细，不过要注意各个负责老师换的很频繁，以前的经验里的老师们大多数都不再负责了，可以问问最近的同学或者找刘栋老师办的时候顺便问一下其他几个负责的老师。表格各种签好字扫描上传，加上对方导师发过来的邀请信就可以完成批件资料了，等着国际处几天审核好，去李兆基楼取就好了，取的时候顺手打印好资助证明，自己在上面填金额，要符合对方的要求，比如（2333$ /month），然后让国际处老师盖个章，顺手还可以要一个保险的明信片。</li><li>准备其他材料，<strong>比如去照符合美签要求的照片，自己、导师的CV准备好，写好过去之后的工作计划，日程表</strong>。</li><li>开始准备面签流程，现在真的比过去简化太多了，<strong>交SEVIS FEE，留好确认收据</strong>，填DS160表，网上各种指南，填着也不是很麻烦，然后<strong>打印DS160确认页</strong>，然后缴费，用中信银行交就行，我当时用手机银行交的，没有收据，因为处于很担心因为手续不全耽误签证又不敢确信是否需要的心情下，就专门去中信银行开了签证费收据，后来发现面签的时候完全没用，并不需要，但是报销应该需要。因为现在各种信息都联网了，所以扫一下你的DS160的码就知道了，各种收费的收据也不需要看了。</li><li>选一个面签时间，我选的是周一下午3：15的场</li></ul><p>简单总结一下材料：</p><ul><li><strong>DS-160确认页 【彩印】，面签确认页，SEVIS FEE收据</strong></li><li><strong>DS7002 DS2019，护照，美签标准照片</strong> </li><li><strong>派出许可、资助证明 ， 批件</strong></li><li><strong>邀请信， 自己，教授CV，实验室网页，国内外导师网页介绍， 托福成绩单，英文成绩单  ，日程计划，proposal</strong> </li><li><strong>户口本复印件，身份证，学生证，在学证明</strong></li><li><strong>资产证明，全家福照片，父母的在职证明&amp;收入证明</strong></li></ul><p>以上这些，除了护照和DS2019，面签的时候我一个都没用上。。。</p><p><strong>面签当日流程：</strong></p><p>上午仔细总结梳理了整个流程和材料，处于一种相当担惊受怕的心情，很怕被check了耽误出国，尤其是最近发生了好几起暑研办签证被check的现象，如果check一个月的话怕是真的会耽误。。</p><p>中午一点出发，大概两点到，站了一路，北京当天的气温是三十多度，今年第一次上三十吧，相当热了，然后愚蠢的百度地图导航错误，走到了大使馆的另一侧，于是又绕了一公里才找到了非移民签证处，人头攒动，很多人堵着在等着面签。按照经验去马路斜对面的一个办事处花了三十元存包，然后去了另一个大楼上个卫生间，，，（真是相当奇葩，附近大概没有其他可以去卫生间的地方和存东西的地方，而大使馆里没有坐的地方，真是毫无人权，，）</p><p>先要在外面排队，排队非常非常的混乱，靠纸牌子显示时间，，而且虽然预约单上的最晚时间已经到了，但是显然由于低效率，纸牌子上的时间远远慢于当前时间，但是后来发现好像也无所谓，，在拥挤的人群中在太阳地下站了大概一个小时，脑子接近短路，然后进去，经过几关之后，到一个窗口先扫描了160上的码登记了信息，不同的队伍效率差距挺大的，然后去扫了指纹，然后就上去开始排队准备面签了，两个厅，大概一个厅几十个人，面签的时候隔着玻璃，毫无尊严地站着面签。。</p><p>中间发生了一个非常非常严重的插曲，刚排没多久，一个女人就开始大哭，应该是被拒了，然后崩溃，对着签证官大哭大吼，说什么又被拒了，要把肚子里的孩子打掉，场面混乱至极，女人赖着不走，所有人在一片哭闹中默默地等待面签，我不知道多少人如果被拒绝或者check了会在事后归结于这个女的身上。我觉得这个女人真的是极其极其的自私。ok你想去美国生孩子我就不说什么了，崇洋媚外也行吧，谁让我国跪舔外国人清华北大对外国人也优待呢。你八个月了要出去花几十万买个美国籍，全家人明明是中国人，估计英语都不怎么会说，当签证官傻吗，，，这也算了，你出不去就威胁说你要把孩子打掉，以此威胁签证官说他们没人性？？拜托，，，我觉得在场所有人都石化了，，然后不断的有安保和工作人员和级别更高的美国人过来，无奈该女性一直赖在窗口不走。然后我果然运气很差地被分到了隔壁窗口，还是刚才经历了这么糟糕的经历的签证官来面试我。</p><p>面试我的签证官是华裔，而且据排队时观察，很多签证官都中英文切换顺利，不过作为J-1签证申请者，还是主动地选择了用英语。。</p><p>不过面试的经历极其简单，大概如下：</p><ul><li>Me: good afternoon, how are you doing</li><li>Official: fine, thank you （内心独白可能是，刚才被孕妇闹得，不知道得有多少事，你还问我感觉怎么样）</li><li>Official: Could you give me your passport and DS2019</li><li>Me: Sure</li><li>Official: Wow, you must be very smart, Tsinghua, Harvard</li><li>Me: Thank you（没想到会说这个，完全忘了怎么回答）</li><li>Official: So it is your “Dasan? Daer?”（又一个奇怪的但是经常出现的问题，大概因为太多人听不懂大一大二大三的英文，面试官已经习惯直接说拼音了，但是当时处在思维定式专注地等着听英语的我楞了一下）</li><li>Me: dasan</li><li>Official: what’s your plan after you return to China（一个坑问题，看看你会不会有留在美国的心思）</li><li>Me: I think I will finish my undergraduate study to get my bachelor degree, and then I will continue my phd program in Tsinghua in my current lab（瞎扯就好了）</li></ul><p>然后他竟然就懒得再问了，直接在DS2019上面签字，递蓝条子，说:congratulations, you pass!</p><p>于是只花了两分钟就面试完了，真的非常顺利，感觉学校帮了很大的忙，其他资料完全没用上，但是还是一定要准备好的。</p><p>出门取了存的包，然后坐地铁回西门，整整四个小时一直在紧张的情绪下站着真的是对身体极大的消耗，不过过了就好，后续就可以在<strong>校内买保险、让国际处老师买机票，以及领生活费了</strong>。</p><p>大致流程就是这样了</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;5.14下午去美国大使馆非移民签证处办了签证，虽然很折腾但是比较顺利，把过程记录一下。&lt;/p&gt;
&lt;p&gt;首先是漫长的准备阶段，参考了去年和以前的去芝加哥的学长学姐的各种流程和资料，网上搜罗的各种经验（靠谱的不靠谱的）甚至看了看youtube的面试指南。办签证这件事网上的信息实在是非常杂乱，信息量很低，包括很多out of date的信息，需要大量的过滤。&lt;/p&gt;
&lt;p&gt;办签证的过程一定要记好笔记，因为这可能是个持续数月的艰苦工程，笔记太凌乱就不放上来了，现在简单总结下重要的材料和节点&lt;/p&gt;
&lt;iframe frameborder=&quot;no&quot; border=&quot;0&quot; marginwidth=&quot;0&quot; marginheight=&quot;0&quot; width=&quot;330&quot; height=&quot;86&quot; src=&quot;//music.163.com/outchain/player?type=2&amp;id=28828120&amp;auto=1&amp;height=66&quot;&gt;&lt;/iframe&gt;
    
    </summary>
    
      <category term="summer research" scheme="https://www.cmwonderland.com/categories/summer-research/"/>
    
    
      <category term="school" scheme="https://www.cmwonderland.com/tags/school/"/>
    
      <category term="research" scheme="https://www.cmwonderland.com/tags/research/"/>
    
      <category term="summer research" scheme="https://www.cmwonderland.com/tags/summer-research/"/>
    
  </entry>
  
  <entry>
    <title>母爱是一弯柔缓的河流</title>
    <link href="https://www.cmwonderland.com/2018/05/12/mom/"/>
    <id>https://www.cmwonderland.com/2018/05/12/mom/</id>
    <published>2018-05-13T03:58:06.000Z</published>
    <updated>2018-05-13T04:42:08.186Z</updated>
    
    <content type="html"><![CDATA[<p><img src="http://i1.fuimg.com/640680/387ef9bb6b1632fa.jpg" alt="Markdown"><br>前几天读到了一篇文章，是从一个医学类的微信公众号看到的，辗转找到了知乎的原文<a href="https://www.zhihu.com/question/269575911/answer/372282921" target="_blank" rel="noopener">你手机里最舍不得删的那张照片有什么故事? - 知乎</a>，从一个丈夫和父亲的角度，讲述自己的妻子产后大出血的惊心动魄的故事，讲的特别细腻而揪心，大晚上看很容易流泪，让人有感动又害怕，把这样好的文章放在开头，似乎已经不用再写什么东西了，一切都在里面了。</p><p>看着文章的第一反应是如果生孩子这么辛苦和危险，要想办法找到更好的条件减轻风险，第二反应是妈妈真不容易，很勇敢，很了不起，勇敢地把我生了下来。我想世界的话语权长期被男性把持着，所以一直低估了女性在生育过程中经历的漫长的、折磨人的却又自愿的痛苦。如果细细回想，细细品味，从男人的角度看那些令人揪心的描述，想象漫长的数十个月甚至更久的煎熬和痛苦，实在是值得每个男人潸然泪下，为自己的母亲、自己的妻子和自己的孩子。</p><a id="more"></a><p>每个女人都曾经是二十多岁的美女，却要经受改变身材的难过，经历风险和巨大的痛苦，经历产后的日日夜夜的辛苦和煎熬，初为父母一定有很多不会、不懂的慌张，有很多劳力、劳心的疲乏与无力，想想已经让我感到紧张和担心了，所以我总是感觉到我们每个人，都远远低估了父母之爱的沉重与宝贵，尤其是一个个从弱女子成长起来的人送出的母爱，我相信等我为人父母的那一天我会体会地愈发深刻。你永远体会不到、甚至记不得了父母在你最弱小无力的那些年岁所做的事情，你记得青春年少时每一件鸡毛蒜皮的小事，你写诗唱歌、反复吟咏每一个细节，你却不知道母爱，你以为母爱是一句句唠叨和叮嘱，我想不是，因为母爱的沉重和复杂已经超出了母亲的表达能力，超出了青春年少文采飞扬的你所能表达描述的极限，以至于你熟视无睹，理所当然，无力描述，以至于多年后你理解了，甚至相当后悔。</p><p>我不是个愿意无休止的后悔的人，所以我一直努力不做后悔的事情，对于母爱，不后悔的源头就在于，认清楚这份爱多么重要，以及它明明不是<strong>unconditional love</strong>，不是<strong>无条件的爱</strong>，但母爱的伟大之处就在于有很多伟大的母亲让这份爱变得无条件，它浑然天成，不需要条件，这是怎样浓厚沉重的馈赠。  </p><p>我想清楚这件事在于“<strong>一个自私的念头</strong>”，我想到很多夫妻在结婚后重心都转移给了孩子，孩子成了纽带，中心，一切，在我看来这甚至算是一个破坏两个人的爱情的因素。我担心有一天我和我的妻子也会把很多倾注在孩子身上，没有了当年的浓浓的爱，而是对孩子的爱，对孩子的爱超过了对对方，可是沉浸在爱情的年轻人总是想独占对方的爱，不想和一个未来的无知的小生命分享爱，甚至抢走自己的爱，打扰自己的生活。  这个时候我想到了过去的自己，在过去的很多年里，我就是这样一个无知的小生命，我打扰了父母的美好的生活，我带给了他们很多快乐，也占有了他们无数的生命、时间和精力。我不知道我会不会向我未来的孩子抱怨：“都怪你，如果没有你，我和你妈妈两个人会多幸福，想去哪里去哪里，没有牵挂，没有烦恼”，可是我知道父母从没有这么对我说过， 我很惊讶，他们竟然没有我这样的自私的念头，让我占了很多年的光，让我得到了很多的东西，而我好像没有认真地感谢过几次。如果是我，我该如何向我的孩子抱怨呢？也许是：“我为你付出了这么多我生命的时光和精力，你无论如何感谢我都不够。”那我想我的父母一定很有资格这么对我说，这么想我就懂得母亲有的时候会有多伤心了，当这么多年的无条件的，如此沉重的付出之后，人永远无法理性起来，如果你还是个理性的人，那是因为你还不懂这份爱，现在我慢慢懂了，<strong>如果一份爱柔缓地滋润了你那么多年，那它就不该受到伤害。</strong></p><p>这不是我应得的，不是我无条件就必然获得的，但是父母从没有质疑过这是不是我应得的，这就是母爱和父爱的伟大之处，<strong>Unconditional Love</strong></p><p>我觉得妈妈很厉害，不仅仅是无声无言的滋润，妈妈还有很多的唠叨，交代，叮嘱。我是个很细致和谨慎的人了，但是妈妈总是比我更加谨慎细致，我胜在有更多获取资源学习分析的工具，妈妈胜在她真的很关心一切事情。我觉得妈妈能解决很多我解决不了的事情，我相信很多事情都是锻炼来的，妈妈一定是经历了很多锻炼，我记得有一次我想解决一件事，但是搞的一团乱麻，毫无头绪，中午还昏昏沉沉的，就给妈妈打电话，妈妈给我提供了一个简洁得让我惊讶的正确方法，那时候我已经开始读大学了。所以后来我意识到，很多时候妈妈看似做不到某件事了，只是因为她得不到我身边的资源和知识，她比你过去以为的厉害得多呢。我的妈妈是个雷厉风行又富有效率的人呀，当你需要她的时候，你就可以耍赖地静静地看着她为你忙碌，看着她忙碌着，找这找那，照顾好每个细节，在这样的节奏中你就可以彻底地放松下来，什么都不用想，不用操心，这样的生活真好呀，母爱的河流流的湍急的时候，自己的心反而柔缓下来了，很放松很舒适。</p><p>但是这样的生活也不好呀，活的太幸福了，自己就变笨了变懒了，还怎么给自己关心牵挂的人们带来帮助呢。如果我们都像这个世界上伟大的母亲一样，要锻炼自己来帮助自己的孩子，也许我们也可以变得一样强大，所以我也感谢我中学的时候就去外地读书了，让我没有变笨变懒，学着自己勤快一点，远离了母爱的滋润也让我更加懂得母爱，也给了我更深刻的关于母爱的记忆。我还记得在郑州的很多个日子，从初一到高三，天知道父母多费了多少心力。所以我以为我远离了，但是我一直没有远离他们的滋润和灌溉，这是一份恰到好处的爱。</p><p>我不知道世界上是否有一份考卷，我一直觉得应该有一份的，因为有的新闻让我愤怒地想“为什么为人父母这么重要的事情不用考试”，我喜欢列出来各种条件来评估一件事情，我很多次回想过我的人生经历，让我这样吹毛求疵的人都感觉满意、幸运、幸福，（方法之一是我问我自己，愿不愿意再来一次自己的人生重新选择、我的回答是不，我很满足，也不后悔于什么事），我知道在我幼小无知的很多年里，在我将自己的命运交给父母选择的很多年里，是妈妈为我做了一个又一个正确的决定，明明有那么多歧路，我竟然都没有走错，因此我想如果有这样的考卷的话，我相信我的妈妈一定是满分。</p><p><strong>谢谢妈妈，母亲节快乐！</strong></p><p><img src="http://i1.fuimg.com/640680/e2bfc669eb590032.jpg" alt="Markdow"></p><center><font color="red" ,font="" size="5">如果我懂得了爱，那一定是因为你</font></center>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;http://i1.fuimg.com/640680/387ef9bb6b1632fa.jpg&quot; alt=&quot;Markdown&quot;&gt;&lt;br&gt;前几天读到了一篇文章，是从一个医学类的微信公众号看到的，辗转找到了知乎的原文&lt;a href=&quot;https://www.zhihu.com/question/269575911/answer/372282921&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;你手机里最舍不得删的那张照片有什么故事? - 知乎&lt;/a&gt;，从一个丈夫和父亲的角度，讲述自己的妻子产后大出血的惊心动魄的故事，讲的特别细腻而揪心，大晚上看很容易流泪，让人有感动又害怕，把这样好的文章放在开头，似乎已经不用再写什么东西了，一切都在里面了。&lt;/p&gt;
&lt;p&gt;看着文章的第一反应是如果生孩子这么辛苦和危险，要想办法找到更好的条件减轻风险，第二反应是妈妈真不容易，很勇敢，很了不起，勇敢地把我生了下来。我想世界的话语权长期被男性把持着，所以一直低估了女性在生育过程中经历的漫长的、折磨人的却又自愿的痛苦。如果细细回想，细细品味，从男人的角度看那些令人揪心的描述，想象漫长的数十个月甚至更久的煎熬和痛苦，实在是值得每个男人潸然泪下，为自己的母亲、自己的妻子和自己的孩子。&lt;/p&gt;
    
    </summary>
    
      <category term="life" scheme="https://www.cmwonderland.com/categories/life/"/>
    
    
      <category term="life" scheme="https://www.cmwonderland.com/tags/life/"/>
    
  </entry>
  
</feed>
